# RTC 时序逻辑详解

## 你的理解 vs 实际逻辑

### 你的理解（部分正确）
```
a1 a2 a3 a4 a5 a6 a7 a8 a9 a10
从a4开始推理，a6结束推理
推理使用剩余动作 a7 a8 a9 a10 生成了新动作序列B
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
因为a4 a6之间隔了3个action，所以把b1 b2 b3丢掉
```

### 实际逻辑（需要修正）

## 关键点1：索引的含义

**重要**：`original_last_index` 表示**已经执行到的索引**，下一个要执行的是 `original_last_index`。

例如：
- 如果 `original_last_index = 4`，意味着已经执行了 a0, a1, a2, a3（索引0,1,2,3）
- 下一个要执行的是 a4（索引4）

## 关键点2：get_left_over() 返回什么？

```python
# action_queue.py 第 133 行
return self.original_queue[self.original_last_index :]
```

**返回从当前执行位置开始的所有剩余动作，包括下一个要执行的动作！**

例如：
- 如果 `original_last_index = 4`（已执行a0-a3，下一个是a4）
- `get_left_over()` 返回：**a4, a5, a6, a7, a8, a9, a10**（包括a4！）

## 关键点3：推理期间的执行

**推理期间，Actor线程继续执行动作！**

时间线：
```
时刻0: 执行a0
时刻1: 执行a1
时刻2: 执行a2
时刻3: 执行a3
时刻4: 执行a4 → 同时触发推理（original_last_index=4）
        推理开始，get_left_over() 返回 [a4, a5, a6, a7, a8, a9, a10]
时刻5: 执行a5（推理进行中）
时刻6: 执行a6（推理进行中）
时刻7: 推理完成，生成新动作序列B: [b0, b1, b2, b3, b4, b5, b6, b7, b8, b9]
        计算 real_delay = 3（推理期间执行了3个动作：a4, a5, a6）
        跳过前3个动作：b0, b1, b2
        从b3开始执行
```

## 正确的完整流程

### 场景设置
- 动作序列A: a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10
- 执行到a3时，`original_last_index = 4`（下一个要执行a4）
- 触发推理条件：`qsize() <= threshold`

### 步骤1：获取剩余动作
```python
action_index_before_inference = 4  # 下一个要执行a4
prev_actions = get_left_over()     # 返回 [a4, a5, a6, a7, a8, a9, a10]
```

**注意**：返回的是 **a4, a5, a6, a7, a8, a9, a10**，不是 a7, a8, a9, a10！

### 步骤2：推理期间继续执行
```
推理开始时刻: 准备执行a4
推理期间: 执行 a4, a5, a6（假设推理延迟3步）
推理结束时刻: 生成新动作序列B
```

### 步骤3：生成新动作序列B
```python
# 使用 prev_chunk_left_over = [a4, a5, a6, a7, a8, a9, a10] 作为引导
# 生成新动作序列B: [b0, b1, b2, b3, b4, b5, b6, b7, b8, b9]
```

**RTC引导的作用**：
- 新动作序列B的前几个时间步（b0, b1, b2, b3）会被引导对齐到 [a4, a5, a6, a7]
- 但由于推理延迟，a4, a5, a6已经执行了，所以需要跳过b0, b1, b2

### 步骤4：计算实际延迟
```python
new_latency = 实际推理时间（例如0.35秒）
new_delay = ceil(0.35 / 0.1) = 4  # 假设是4步
```

**注意**：`new_delay` 是**实际测量的推理延迟**，可能和预估的 `inference_delay` 不同。

### 步骤5：合并到队列（跳过延迟）
```python
# action_queue.py 第 174-175 行
self.original_queue = original_actions[real_delay:].clone()  # 跳过前4个
self.queue = processed_actions[real_delay*10:].clone()       # 跳过前40个（100Hz）
```

**结果**：
- `original_queue = [b4, b5, b6, b7, b8, b9]`（跳过b0, b1, b2, b3）
- `queue = [b4_100Hz, b5_100Hz, ..., b9_100Hz]`（重采样后跳过前40个）

### 步骤6：继续执行
```
从b4开始执行（对应10Hz的b4）
```

## 为什么是 a4, a5, a6, a7, a8, a9, a10 而不是 a7, a8, a9, a10？

**关键理解**：RTC引导需要**对齐到下一个要执行的动作**，而不是已经执行完的动作。

1. **对齐目标**：新动作序列B应该从a4的位置开始对齐（因为a4是下一个要执行的）
2. **引导范围**：使用 [a4, a5, a6, a7, a8, a9, a10] 作为引导，让B的前几个时间步对齐到这些动作
3. **延迟补偿**：由于推理延迟，a4, a5, a6已经执行了，所以跳过B的前几个动作（b0, b1, b2），从b3开始执行

## 可视化时间线

```
时间轴（10Hz）:  0    1    2    3    4    5    6    7    8    9    10
─────────────────────────────────────────────────────────────────────
旧动作A:        [a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10]
执行状态:       ✓    ✓    ✓    ✓    → 推理开始（original_last_index=4）
                ↑                    ↑
              已执行              下一个要执行

推理期间:                         执行a4, a5, a6（推理进行中）
                                  get_left_over() = [a4, a5, a6, a7, a8, a9, a10]
                                  ↑
                                包括下一个要执行的动作！

推理完成:                         生成新动作B: [b0, b1, b2, b3, b4, b5, b6, b7, b8, b9]
                                  real_delay = 3（实际执行了3个动作）
                                  跳过前3个: b0, b1, b2
                                  从b3开始执行

新队列:                           [b3, b4, b5, b6, b7, b8, b9]
                                  ↑
                                从b3开始执行（对齐到a7的位置）
```

## 总结

### 你的理解中正确的部分：
1. ✅ 从a4开始推理
2. ✅ 推理期间执行了a4, a5, a6
3. ✅ 生成新动作序列B
4. ✅ 跳过前几个动作（b0, b1, b2）

### 需要修正的部分：
1. ❌ **不是**使用 a7, a8, a9, a10 生成B
2. ✅ **而是**使用 a4, a5, a6, a7, a8, a9, a10 生成B（包括a4！）
3. ✅ 新动作B的前几个时间步（b0, b1, b2, b3）会被引导对齐到 [a4, a5, a6, a7]
4. ✅ 但由于推理延迟，a4, a5, a6已经执行了，所以跳过b0, b1, b2，从b3开始执行

### 关键公式：
```
get_left_over() = original_queue[original_last_index:]
                = 从下一个要执行的动作开始的所有剩余动作

real_delay = ceil(实际推理时间 / time_per_chunk)
           = 推理期间实际执行的动作数量

新队列 = 新动作序列[real_delay:]
      = 跳过推理期间已执行的动作数量
```

## 为什么这样设计？

1. **对齐到未来**：新动作应该对齐到**下一个要执行的动作**，而不是已经执行完的动作
2. **平滑过渡**：通过引导，新动作序列B的前几个时间步会自然对齐到旧动作序列A的剩余部分
3. **延迟补偿**：跳过推理期间已执行的动作，确保动作的时效性

