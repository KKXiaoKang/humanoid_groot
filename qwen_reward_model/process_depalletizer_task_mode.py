"""
ä½¿ç”¨ Qwen2.5-VL æ¨¡å‹æ ¹æ®å®æ—¶å›¾åƒåºåˆ—åˆ¤æ–­å½“å‰åº”è¯¥æ‰§è¡Œçš„ä»»åŠ¡

è¯¥è„šæœ¬ä¼šï¼š
1. è®¢é˜… ROS è¯é¢˜ /camera/color/image_raw è·å–å®æ—¶å›¾åƒ
2. ç»´æŠ¤æœ€è¿‘5å¸§çš„å›¾åƒå†å²
3. ä½¿ç”¨ Qwen2.5-VL æ¨¡å‹æ ¹æ®å›¾åƒåºåˆ—åˆ¤æ–­å½“å‰åº”è¯¥æ‰§è¡Œçš„ä»»åŠ¡
4. ä» tasks.jsonl è¯»å–ä»»åŠ¡åˆ—è¡¨ï¼ˆ5ä¸ªä»»åŠ¡ï¼‰
"""

import argparse
import json
import os
import threading
import time
from collections import deque
from pathlib import Path
from typing import List, Dict, Any, Optional
import re

import numpy as np
import rospy
from sensor_msgs.msg import Image
from std_srvs.srv import Empty, EmptyResponse
# å»¶è¿Ÿå¯¼å…¥cv_bridgeä»¥é¿å…NumPyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
CV_BRIDGE_AVAILABLE = False
CvBridge = None
try:
    from cv_bridge import CvBridge
    # å³ä½¿å¯¼å…¥æˆåŠŸï¼Œä¹Ÿå¯èƒ½åœ¨è¿è¡Œæ—¶å´©æºƒï¼Œæ‰€ä»¥æ ‡è®°ä¸ºå¯ç”¨ä½†ä¼šåœ¨ä½¿ç”¨æ—¶æµ‹è¯•
    CV_BRIDGE_AVAILABLE = True
except (ImportError, AttributeError, SystemError) as e:
    CV_BRIDGE_AVAILABLE = False
    print(f"âš ï¸  Warning: cv_bridge import failed: {e}")
    print("  This may be due to NumPy version incompatibility.")
    print("  Will use direct conversion from ROS message data.")
    # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç±»
    class CvBridge:
        def imgmsg_to_cv2(self, msg, desired_encoding='passthrough'):
            raise RuntimeError("cv_bridge not available. Please install or fix NumPy compatibility.")
except Exception as e:
    # æ•è·å…¶ä»–å¯èƒ½çš„å¼‚å¸¸
    CV_BRIDGE_AVAILABLE = False
    print(f"âš ï¸  Warning: cv_bridge import error: {e}")
    print("  Will use direct conversion from ROS message data.")
    class CvBridge:
        def imgmsg_to_cv2(self, msg, desired_encoding='passthrough'):
            raise RuntimeError("cv_bridge not available.")

from PIL import Image as PILImage
import torch

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("Warning: OpenCV not available. Some image format conversions may fail.")

# ä½¿ç”¨matplotlibè¿›è¡Œæ˜¾ç¤º
try:
    import matplotlib
    matplotlib.use('TkAgg')  # ä½¿ç”¨TkAggåç«¯ï¼Œæ›´å¯é 
    import matplotlib.pyplot as plt
    from matplotlib.animation import FuncAnimation
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("Warning: matplotlib not available. GUI visualization will be disabled.")
except Exception as e:
    MATPLOTLIB_AVAILABLE = False
    print(f"Warning: matplotlib initialization failed: {e}. GUI visualization will be disabled.")

try:
    from transformers import AutoProcessor
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration
        QWEN_MODEL_CLASS = Qwen2_5_VLForConditionalGeneration
    except ImportError:
        try:
            from transformers import AutoModel
            QWEN_MODEL_CLASS = AutoModel
        except ImportError:
            from transformers import AutoModelForCausalLM
            QWEN_MODEL_CLASS = AutoModelForCausalLM
except ImportError:
    raise ImportError("transformers library is required. Install with: pip install transformers")


class ImageBuffer:
    """ç»´æŠ¤å›¾åƒå†å²ç¼“å†²åŒº"""
    def __init__(self, max_size: int = 5):
        self.buffer = deque(maxlen=max_size)
        self.maxlen = max_size
        self.lock = threading.Lock()
    
    def add_image(self, image: PILImage.Image):
        """æ·»åŠ æ–°å›¾åƒåˆ°ç¼“å†²åŒº"""
        with self.lock:
            self.buffer.append(image)
    
    def get_images(self) -> List[PILImage.Image]:
        """è·å–å½“å‰æ‰€æœ‰å›¾åƒï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            return list(self.buffer)
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦å·²æ»¡ï¼ˆè¾¾åˆ°æœ€å¤§å¤§å°ï¼‰"""
        with self.lock:
            return len(self.buffer) >= self.buffer.maxlen


class TaskClassifier:
    """ä½¿ç”¨ Qwen2.5-VL è¿›è¡Œä»»åŠ¡åˆ†ç±»"""
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct",
        device: str = "auto",
        tasks_file: str = None,
        prompt_template: str = None,
        prompt_style: str = "detailed"
    ):
        """
        åˆå§‹åŒ–ä»»åŠ¡åˆ†ç±»å™¨
        
        Args:
            model_name: Qwen2.5-VL æ¨¡å‹åç§°
            device: è®¾å¤‡ï¼ˆauto, cuda, cpuï¼‰
            tasks_file: ä»»åŠ¡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆtasks.jsonlï¼‰
            prompt_template: è‡ªå®šä¹‰ prompt æ¨¡æ¿ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼‰
            prompt_style: prompt é£æ ¼ ("detailed", "simple", "step_by_step")
        """
        print(f"Loading model: {model_name}")
        self.model, self.processor = self._load_model_and_processor(model_name, device)
        self.device = next(self.model.parameters()).device
        
        # åŠ è½½ä»»åŠ¡åˆ—è¡¨
        if tasks_file is None:
            # é»˜è®¤è·¯å¾„
            tasks_file = "/home/lab/lerobot_groot/lerobot_data/1125_groot_train_data_with_task_filtered/meta/tasks.jsonl"
        
        self.tasks = self._load_tasks(tasks_file)
        self.prompt_template = prompt_template
        self.prompt_style = prompt_style
        print(f"Loaded {len(self.tasks)} tasks:")
        for task_idx, task_desc in self.tasks.items():
            print(f"  Task {task_idx}: {task_desc}")
    
    def _load_model_and_processor(self, model_name: str, device: str):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
            
            try:
                model = QWEN_MODEL_CLASS.from_pretrained(
                    model_name,
                    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                    device_map=device,
                    trust_remote_code=True
                )
            except (ValueError, TypeError) as e:
                print(f"Warning: Direct model class failed, trying AutoModel: {e}")
                from transformers import AutoModel
                model = AutoModel.from_pretrained(
                    model_name,
                    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                    device_map=device,
                    trust_remote_code=True
                )
            
            model.eval()
            print("âœ… Model loaded successfully!")
            return model, processor
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            raise
    
    def _load_tasks(self, tasks_file: str) -> Dict[int, str]:
        """ä» tasks.jsonl åŠ è½½ä»»åŠ¡åˆ—è¡¨"""
        tasks = {}
        try:
            with open(tasks_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        task_data = json.loads(line.strip())
                        task_idx = task_data.get('task_index')
                        task_desc = task_data.get('task')
                        if task_idx is not None and task_desc:
                            tasks[task_idx] = task_desc
        except Exception as e:
            print(f"Warning: Failed to load tasks from {tasks_file}: {e}")
            # ä½¿ç”¨é»˜è®¤ä»»åŠ¡
            tasks = {
                0: "Depalletize the green box on the left",
                1: "Depalletize the gray box on the left",
                2: "Depalletize the gray box on the right",
                3: "Depalletize the green box on the right",
                4: "Pick up a single green box"
            }
            print("Using default tasks")
        
        return tasks
    
    def _build_prompt(self) -> str:
        """æ„å»ºåˆ†ç±» prompt"""
        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰æ¨¡æ¿ï¼Œç›´æ¥ä½¿ç”¨
        if self.prompt_template is not None:
            task_list_text = "\n".join([f"Task {idx}: {desc}" for idx, desc in sorted(self.tasks.items())])
            return self.prompt_template.format(task_list=task_list_text)
        
        # æ ¹æ®é£æ ¼é€‰æ‹©ä¸åŒçš„ prompt
        task_list_text = "\n".join([f"Task {idx}: {desc}" for idx, desc in sorted(self.tasks.items())])
        
        if self.prompt_style == "simple":
            return f"""You are a robot task classifier. Analyze the image sequence and determine which task to execute.

Available tasks:
{task_list_text}

Observe the images carefully. Identify box colors (green/gray) and positions (left/right). Output ONLY the task index number (0-4)."""
        
        elif self.prompt_style == "step_by_step":
            return f"""You are a robot task classifier. Follow these steps to determine the task:

STEP 1: Identify box colors in the scene
- Look for GREEN boxes
- Look for GRAY boxes

STEP 2: Identify box positions
- Determine if boxes are on the LEFT side
- Determine if boxes are on the RIGHT side

STEP 3: Count boxes
- Count how many boxes are visible

STEP 4: Match to task
Available tasks:
{task_list_text}

STEP 5: Output the task index (0-4) that matches your observations.

Output format: Just the number (0, 1, 2, 3, or 4)"""
        
        else:  # detailed (default)
            return f"""You are a precision robot vision system. Your goal is to analyze the provided camera image and determine the correct task index from the list below by following a strict reasoning process.

AVAILABLE TASKS:
{task_list_text}

REASONING PROCESS:

Step 1: Locate the Target Box.
The target box is the one with a square black-and-white tag (QR-like code) on it. All other boxes are context.

Step 2: Analyze the Target Box and its Surroundings.
- What is the color of the tagged box? (Green or Gray)
- Where is the tagged box located within the camera frame? (e.g., primarily in the left half, right half, or center).
- Observe the boxes immediately next to the tagged box. Is there a gray box visible anywhere in the image?

Step 3: Apply the Perspective Inversion Rule.
The task names use "left" and "right" from the robot's point of view. The camera sees an inverted perspective.
- If the target box is in the **LEFT half** of the image, it corresponds to a **"right"** task.
- If the target box is in the **RIGHT half** of the image, it corresponds to a **"left"** task.

Step 4: Match Observations to Task Criteria.
Use your observations from Step 2 and the rule from Step 3 to find the single best match from the criteria below.

TASK IDENTIFICATION CRITERIA:
- **Task 0 (Depalletize the green box on the left)**:
  - The tagged box is GREEN.
  - The tagged box is located in the **RIGHT half** of the image.
  - There are typically other green boxes but no gray boxes nearby.

- **Task 1 (Depalletize the gray box on the left)**:
  - The tagged box is GRAY.
  - The tagged box is located in the **RIGHT half** of the image.

- **Task 2 (Depalletize the gray box on the right)**:
  - The tagged box is GRAY.
  - The tagged box is located in the **LEFT half** of the image.

- **Task 3 (Depalletize the green box on the right)**:
  - The tagged box is GREEN.
  - The tagged box is located in the **LEFT half** of the image.
  - A key distinguishing feature is the presence of a **GRAY box** next to the green boxes.

- **Task 4 (Pick up a single green box)**:
  - The scene shows **only ONE green box remaining on the top-most layer**.

Step 5: State Your Final Answer.
Explain your reasoning, then conclude with a sentence like "The final answer is Task X." where X is the index.

OUTPUT FORMAT:
Explain your reasoning, then conclude with a sentence like "The final answer is Task X." where X is the index."""
    
    def classify_task(self, images: List[PILImage.Image], verbose: bool = True) -> Dict[str, Any]:
        """
        æ ¹æ®å›¾åƒåºåˆ—åˆ†ç±»ä»»åŠ¡
        
        Args:
            images: å›¾åƒåˆ—è¡¨ï¼ˆæœ€å¤š5å¸§ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            åŒ…å«ä»»åŠ¡ç´¢å¼•ã€ä»»åŠ¡æè¿°ã€ç½®ä¿¡åº¦ç­‰çš„å­—å…¸
        """
        if len(images) == 0:
            return {
                "task_index": None,
                "task_description": None,
                "confidence": 0.0,
                "response": "No images provided",
                "error": "No images"
            }
        
        # æ„å»ºæç¤ºæ–‡æœ¬
        prompt = self._build_prompt()
        
        # å‡†å¤‡å¯¹è¯æ ¼å¼
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": img} for img in images
                ] + [
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        try:
            # å¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.processor(
                text=text,
                images=images,
                padding=True,
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if hasattr(inputs, 'to'):
                inputs = inputs.to(self.device)
            else:
                inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                         for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False
                )
            
            # è§£ç å“åº”
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            if verbose:
                print(f"Model response: {response}")
            
            # æå–ä»»åŠ¡ç´¢å¼•
            task_index = self._extract_task_index(response)
            
            if task_index is not None and task_index in self.tasks:
                task_description = self.tasks[task_index]
                return {
                    "task_index": task_index,
                    "task_description": task_description,
                    "confidence": 1.0,  # å¯ä»¥åç»­æ”¹è¿›ä¸ºå®é™…ç½®ä¿¡åº¦
                    "response": response,
                    "num_images": len(images)
                }
            else:
                return {
                    "task_index": None,
                    "task_description": None,
                    "confidence": 0.0,
                    "response": response,
                    "error": f"Could not extract valid task index from response: {response}"
                }
        
        except Exception as e:
            print(f"Error during classification: {e}")
            import traceback
            traceback.print_exc()
            return {
                "task_index": None,
                "task_description": None,
                "confidence": 0.0,
                "response": f"Error: {str(e)}",
                "error": str(e)
            }
    
    def _extract_task_index(self, response: str) -> Optional[int]:
        """ä»æ¨¡å‹å“åº”ä¸­æå–ä»»åŠ¡ç´¢å¼•"""
        if not response:
            return None
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        response_clean = response.strip()
        
        # å°è¯•æå–æ•°å­—ï¼ˆ0-4ï¼‰
        patterns = [
            r'\b([0-4])\b',  # ç›´æ¥åŒ¹é… 0-4
            r'task\s*[ï¼š:]\s*([0-4])',  # "task: 0"
            r'task\s*([0-4])',  # "task 0"
            r'([0-4])\s*$',  # è¡Œå°¾çš„æ•°å­—
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response_clean, re.IGNORECASE)
            if matches:
                try:
                    task_idx = int(matches[0])
                    if 0 <= task_idx <= 4:
                        return task_idx
                except ValueError:
                    continue
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'\d+', response_clean)
        if numbers:
            try:
                task_idx = int(numbers[0])
                if 0 <= task_idx <= 4:
                    return task_idx
            except ValueError:
                pass
        
        return None


class ROSImageSubscriber:
    """ROS å›¾åƒè®¢é˜…å™¨"""
    def __init__(
        self,
        topic: str = "/camera/color/image_raw",
        image_buffer: ImageBuffer = None,
        classification_interval: float = 2.0,
        enable_gui: bool = True
    ):
        """
        åˆå§‹åŒ– ROS å›¾åƒè®¢é˜…å™¨
        
        Args:
            topic: ROS è¯é¢˜åç§°
            image_buffer: å›¾åƒç¼“å†²åŒº
            classification_interval: åˆ†ç±»é—´éš”ï¼ˆç§’ï¼‰
            enable_gui: æ˜¯å¦å¯ç”¨ GUI å¯è§†åŒ–
        """
        self.topic = topic
        self.image_buffer = image_buffer if image_buffer else ImageBuffer(max_size=5)
        
        # ç”±äºNumPyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œcv_bridgeåœ¨è¿è¡Œæ—¶ä¼šå¯¼è‡´segmentation fault
        # å› æ­¤é»˜è®¤ä¸ä½¿ç”¨cv_bridgeï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨è½¬æ¢æ–¹æ³•
        # åªæœ‰åœ¨ç¯å¢ƒå˜é‡USE_CV_BRIDGE=1æ—¶æ‰å°è¯•ä½¿ç”¨cv_bridge
        self.bridge = None
        self.use_cv_bridge = False
        
        # æ£€æŸ¥æ˜¯å¦ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨cv_bridge
        use_cv_bridge_env = os.environ.get('USE_CV_BRIDGE', '0')
        if use_cv_bridge_env.lower() in ['1', 'true', 'yes']:
            if CV_BRIDGE_AVAILABLE and CvBridge is not None:
                try:
                    self.bridge = CvBridge()
                    self.use_cv_bridge = True
                    print("âš ï¸  Warning: cv_bridge enabled via USE_CV_BRIDGE=1")
                    print("   This may cause segmentation fault due to NumPy incompatibility!")
                except Exception as e:
                    print(f"âš ï¸  Warning: cv_bridge initialization failed: {e}")
                    print("   Will use direct conversion from ROS message data.")
                    self.use_cv_bridge = False
            else:
                print("âš ï¸  Warning: USE_CV_BRIDGE=1 but cv_bridge is not available")
                print("   Will use direct conversion from ROS message data.")
        else:
            # é»˜è®¤ä½¿ç”¨ç›´æ¥è½¬æ¢æ–¹æ³•ï¼ˆé¿å…segmentation faultï¼‰
            print("â„¹ï¸  Using direct conversion from ROS message data (default)")
            print("   Set USE_CV_BRIDGE=1 to enable cv_bridge (may cause crashes)")
        
        self.classification_interval = classification_interval
        self.last_classification_time = 0
        self.classifier = None
        self.running = False
        # GUIéœ€è¦matplotlibå¯ç”¨
        self.enable_gui = enable_gui and MATPLOTLIB_AVAILABLE
        
        # å¯è§†åŒ–ç›¸å…³
        self.current_task_result = None
        self.latest_cv_image = None
        self.latest_cv_image_lock = threading.Lock()
        self.window_name = "Task Classifier - Camera View"
        self._image_info_printed = False  # ç”¨äºæ§åˆ¶è°ƒè¯•ä¿¡æ¯è¾“å‡º
        self._display_error_printed = False  # ç”¨äºæ§åˆ¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯è¾“å‡º
        self._last_display_time = 0
        self._display_interval = 0.1  # é™åˆ¶æ˜¾ç¤ºé¢‘ç‡ï¼Œæ¯100msæœ€å¤šæ˜¾ç¤ºä¸€æ¬¡
        self._frame_count = 0
        self._display_every_n_frames = 3  # æ¯3å¸§æ˜¾ç¤ºä¸€æ¬¡ï¼Œå‡å°‘å¼€é”€
        
        # matplotlibç›¸å…³
        self.fig = None
        self.ax = None
        
        # ROS åˆå§‹åŒ–
        if not rospy.get_node_uri():
            rospy.init_node('task_classifier_node', anonymous=True)
        
        # åˆ›å»ºæœåŠ¡ï¼Œç”¨äºé‡ç½®çŠ¶æ€
        self.reset_service = rospy.Service('~reset', Empty, self.handle_reset_service)
        print(f"âœ… Reset service available at: {rospy.get_name()}/reset")
        
        # åˆ›å»ºè®¢é˜…è€…
        self.subscriber = rospy.Subscriber(
            topic,
            Image,
            self.image_callback,
            queue_size=1
        )
        
        print(f"âœ… Subscribed to ROS topic: {topic}")
        
        # å¦‚æœå¯ç”¨ GUIï¼Œåˆ›å»ºmatplotlibçª—å£
        if self.enable_gui:
            try:
                self.fig, self.ax = plt.subplots(figsize=(12, 8))
                self.fig.canvas.manager.set_window_title(self.window_name)
                self.ax.axis('off')
                # è®¾ç½®éé˜»å¡æ¨¡å¼
                plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼
                plt.show(block=False)
                print("âœ… GUI visualization enabled (using matplotlib)")
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to create matplotlib window: {e}")
                print("   Disabling GUI visualization. The program will continue without GUI.")
                self.enable_gui = False
    
    def set_classifier(self, classifier: TaskClassifier):
        """è®¾ç½®ä»»åŠ¡åˆ†ç±»å™¨"""
        self.classifier = classifier
    
    def handle_reset_service(self, req):
        """Handles requests to reset the classifier's state."""
        rospy.loginfo("Received request to reset task classifier state.")
        self.reset_state()
        return EmptyResponse()

    def reset_state(self):
        """Clears the image buffer and resets classification state."""
        # Clear the image buffer
        with self.image_buffer.lock:
            self.image_buffer.buffer.clear()
        
        # Reset the last classification result shown on GUI
        self.current_task_result = None
        
        # Allow for immediate re-classification once buffer is ready
        self.last_classification_time = 0
        
        rospy.loginfo("âœ… Classifier state has been reset. Buffer is now empty.")
    
    def _convert_ros_image_to_numpy(self, msg: Image) -> np.ndarray:
        """
        å°†ROS Imageæ¶ˆæ¯è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆRGBæ ¼å¼ï¼‰
        å¤‡ç”¨æ–¹æ³•ï¼Œä¸ä¾èµ–cv_bridge
        """
        # æ ¹æ®ç¼–ç æ ¼å¼ç¡®å®šé€šé“æ•°å’Œæ¯åƒç´ å­—èŠ‚æ•°
        if msg.encoding in ['rgb8', 'bgr8']:
            channels = 3
            bytes_per_pixel = 3
        elif msg.encoding in ['rgba8', 'bgra8']:
            channels = 4
            bytes_per_pixel = 4
        elif msg.encoding in ['mono8', '8UC1']:
            channels = 1
            bytes_per_pixel = 1
        elif msg.encoding in ['mono16', '16UC1']:
            channels = 1
            bytes_per_pixel = 2
        else:
            # é»˜è®¤å°è¯•3é€šé“
            channels = 3
            bytes_per_pixel = 3
            print(f"âš ï¸  Warning: Unknown encoding '{msg.encoding}', assuming 3 channels")
        
        # å¤„ç†16ä½å›¾åƒ
        if msg.encoding in ['mono16', '16UC1']:
            img_data = np.frombuffer(msg.data, dtype=np.uint16)
        else:
            img_data = np.frombuffer(msg.data, dtype=np.uint8)
        
        # è®¡ç®—æ¯è¡Œçš„æœŸæœ›å­—èŠ‚æ•°
        expected_row_size = msg.width * bytes_per_pixel
        
        # æ£€æŸ¥stepå­—æ®µï¼ˆæ¯è¡Œçš„å®é™…å­—èŠ‚æ•°ï¼Œå¯èƒ½åŒ…å«paddingï¼‰
        if hasattr(msg, 'step') and msg.step > 0:
            actual_row_size = msg.step
        else:
            actual_row_size = expected_row_size
        
        # å¦‚æœstepä¸æœŸæœ›å€¼ä¸åŒï¼Œè¯´æ˜æœ‰paddingï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if actual_row_size != expected_row_size:
            # æœ‰paddingçš„æƒ…å†µï¼šéœ€è¦é€è¡Œæå–æ•°æ®
            img_arr = np.zeros((msg.height, msg.width, channels) if channels > 1 else (msg.height, msg.width), 
                             dtype=img_data.dtype)
            for row in range(msg.height):
                start_idx = row * actual_row_size
                end_idx = start_idx + expected_row_size
                row_data = img_data[start_idx:end_idx]
                if channels == 1:
                    img_arr[row, :] = row_data.reshape(msg.width)
                else:
                    img_arr[row, :, :] = row_data.reshape(msg.width, channels)
        else:
            # æ²¡æœ‰paddingï¼Œç›´æ¥reshape
            if channels == 1:
                img_arr = img_data.reshape(msg.height, msg.width)
            else:
                img_arr = img_data.reshape(msg.height, msg.width, channels)
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        if msg.encoding == 'bgr8':
            if CV2_AVAILABLE:
                img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)
            else:
                # æ‰‹åŠ¨è½¬æ¢BGRåˆ°RGB
                img_arr = img_arr[:, :, ::-1]
        elif msg.encoding == 'bgra8':
            if CV2_AVAILABLE:
                img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGRA2RGB)
            else:
                # æ‰‹åŠ¨è½¬æ¢BGRAåˆ°RGB
                img_arr = img_arr[:, :, [2, 1, 0]]
        elif msg.encoding == 'rgba8':
            if CV2_AVAILABLE:
                img_arr = cv2.cvtColor(img_arr, cv2.COLOR_RGBA2RGB)
            else:
                # æ‰‹åŠ¨è½¬æ¢RGBAåˆ°RGB
                img_arr = img_arr[:, :, :3]
        elif msg.encoding in ['mono8', '8UC1', 'mono16', '16UC1']:
            # ç°åº¦å›¾è½¬RGB
            if CV2_AVAILABLE:
                img_arr = cv2.cvtColor(img_arr, cv2.COLOR_GRAY2RGB)
            else:
                # æ‰‹åŠ¨è½¬æ¢ç°åº¦åˆ°RGB
                img_arr = np.stack([img_arr, img_arr, img_arr], axis=-1)
        
        # ç¡®ä¿æ˜¯uint8ç±»å‹
        if img_arr.dtype != np.uint8:
            if img_arr.max() > 255:
                # 16ä½å›¾åƒï¼Œç¼©æ”¾åˆ°8ä½
                img_arr = (img_arr / 256).astype(np.uint8)
            else:
                img_arr = img_arr.astype(np.uint8)
        
        return img_arr
    
    def image_callback(self, msg: Image):
        """å›¾åƒå›è°ƒå‡½æ•°"""
        try:
            # è½¬æ¢ ROS Image æ¶ˆæ¯ä¸º numpy æ•°ç»„ï¼ˆRGBæ ¼å¼ï¼‰
            cv_image = None
            
            # ç”±äºNumPyå…¼å®¹æ€§é—®é¢˜ï¼Œcv_bridgeä¼šå¯¼è‡´segmentation fault
            # å› æ­¤é»˜è®¤ç›´æ¥ä½¿ç”¨å¤‡ç”¨è½¬æ¢æ–¹æ³•
            # åªæœ‰åœ¨æ˜ç¡®å¯ç”¨ä¸”æ²¡æœ‰NumPyé—®é¢˜æ—¶æ‰ä½¿ç”¨cv_bridge
            if self.use_cv_bridge and self.bridge is not None:
                # æ³¨æ„ï¼šå³ä½¿è¿™é‡Œå°è¯•ä½¿ç”¨ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´segmentation fault
                # æ‰€ä»¥é»˜è®¤æƒ…å†µä¸‹ä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
                try:
                    cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')
                except Exception as e:
                    print(f"âš ï¸  cv_bridge conversion failed: {e}")
                    print("   Switching to direct conversion method.")
                    self.use_cv_bridge = False
                    cv_image = None
            else:
                cv_image = None
            
            # ä½¿ç”¨ç›´æ¥è½¬æ¢æ–¹æ³•ï¼ˆé»˜è®¤æˆ–ä½œä¸ºå¤‡ç”¨ï¼‰
            if cv_image is None:
                cv_image = self._convert_ros_image_to_numpy(msg)
            
            # éªŒè¯å›¾åƒæ•°æ®
            if cv_image is None or cv_image.size == 0:
                print(f"âš ï¸  Warning: Invalid image data (shape: {cv_image.shape if cv_image is not None else 'None'})")
                return
            
            # æ‰“å°å›¾åƒä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
            if not self._image_info_printed:
                print(f"â„¹ï¸  Image info: encoding={msg.encoding}, shape={cv_image.shape}, dtype={cv_image.dtype}, "
                      f"min={cv_image.min()}, max={cv_image.max()}, step={getattr(msg, 'step', 'N/A')}")
                self._image_info_printed = True
            
            # ç¡®ä¿å›¾åƒæ˜¯3é€šé“RGBæ ¼å¼
            if len(cv_image.shape) == 2:
                # ç°åº¦å›¾ï¼Œè½¬æ¢ä¸ºRGB
                if CV2_AVAILABLE:
                    cv_image = cv2.cvtColor(cv_image, cv2.COLOR_GRAY2RGB)
                else:
                    cv_image = np.stack([cv_image, cv_image, cv_image], axis=-1)
            elif len(cv_image.shape) == 3 and cv_image.shape[2] != 3:
                # å¦‚æœä¸æ˜¯3é€šé“ï¼Œè½¬æ¢ä¸º3é€šé“
                if cv_image.shape[2] == 4:
                    # RGBAè½¬RGB
                    if CV2_AVAILABLE:
                        cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGBA2RGB)
                    else:
                        cv_image = cv_image[:, :, :3]
                else:
                    print(f"âš ï¸  Warning: Unexpected image shape: {cv_image.shape}")
                    return
            
            # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œå°†å›¾åƒæ—‹è½¬180åº¦
            if CV2_AVAILABLE:
                cv_image = cv2.rotate(cv_image, cv2.ROTATE_180)
            else:
                # å¦‚æœOpenCVä¸å¯ç”¨ï¼Œä½¿ç”¨NumPyè¿›è¡Œæ—‹è½¬
                cv_image = np.rot90(cv_image, 2)

            # ä¿å­˜å›¾åƒæ•°æ®ç”¨äºæ˜¾ç¤ºï¼ˆä¸åœ¨å›è°ƒä¸­ç›´æ¥æ“ä½œmatplotlibï¼Œé¿å…çº¿ç¨‹é—®é¢˜ï¼‰
            if self.enable_gui:
                self._frame_count += 1
                current_time = time.time()
                
                # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼šæ—¶é—´é—´éš”å’Œå¸§æ•°åŒé‡é™åˆ¶
                if (self._frame_count % self._display_every_n_frames == 0 and 
                    current_time - self._last_display_time >= self._display_interval):
                    try:
                        # ç¡®ä¿å›¾åƒæ˜¯è¿ç»­çš„
                        if not cv_image.flags['C_CONTIGUOUS']:
                            cv_image = np.ascontiguousarray(cv_image)
                        
                        # åªä¿å­˜å›¾åƒæ•°æ®ï¼Œä¸åœ¨è¿™é‡Œæ“ä½œmatplotlibï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                        with self.latest_cv_image_lock:
                            self.latest_cv_image = cv_image.copy()
                        
                        self._last_display_time = current_time
                    except Exception as e:
                        if not self._display_error_printed:
                            print(f"âš ï¸  Warning: Failed to save image for display: {e}")
                            self._display_error_printed = True
            
            # è½¬æ¢ä¸º PIL Image
            pil_image = PILImage.fromarray(cv_image)
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.image_buffer.add_image(pil_image)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†ç±»
            current_time = time.time()
            if (self.classifier is not None and 
                self.image_buffer.is_ready() and
                current_time - self.last_classification_time >= self.classification_interval):
                
                self.last_classification_time = current_time
                self.classify_current_task()
        
        except Exception as e:
            print(f"Error processing image: {e}")
            import traceback
            traceback.print_exc()
    
    def classify_current_task(self):
        """å¯¹å½“å‰å›¾åƒåºåˆ—è¿›è¡Œåˆ†ç±»"""
        images = self.image_buffer.get_images()
        if len(images) > 0 and self.classifier is not None:
            print(f"\nğŸ” Classifying task with {len(images)} images...")
            result = self.classifier.classify_task(images, verbose=True)
            
            # ä¿å­˜ä»»åŠ¡ç»“æœç”¨äºå¯è§†åŒ–
            self.current_task_result = result
            
            if result["task_index"] is not None:
                print(f"âœ… Predicted Task: {result['task_index']} - {result['task_description']}")
                print(f"   Confidence: {result['confidence']:.2f}")
            else:
                print(f"âŒ Failed to classify task: {result.get('error', 'Unknown error')}")
                print(f"   Response: {result['response']}")
    
    def _update_display_timer_callback(self, event):
        """ROSå®šæ—¶å™¨å›è°ƒï¼Œåœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°matplotlibæ˜¾ç¤ºï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if not self.enable_gui or self.fig is None:
            return
        
        try:
            # æ£€æŸ¥çª—å£æ˜¯å¦å…³é—­
            if not plt.fignum_exists(self.fig.number):
                print("\nMatplotlib window closed. Shutting down...")
                rospy.signal_shutdown("Window closed")
                return
            
            # è·å–æœ€æ–°çš„å›¾åƒæ•°æ®
            current_image = None
            with self.latest_cv_image_lock:
                if self.latest_cv_image is not None:
                    current_image = self.latest_cv_image
                    self.latest_cv_image = None  # æ¶ˆè´¹å›¾åƒï¼Œé¿å…é‡å¤æ˜¾ç¤º
            
            # å¦‚æœæœ‰æ–°å›¾åƒï¼Œæ›´æ–°æ˜¾ç¤º
            if current_image is not None:
                self._draw_task_info_matplotlib(current_image)
                # åˆ·æ–°æ˜¾ç¤ºï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œå®‰å…¨ï¼‰
                self.fig.canvas.draw_idle()
                self.fig.canvas.flush_events()
        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…é¢‘ç¹æ‰“å°
            pass
    
    def _draw_task_info_matplotlib(self, image: np.ndarray):
        """ä½¿ç”¨matplotlibç»˜åˆ¶ä»»åŠ¡ä¿¡æ¯ï¼ˆRGBæ ¼å¼ï¼‰- å¿…é¡»åœ¨ä¸»çº¿ç¨‹ä¸­è°ƒç”¨"""
        if image is None or image.size == 0:
            return
        
        try:
            # ç¡®ä¿å›¾åƒæ˜¯è¿ç»­çš„
            if not image.flags['C_CONTIGUOUS']:
                image = np.ascontiguousarray(image)
            
            h, w = image.shape[:2]
            
            if h <= 0 or w <= 0 or len(image.shape) != 3:
                return
            
            # æ¸…é™¤ä¹‹å‰çš„æ–‡æœ¬å’Œå›¾åƒ
            self.ax.clear()
            self.ax.axis('off')
            
            # æ˜¾ç¤ºå›¾åƒ
            self.ax.imshow(image)
            
            # æ·»åŠ ä»»åŠ¡ä¿¡æ¯æ–‡æœ¬ï¼ˆä½¿ç”¨ç›¸å¯¹åæ ‡ï¼Œ0-1ä¹‹é—´ï¼‰
            if self.current_task_result is not None:
                result = self.current_task_result
                if result.get("task_index") is not None:
                    task_text = f"Task {result['task_index']}: {result.get('task_description', '')[:50]}"
                    self.ax.text(0.02, 0.05, task_text, transform=self.ax.transAxes,
                               fontsize=14, color='green', weight='bold',
                               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
                    conf_text = f"Confidence: {result.get('confidence', 0):.2f}"
                    self.ax.text(0.02, 0.10, conf_text, transform=self.ax.transAxes,
                               fontsize=12, color='yellow',
                               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
                else:
                    error_msg = result.get('error', 'Unknown error')
                    if len(error_msg) > 50:
                        error_msg = error_msg[:47] + "..."
                    self.ax.text(0.02, 0.05, "Classification Failed", transform=self.ax.transAxes,
                               fontsize=14, color='red', weight='bold',
                               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
                    self.ax.text(0.02, 0.10, error_msg, transform=self.ax.transAxes,
                               fontsize=10, color='red',
                               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
            else:
                buffer_status = f"Buffer: {len(self.image_buffer.get_images())}/{self.image_buffer.maxlen}"
                self.ax.text(0.02, 0.05, "Waiting for classification...", transform=self.ax.transAxes,
                           fontsize=14, color='yellow', weight='bold',
                           bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
                self.ax.text(0.02, 0.10, buffer_status, transform=self.ax.transAxes,
                           fontsize=10, color='gray',
                           bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))
            
            # Frame counter for debugging
            self.ax.text(0.02, 0.98, f"Frame: {self._frame_count}", transform=self.ax.transAxes,
                       fontsize=10, color='white', ha='left', va='top',
                       bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))

            # åº•éƒ¨æç¤º
            self.ax.text(0.98, 0.98, "Close window to quit", transform=self.ax.transAxes,
                       fontsize=10, color='gray', ha='right', va='top',
                       bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))
        except Exception as e:
            if not self._display_error_printed:
                print(f"âš ï¸  Warning: Failed to draw task info: {e}")
                import traceback
                traceback.print_exc()
                self._display_error_printed = True
    
    
    
    def spin(self):
        """è¿è¡Œ ROS èŠ‚ç‚¹ï¼Œå¹¶åœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç† GUI æ›´æ–°"""
        self.running = True
        print("ğŸ”„ Starting ROS node...")

        # å¦‚æœç¦ç”¨ GUIï¼Œåˆ™ä½¿ç”¨ rospy.spin() é˜»å¡
        if not self.enable_gui:
            rospy.spin()
            self.running = False
            return

        # å¦‚æœå¯ç”¨ GUIï¼Œåˆ™è¿è¡Œè‡ªå®šä¹‰å¾ªç¯ä»¥åœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç† matplotlib æ›´æ–°
        rate = rospy.Rate(30)  # 30 Hz
        try:
            while not rospy.is_shutdown():
                self._update_display_timer_callback(None)  # è°ƒç”¨ GUI æ›´æ–°é€»è¾‘
                rate.sleep()
        finally:
            self.running = False
            # æ¸…ç† matplotlib çª—å£
            if self.fig is not None:
                try:
                    plt.close(self.fig)
                except Exception:
                    pass


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Qwen2.5-VL æ ¹æ®å®æ—¶å›¾åƒåˆ¤æ–­ä»»åŠ¡")
    parser.add_argument(
        "--topic",
        type=str,
        default="/camera/color/image_raw",
        help="ROS å›¾åƒè¯é¢˜åç§°"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="Qwen2.5-VL æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡ï¼ˆauto, cuda, cpuï¼‰"
    )
    parser.add_argument(
        "--tasks_file",
        type=str,
        default=None,
        help="ä»»åŠ¡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆtasks.jsonlï¼‰"
    )
    parser.add_argument(
        "--history_size",
        type=int,
        default=5,
        help="å›¾åƒå†å²ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤5å¸§ï¼‰"
    )
    parser.add_argument(
        "--classification_interval",
        type=float,
        default=2.0,
        help="åˆ†ç±»é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤2.0ç§’ï¼‰"
    )
    parser.add_argument(
        "--enable_gui",
        action="store_true",
        default=True,
        help="å¯ç”¨ GUI å¯è§†åŒ–çª—å£ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--disable_gui",
        action="store_true",
        help="ç¦ç”¨ GUI å¯è§†åŒ–çª—å£"
    )
    parser.add_argument(
        "--prompt_style",
        type=str,
        default="detailed",
        choices=["detailed", "simple", "step_by_step"],
        help="Prompt é£æ ¼ï¼šdetailedï¼ˆè¯¦ç»†ï¼Œé»˜è®¤ï¼‰ã€simpleï¼ˆç®€å•ï¼‰ã€step_by_stepï¼ˆåˆ†æ­¥ï¼‰"
    )
    parser.add_argument(
        "--prompt_template",
        type=str,
        default=None,
        help="è‡ªå®šä¹‰ prompt æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›– prompt_styleï¼‰"
    )
    
    args = parser.parse_args()
    
    # å¤„ç† GUI é€‰é¡¹
    enable_gui = args.enable_gui and not args.disable_gui
    
    if enable_gui and not MATPLOTLIB_AVAILABLE:
        print("âš ï¸  Warning: GUI requested but matplotlib not available. Disabling GUI.")
        print("   Install matplotlib to enable GUI visualization: pip install matplotlib")
        enable_gui = False
    
    # è¯»å–è‡ªå®šä¹‰ prompt æ¨¡æ¿ï¼ˆå¦‚æœæœ‰ï¼‰
    prompt_template = None
    if args.prompt_template:
        try:
            with open(args.prompt_template, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
            print(f"âœ… Loaded custom prompt template from: {args.prompt_template}")
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to load prompt template: {e}")
            print("   Using default prompt style instead.")
    
    # åˆ›å»ºå›¾åƒç¼“å†²åŒº
    image_buffer = ImageBuffer(max_size=args.history_size)
    
    # åˆ›å»ºä»»åŠ¡åˆ†ç±»å™¨
    print("Initializing task classifier...")
    classifier = TaskClassifier(
        model_name=args.model_name,
        device=args.device,
        tasks_file=args.tasks_file,
        prompt_template=prompt_template,
        prompt_style=args.prompt_style
    )
    
    # åˆ›å»º ROS è®¢é˜…å™¨
    subscriber = ROSImageSubscriber(
        topic=args.topic,
        image_buffer=image_buffer,
        classification_interval=args.classification_interval,
        enable_gui=enable_gui
    )
    subscriber.set_classifier(classifier)
    
    # è¿è¡Œ
    try:
        print("\n" + "="*60)
        print("Task Classifier is running...")
        print(f"  Topic: {args.topic}")
        print(f"  History size: {args.history_size} frames")
        print(f"  Classification interval: {args.classification_interval} seconds")
        print(f"  GUI visualization: {'Enabled' if enable_gui else 'Disabled'}")
        print("="*60)
        if enable_gui:
            print("\nğŸ’¡ GUI Window opened. Close the matplotlib window to quit.")
        print("\nWaiting for images...")
        subscriber.spin()
    except KeyboardInterrupt:
        print("\n\nShutting down...")
    except rospy.ROSInterruptException:
        print("\n\nROS interrupted")


if __name__ == "__main__":
    main()
