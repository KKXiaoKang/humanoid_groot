"""
ä½¿ç”¨ Qwen2.5-VL-7B-Instruct æ¨¡å‹è¯„ä¼° lerobot v3.0 æ•°æ®é›†çš„ä»»åŠ¡å®Œæˆåº¦

è¯¥è„šæœ¬ä¼šï¼š
1. åŠ è½½ lerobot v3.0 æ•°æ®é›†
2. å¯¹æ¯ä¸ªepisodeçš„è§†è§‰åºåˆ—ä½¿ç”¨ Qwen2.5-VL-7B-Instruct è¿›è¡Œè¯„ä¼°
3. è¾“å‡ºä»»åŠ¡å®Œæˆåº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
"""

import argparse
import json
import os
import base64
import io
from pathlib import Path
from typing import List, Dict, Any, Tuple
import re

import torch
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.animation import FuncAnimation
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("Warning: OpenCV not available. MP4 generation will be disabled.")
try:
    from transformers import AutoProcessor
    # å°è¯•å¯¼å…¥ Qwen2.5-VL çš„ä¸“ç”¨ç±»
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration
        QWEN_MODEL_CLASS = Qwen2_5_VLForConditionalGeneration
    except ImportError:
        try:
            # å¦‚æœä¸Šé¢çš„å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ AutoModel
            from transformers import AutoModel
            QWEN_MODEL_CLASS = AutoModel
        except ImportError:
            # æœ€åå°è¯• AutoModelForCausalLM
            from transformers import AutoModelForCausalLM
            QWEN_MODEL_CLASS = AutoModelForCausalLM
except ImportError:
    raise ImportError("transformers library is required. Install with: pip install transformers")
from tqdm import tqdm

from lerobot.datasets.lerobot_dataset import LeRobotDataset


def tensor_to_pil_image(tensor: torch.Tensor) -> Image.Image:
    """å°† PyTorch tensor (C, H, W) è½¬æ¢ä¸º PIL Image"""
    # ç¡®ä¿ tensor åœ¨ CPU ä¸Š
    if tensor.is_cuda:
        tensor = tensor.cpu()
    
    # è½¬æ¢ä¸º numpy array
    if tensor.dim() == 3:
        # (C, H, W) -> (H, W, C)
        arr = tensor.permute(1, 2, 0).numpy()
    else:
        arr = tensor.numpy()
    
    # å½’ä¸€åŒ–åˆ° [0, 255]
    if arr.dtype != np.uint8:
        if arr.max() <= 1.0:
            arr = (arr * 255).astype(np.uint8)
        else:
            arr = arr.astype(np.uint8)
    
    # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
    arr = np.clip(arr, 0, 255).astype(np.uint8)
    
    return Image.fromarray(arr)


def load_model_and_processor(model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct", device: str = "auto"):
    """
    åŠ è½½ Qwen2.5-VL æ¨¡å‹å’Œå¤„ç†å™¨
    """
    print(f"Loading model: {model_name}")
    
    # æ£€æŸ¥ transformers ç‰ˆæœ¬
    try:
        import transformers
        version = transformers.__version__
        print(f"Transformers version: {version}")
        # Qwen2.5-VL éœ€è¦è¾ƒæ–°çš„ transformers ç‰ˆæœ¬
        from packaging import version as pkg_version
        if pkg_version.parse(version) < pkg_version.parse("4.40.0"):
            print("âš ï¸  è­¦å‘Š: transformers ç‰ˆæœ¬å¯èƒ½è¿‡æ—§ï¼ŒQwen2.5-VL éœ€è¦ >= 4.40.0")
            print("   å»ºè®®å‡çº§: pip install --upgrade transformers")
    except Exception:
        pass
    
    # æ£€æŸ¥ä¸‹è½½æº
    hf_endpoint = os.environ.get("HF_ENDPOINT", "https://huggingface.co")
    if "hf-mirror.com" in hf_endpoint:
        print(f"ğŸ“¡ ä½¿ç”¨ Hugging Face é•œåƒæº: {hf_endpoint}")
    else:
        print(f"ğŸ“¡ ä½¿ç”¨ Hugging Face å®˜æ–¹æº: {hf_endpoint}")
        print("   æç¤ºï¼šå¦‚æœåœ¨å›½å†…ä¸‹è½½è¾ƒæ…¢ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
        print("   export HF_ENDPOINT=https://hf-mirror.com")
    
    try:
        # åŠ è½½å¤„ç†å™¨
        print("Loading processor...")
        processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        
        # åŠ è½½æ¨¡å‹ - å°è¯•ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç±»
        print("Loading model (this may take a while)...")
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ä¸“ç”¨ç±»
            model = QWEN_MODEL_CLASS.from_pretrained(
                model_name,
                dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                device_map=device,
                trust_remote_code=True
            )
        except (ValueError, TypeError) as e:
            # å¦‚æœä¸“ç”¨ç±»å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ AutoModel
            print(f"Warning: Direct model class failed, trying AutoModel: {e}")
            from transformers import AutoModel
            model = AutoModel.from_pretrained(
                model_name,
                dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                device_map=device,
                trust_remote_code=True
            )
        model.eval()
        
        print("âœ… Model loaded successfully!")
        return model, processor
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
        print("1. å‡çº§ transformers åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆQwen2.5-VL éœ€è¦è¾ƒæ–°ç‰ˆæœ¬ï¼‰ï¼š")
        print("   pip install --upgrade transformers")
        print("   å»ºè®®ç‰ˆæœ¬ >= 4.40.0")
        print("2. å¦‚æœä½¿ç”¨å›½å†…ç½‘ç»œï¼Œå¯ä»¥è®¾ç½®é•œåƒï¼š")
        print("   export HF_ENDPOINT=https://hf-mirror.com")
        print("3. æ£€æŸ¥ transformers ç‰ˆæœ¬ï¼š")
        print("   python -c 'import transformers; print(transformers.__version__)'")
        print("4. å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦ä»æºç å®‰è£… transformersï¼š")
        print("   pip install git+https://github.com/huggingface/transformers.git")
        raise


def escape_html(text: str) -> str:
    """è½¬ä¹‰ HTML ç‰¹æ®Šå­—ç¬¦"""
    if text is None:
        return ""
    return (str(text)
            .replace('&', '&amp;')
            .replace('<', '&lt;')
            .replace('>', '&gt;')
            .replace('"', '&quot;')
            .replace("'", '&#39;'))


def generate_html_report(output_data: Dict[str, Any], html_path: str):
    """
    ç”Ÿæˆ HTML å¯è§†åŒ–æŠ¥å‘Š
    
    Args:
        output_data: è¯„ä¼°ç»“æœæ•°æ®
        html_path: HTML æ–‡ä»¶ä¿å­˜è·¯å¾„
    """
    html_content = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qwen Reward Model è¯„ä¼°æŠ¥å‘Š</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}
        .header .subtitle {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
            border-bottom: 2px solid #e9ecef;
        }}
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            text-align: center;
        }}
        .stat-card .label {{
            font-size: 0.9em;
            color: #6c757d;
            margin-bottom: 8px;
        }}
        .stat-card .value {{
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }}
        .episodes {{
            padding: 30px;
        }}
        .episode-card {{
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 12px;
            margin-bottom: 30px;
            overflow: hidden;
            transition: transform 0.2s, box-shadow 0.2s;
        }}
        .episode-card:hover {{
            transform: translateY(-4px);
            box-shadow: 0 8px 24px rgba(0,0,0,0.15);
        }}
        .episode-header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .episode-header h2 {{
            font-size: 1.5em;
        }}
        .score-badge {{
            background: rgba(255,255,255,0.2);
            padding: 10px 20px;
            border-radius: 20px;
            font-size: 1.3em;
            font-weight: bold;
        }}
        .score-badge.success {{
            background: rgba(40, 167, 69, 0.3);
        }}
        .score-badge.failed {{
            background: rgba(220, 53, 69, 0.3);
        }}
        .episode-content {{
            padding: 25px;
        }}
        .section {{
            margin-bottom: 25px;
        }}
        .section-title {{
            font-size: 1.2em;
            font-weight: bold;
            color: #495057;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #e9ecef;
        }}
        .prompt-box {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.6;
        }}
        .response-box {{
            background: #e7f3ff;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.6;
        }}
        .error-box {{
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #ffc107;
            color: #856404;
        }}
        .images-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }}
        .image-item {{
            background: #f8f9fa;
            border-radius: 8px;
            padding: 10px;
            text-align: center;
            border: 2px solid #e9ecef;
            transition: transform 0.2s;
        }}
        .image-item:hover {{
            transform: scale(1.05);
            border-color: #667eea;
        }}
        .image-item img {{
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-bottom: 8px;
        }}
        .image-item .image-info {{
            font-size: 0.85em;
            color: #6c757d;
        }}
        .processed-text {{
            background: #f1f3f5;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            max-height: 200px;
            overflow-y: auto;
            word-break: break-all;
        }}
        .task-info {{
            background: #e7f5e7;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #28a745;
            margin-bottom: 20px;
        }}
        .task-info strong {{
            color: #155724;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¤– Qwen Reward Model è¯„ä¼°æŠ¥å‘Š</h1>
            <div class="subtitle">ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ç»“æœå¯è§†åŒ–</div>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="label">æ•°æ®é›†è·¯å¾„</div>
                <div class="value" style="font-size: 0.9em; word-break: break-all;">{escape_html(output_data.get('dataset_path', 'N/A'))}</div>
            </div>
            <div class="stat-card">
                <div class="label">æ¨¡å‹åç§°</div>
                <div class="value" style="font-size: 1em;">{escape_html(output_data.get('model_name', 'N/A'))}</div>
            </div>
            <div class="stat-card">
                <div class="label">æ€» Episodes</div>
                <div class="value">{output_data.get('num_episodes', 0)}</div>
            </div>
            <div class="stat-card">
                <div class="label">æˆåŠŸè¯„ä¼°</div>
                <div class="value" style="color: #28a745;">{output_data['statistics'].get('successful_episodes', 0)}</div>
            </div>
            <div class="stat-card">
                <div class="label">å¤±è´¥è¯„ä¼°</div>
                <div class="value" style="color: #dc3545;">{output_data['statistics'].get('failed_episodes', 0)}</div>
            </div>
"""
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    stats = output_data['statistics']
    if stats.get('mean_score') is not None:
        html_content += f"""
            <div class="stat-card">
                <div class="label">å¹³å‡åˆ†æ•°</div>
                <div class="value">{stats['mean_score']:.3f}</div>
            </div>
            <div class="stat-card">
                <div class="label">æ ‡å‡†å·®</div>
                <div class="value">{stats['std_score']:.3f}</div>
            </div>
            <div class="stat-card">
                <div class="label">æœ€ä½åˆ†æ•°</div>
                <div class="value">{stats['min_score']:.3f}</div>
            </div>
            <div class="stat-card">
                <div class="label">æœ€é«˜åˆ†æ•°</div>
                <div class="value">{stats['max_score']:.3f}</div>
            </div>
"""
    
    html_content += """
        </div>
        
        <div class="episodes">
"""
    
    # æ·»åŠ æ¯ä¸ª episode çš„ç»“æœ
    for result in output_data.get('results', []):
        episode_idx = result.get('episode', 'N/A')
        score = result.get('score')
        response = result.get('response', 'N/A')
        prompt = result.get('prompt', 'N/A')
        processed_text = result.get('processed_text', '')
        images = result.get('images', [])
        task = result.get('task', 'N/A')
        error = result.get('error')
        
        # ç¡®å®šåˆ†æ•°æ˜¾ç¤ºæ ·å¼
        if score is not None:
            score_class = "success" if score >= 0.7 else ("failed" if score < 0.3 else "")
            score_display = f"{score:.3f}"
        else:
            score_class = "failed"
            score_display = "å¤±è´¥"
        
        html_content += f"""
            <div class="episode-card">
                <div class="episode-header">
                    <h2>Episode {episode_idx}</h2>
                    <div class="score-badge {score_class}">{score_display}</div>
                </div>
                <div class="episode-content">
                    <div class="task-info">
                        <strong>ä»»åŠ¡æè¿°ï¼š</strong>{escape_html(task)}
                    </div>
"""
        
        # å¦‚æœæœ‰é”™è¯¯ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if error:
            html_content += f"""
                    <div class="section">
                        <div class="section-title">âŒ é”™è¯¯ä¿¡æ¯</div>
                        <div class="error-box">{escape_html(error)}</div>
                    </div>
"""
        
        # æ˜¾ç¤ºæç¤ºæ–‡æœ¬
        html_content += f"""
                    <div class="section">
                        <div class="section-title">ğŸ“ æç¤ºæ–‡æœ¬ (Prompt)</div>
                        <div class="prompt-box">{escape_html(prompt)}</div>
                    </div>
"""
        
        # æ˜¾ç¤ºå¤„ç†åçš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        if processed_text:
            processed_text_escaped = escape_html(processed_text[:500])
            if len(processed_text) > 500:
                processed_text_escaped += "..."
            html_content += f"""
                    <div class="section">
                        <div class="section-title">ğŸ”¤ å¤„ç†åçš„æ–‡æœ¬ (åŒ…å«å›¾åƒå ä½ç¬¦)</div>
                        <div class="processed-text">{processed_text_escaped}</div>
                    </div>
"""
        
        # æ˜¾ç¤ºå›¾åƒ
        if images:
            html_content += f"""
                    <div class="section">
                        <div class="section-title">ğŸ–¼ï¸ ä½¿ç”¨çš„å›¾åƒ ({len(images)} å¼ )</div>
                        <div class="images-grid">
"""
            for img_info in images:
                idx = img_info.get('index', 0)
                size = img_info.get('size', [0, 0])
                thumbnail = img_info.get('thumbnail_base64', '')
                html_content += f"""
                            <div class="image-item">
                                <img src="{thumbnail}" alt="Image {idx}" />
                                <div class="image-info">
                                    <div>å›¾åƒ #{idx}</div>
                                    <div>å°ºå¯¸: {size[0]}Ã—{size[1]}</div>
                                </div>
                            </div>
"""
            html_content += """
                        </div>
                    </div>
"""
        
        # æ˜¾ç¤ºæ¨¡å‹å“åº”
        html_content += f"""
                    <div class="section">
                        <div class="section-title">ğŸ’¬ æ¨¡å‹å“åº” (Response)</div>
                        <div class="response-box">{escape_html(response)}</div>
                    </div>
"""
        
        html_content += """
                </div>
            </div>
"""
    
    html_content += """
        </div>
    </div>
</body>
</html>
"""
    
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… HTML report generated: {html_path}")


def extract_score_from_response(response: str, verbose: bool = True) -> float:
    """ä»æ¨¡å‹å“åº”ä¸­æå– 0-1 çš„åˆ†æ•°"""
    if not response:
        print("Warning: Empty response")
        return 0.5
    
    # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„æ ‡è®°å’Œç‰¹æ®Šå­—ç¬¦
    response_clean = response.strip()
    
    # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    patterns = [
        # ä¼˜å…ˆåŒ¹é…æ˜ç¡®çš„åˆ†æ•°æ ¼å¼
        r'åˆ†æ•°[ï¼š:]\s*(\d+\.?\d*)',  # "åˆ†æ•°ï¼š0.8"
        r'(\d+\.?\d*)\s*/\s*1',  # "0.8/1"
        r'(\d+\.?\d*)\s*åˆ†',  # "0.8åˆ†"
        r'(\d+\.?\d*)\s*out\s*of\s*1',  # "0.8 out of 1"
        # åŒ¹é… 0-1 ä¹‹é—´çš„æ•°å­—ï¼ˆæ›´ä¸¥æ ¼çš„æ¨¡å¼ï¼‰
        r'\b(0\.\d+)\b',  # "0.75" è¿™æ ·çš„æ ¼å¼
        r'\b(1\.0)\b',  # "1.0"
        r'\b(0)\b',  # "0"
        r'\b(1)\b',  # "1"
        # æœ€åå°è¯•åŒ¹é…ä»»ä½•æ•°å­—
        r'(\d+\.?\d*)',  # ä»»ä½•æ•°å­—
    ]
    
    # é¦–å…ˆå°è¯•æå–æ˜ç¡®çš„åˆ†æ•°
    for pattern in patterns:
        matches = re.findall(pattern, response_clean, re.IGNORECASE)
        if matches:
            try:
                score = float(matches[0])
                # ç¡®ä¿åˆ†æ•°åœ¨ 0-1 èŒƒå›´å†…
                if score > 1.0:
                    # å¦‚æœæ˜¯å¤§äº1çš„æ•°å­—ï¼Œå¯èƒ½æ˜¯ç™¾åˆ†æ¯”å½¢å¼
                    if score <= 100:
                        score = score / 100.0
                    else:
                        score = 1.0
                score = max(0.0, min(1.0, score))
                # å‡å°‘è¾“å‡ºï¼Œåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°
                # print(f"âœ… Extracted score {score} using pattern: {pattern}")
                return score
            except ValueError:
                continue
    
    # å¦‚æœæ‰¾ä¸åˆ°æ•°å­—ï¼Œå°è¯•æ ¹æ®å…³é”®è¯åˆ¤æ–­
    response_lower = response_clean.lower()
    if any(word in response_lower for word in ['å®Œå…¨æˆåŠŸ', 'å®Œå…¨å®Œæˆ', 'fully successful', 'completely successful', '100%', '100 percent']):
        # print("âœ… Extracted score 1.0 from keywords")
        return 1.0
    elif any(word in response_lower for word in ['å®Œå…¨å¤±è´¥', 'å®Œå…¨æœªå®Œæˆ', 'completely failed', 'no progress', '0%', '0 percent', 'failed']):
        # print("âœ… Extracted score 0.0 from keywords")
        return 0.0
    elif any(word in response_lower for word in ['éƒ¨åˆ†', 'partial', 'some progress', '50%', '50 percent']):
        # print("âœ… Extracted score 0.5 from keywords")
        return 0.5
    
    # é»˜è®¤è¿”å› 0.5ï¼Œä½†æ‰“å°è­¦å‘Š
    if verbose:
        print(f"âš ï¸  Warning: Could not extract score from response. Using default 0.5")
        print(f"   Response: {response[:300]}")
    return 0.5


def evaluate_episode(
    model,  # ä½¿ç”¨åŠ¨æ€ç±»å‹ï¼Œå› ä¸ºå¯èƒ½æ˜¯ä¸åŒçš„æ¨¡å‹ç±»
    processor: AutoProcessor,
    images: List[Image.Image],
    task_description: str,
    prompt_template: str = None,
    device: str = "cuda",
    verbose: bool = True
) -> Dict[str, Any]:
    """
    è¯„ä¼°å•ä¸ªepisodeçš„ä»»åŠ¡å®Œæˆåº¦
    
    Args:
        model: Qwen2.5-VL æ¨¡å‹
        processor: å¤„ç†å™¨
        images: å›¾åƒåˆ—è¡¨ï¼ˆè§†è§‰åºåˆ—ï¼‰
        task_description: ä»»åŠ¡æè¿°
        prompt_template: æç¤ºæ¨¡æ¿ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼‰
        device: è®¾å¤‡
    
    Returns:
        åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    # å¦‚æœ prompt_template ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if prompt_template is None:
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è¯„ä¼°å™¨ã€‚è¯·è¯„ä¼°æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„å®Œæˆç¨‹åº¦ã€‚

ç›®æ ‡ä»»åŠ¡ï¼š{task_description}

ä¸‹é¢æ˜¯ä¸€ç³»åˆ—æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„è§†è§‰å›¾åƒï¼Œå±•ç¤ºäº†æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™äº›å›¾åƒï¼Œåˆ¤æ–­ä»»åŠ¡å®Œæˆçš„è¿›åº¦ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- è§‚å¯Ÿå›¾åƒåºåˆ—ä¸­çš„è§†è§‰å˜åŒ–
- åˆ¤æ–­æœºå™¨äººæ˜¯å¦åœ¨æœç€ç›®æ ‡å‰è¿›
- è¯„ä¼°ä»»åŠ¡å®Œæˆçš„ç™¾åˆ†æ¯”

è¯·è¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—åˆ†æ•°ï¼š
- 1.0 = ä»»åŠ¡å®Œå…¨æˆåŠŸï¼Œç›®æ ‡å·²è¾¾æˆ
- 0.8-0.9 = ä»»åŠ¡åŸºæœ¬å®Œæˆï¼Œæ¥è¿‘æˆåŠŸ
- 0.5-0.7 = ä»»åŠ¡éƒ¨åˆ†å®Œæˆï¼Œæœ‰ä¸€å®šè¿›å±•
- 0.2-0.4 = ä»»åŠ¡è¿›å±•å¾ˆå°ï¼Œåˆšå¼€å§‹
- 0.0-0.1 = ä»»åŠ¡å®Œå…¨å¤±è´¥ï¼Œæ²¡æœ‰è¿›å±•

è¯·åªè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚ï¼š0.75ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    
    # æ„å»ºå®Œæ•´çš„æç¤º
    full_prompt = prompt_template.format(task_description=task_description)
    
    # å‡†å¤‡å¯¹è¯æ ¼å¼ï¼ˆQwen2.5-VL ä½¿ç”¨çš„æ ¼å¼ï¼‰
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": img} for img in images
            ] + [
                {"type": "text", "text": full_prompt}
            ]
        }
    ]
    
    try:
        # æ­£ç¡®çš„æ–¹å¼ï¼šå…ˆä½¿ç”¨ apply_chat_template å¤„ç† messagesï¼Œè¿™ä¼šæ’å…¥å›¾åƒå ä½ç¬¦
        # ç„¶åä½¿ç”¨ processor å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # ä½¿ç”¨ processor å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
        # processor ä¼šè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„å›¾åƒå ä½ç¬¦ï¼Œå¹¶ç”¨å®é™…çš„å›¾åƒç‰¹å¾æ›¿æ¢
        inputs = processor(
            text=text,
            images=images,  # ç›´æ¥ä¼ å…¥å›¾åƒåˆ—è¡¨
            padding=True,
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        else:
            # å¦‚æœ inputs æ˜¯å­—å…¸ï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¯ä¸ªtensor
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False
            )
        
        # è§£ç å“åº” - åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
    except Exception as e:
        # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
        print(f"Warning: Standard method failed ({e}), trying alternative method...")
        try:
            # å¤‡ç”¨æ–¹æ³•ï¼šå°è¯•åªä¼ å…¥ç¬¬ä¸€å¼ å›¾åƒï¼ˆå¯èƒ½å¤šå›¾åƒæœ‰é—®é¢˜ï¼‰
            if len(images) > 1:
                print(f"Warning: Trying with single image instead of {len(images)} images...")
                # åªä½¿ç”¨ç¬¬ä¸€å¼ å’Œæœ€åä¸€å¼ å›¾åƒ
                single_images = [images[0], images[-1]] if len(images) > 1 else images
                messages_single = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": img} for img in single_images
                        ] + [
                            {"type": "text", "text": full_prompt}
                        ]
                    }
                ]
                text = processor.apply_chat_template(messages_single, tokenize=False, add_generation_prompt=True)
                inputs = processor(
                    text=text,
                    images=single_images,
                    padding=True,
                    return_tensors="pt"
                )
            else:
                # å¦‚æœåªæœ‰ä¸€å¼ å›¾åƒï¼Œç›´æ¥ä½¿ç”¨
                text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = processor(
                    text=text,
                    images=images,
                    padding=True,
                    return_tensors="pt"
                )
            
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False
                )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
        except Exception as e2:
            print(f"âŒ All methods failed. Last error: {e2}")
            import traceback
            traceback.print_exc()
            return {
                "score": None,
                "response": f"Error: {str(e2)}",
                "num_frames": len(images)
            }
    
    # æå–åˆ†æ•°
    if verbose:
        print(f"\nğŸ” Debug: Model raw response (first 500 chars): {response[:500]}")
        print(f"ğŸ” Debug: Number of images: {len(images)}")
    
    if response and not response.startswith("Error:"):
        score = extract_score_from_response(response, verbose=verbose)
        if verbose:
            print(f"ğŸ” Debug: Extracted score: {score}")
    else:
        # å¦‚æœæœ‰é”™è¯¯ï¼Œä¸æå–åˆ†æ•°
        score = None
        if verbose:
            print(f"âš ï¸  Warning: Could not get valid response. Response: {response[:200]}")
    
    # ä¿å­˜å›¾åƒä¿¡æ¯ï¼ˆåŒ…æ‹¬ç¼©ç•¥å›¾ï¼‰
    image_info = []
    for i, img in enumerate(images):
        # åˆ›å»ºç¼©ç•¥å›¾ï¼ˆæœ€å¤§å°ºå¯¸ 224x224ï¼Œä¿æŒå®½é«˜æ¯”ï¼‰
        thumbnail_size = (224, 224)
        img_thumbnail = img.copy()
        img_thumbnail.thumbnail(thumbnail_size, Image.Resampling.LANCZOS)
        
        # å°†ç¼©ç•¥å›¾è½¬æ¢ä¸º base64 ç¼–ç çš„å­—ç¬¦ä¸²
        buffer = io.BytesIO()
        img_thumbnail.save(buffer, format='PNG')
        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        image_info.append({
            "index": i,
            "size": img.size,  # (width, height) åŸå§‹å°ºå¯¸
            "mode": img.mode,  # RGB, etc.
            "format": img.format if hasattr(img, 'format') else None,
            "thumbnail_base64": f"data:image/png;base64,{img_base64}"  # base64 ç¼–ç çš„ç¼©ç•¥å›¾
        })
    
    # ä¿å­˜å¤„ç†åçš„æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒå ä½ç¬¦ï¼‰
    processed_text = text if 'text' in locals() else None
    
    return {
        "score": score,
        "response": response,
        "num_frames": len(images),
        "prompt": full_prompt,  # ä¿å­˜å®Œæ•´çš„æç¤ºæ–‡æœ¬
        "processed_text": processed_text,  # ä¿å­˜å¤„ç†åçš„æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒå ä½ç¬¦ï¼‰
        "images": image_info  # ä¿å­˜å›¾åƒå…ƒæ•°æ®å’Œç¼©ç•¥å›¾
    }


def process_dataset(
    dataset_path: str,
    model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct",
    camera_key: str = "observation.images.cam_head",
    task_description: str = None,
    prompt_template: str = None,
    episode: int = None,
    img_start_frame: int = None,
    img_end_frame: int = None,
    output_path: str = None,
    device: str = "auto"
):
    """
    å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œè¯„ä¼°æ¯ä¸ªepisodeçš„ä»»åŠ¡å®Œæˆåº¦
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        model_name: æ¨¡å‹åç§°
        camera_key: ç›¸æœºæ•°æ®é”®å
        task_description: ä»»åŠ¡æè¿°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»æ•°æ®é›†å…ƒæ•°æ®ä¸­è¯»å–ï¼‰
        prompt_template: æç¤ºæ¨¡æ¿
        episode: è¦å¤„ç†çš„episodeç´¢å¼•ï¼ˆNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰episodeï¼‰
        img_start_frame: å›¾åƒå¸§çš„èµ·å§‹ç´¢å¼•ï¼ˆç›¸å¯¹äºepisodeï¼ŒNoneè¡¨ç¤ºä»episodeå¼€å§‹ï¼‰
        img_end_frame: å›¾åƒå¸§çš„ç»“æŸç´¢å¼•ï¼ˆç›¸å¯¹äºepisodeï¼ŒNoneè¡¨ç¤ºåˆ°episodeç»“æŸï¼‰
        output_path: è¾“å‡ºç»“æœä¿å­˜è·¯å¾„
        device: è®¾å¤‡
    """
    # é»˜è®¤æç¤ºæ¨¡æ¿
    if prompt_template is None:
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è¯„ä¼°å™¨ã€‚è¯·è¯„ä¼°æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„å®Œæˆç¨‹åº¦ã€‚

ç›®æ ‡ä»»åŠ¡ï¼š{task_description}

ä¸‹é¢æ˜¯ä¸€ç³»åˆ—æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„è§†è§‰å›¾åƒï¼Œå±•ç¤ºäº†æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™äº›å›¾åƒï¼Œåˆ¤æ–­ä»»åŠ¡å®Œæˆçš„è¿›åº¦ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- è§‚å¯Ÿå›¾åƒåºåˆ—ä¸­çš„è§†è§‰å˜åŒ–
- åˆ¤æ–­æœºå™¨äººæ˜¯å¦åœ¨æœç€ç›®æ ‡å‰è¿›
- è¯„ä¼°ä»»åŠ¡å®Œæˆçš„ç™¾åˆ†æ¯”

è¯·è¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—åˆ†æ•°ï¼š
- 1.0 = ä»»åŠ¡å®Œå…¨æˆåŠŸï¼Œç›®æ ‡å·²è¾¾æˆ
- 0.8-0.9 = ä»»åŠ¡åŸºæœ¬å®Œæˆï¼Œæ¥è¿‘æˆåŠŸ
- 0.5-0.7 = ä»»åŠ¡éƒ¨åˆ†å®Œæˆï¼Œæœ‰ä¸€å®šè¿›å±•
- 0.2-0.4 = ä»»åŠ¡è¿›å±•å¾ˆå°ï¼Œåˆšå¼€å§‹
- 0.0-0.1 = ä»»åŠ¡å®Œå…¨å¤±è´¥ï¼Œæ²¡æœ‰è¿›å±•

è¯·åªè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚ï¼š0.75ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    
    # åŠ è½½æ•°æ®é›†
    print(f"Loading dataset from: {dataset_path}")
    dataset = LeRobotDataset(repo_id=0, root=dataset_path)
    
    print(f"Dataset loaded: {dataset.num_episodes} episodes, {dataset.num_frames} frames")
    print(f"Camera keys available: {dataset.meta.camera_keys}")
    
    # æ£€æŸ¥ç›¸æœºé”®æ˜¯å¦å­˜åœ¨
    if camera_key not in dataset.meta.camera_keys:
        print(f"Warning: {camera_key} not found in camera keys. Available keys: {dataset.meta.camera_keys}")
        # å°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ç›¸æœº
        if len(dataset.meta.camera_keys) > 0:
            camera_key = dataset.meta.camera_keys[0]
            print(f"Using {camera_key} instead")
        else:
            raise ValueError("No camera keys found in dataset")
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_model_and_processor(model_name, device)
    
    # ç¡®å®šè¦å¤„ç†çš„episodeåˆ—è¡¨
    if episode is not None:
        # å¤„ç†æŒ‡å®šçš„episode
        if episode < 0 or episode >= dataset.num_episodes:
            raise ValueError(f"Episode index {episode} is out of range [0, {dataset.num_episodes-1}]")
        episode_indices = [episode]
        print(f"Processing episode {episode} only")
    else:
        # å¤„ç†æ‰€æœ‰episode
        episode_indices = list(range(dataset.num_episodes))
        print(f"Processing all {dataset.num_episodes} episodes")
    
    # è·å–ä»»åŠ¡æè¿°
    if task_description is None:
        try:
            # å°è¯•ä»æ•°æ®é›†å…ƒæ•°æ®ä¸­è¯»å–ä»»åŠ¡
            if hasattr(dataset.meta, 'tasks') and len(dataset.meta.tasks) > 0:
                # tasks æ˜¯ä¸€ä¸ª DataFrameï¼Œä»»åŠ¡åç§°åœ¨ç´¢å¼•ä¸­
                if hasattr(dataset.meta.tasks, 'index') and len(dataset.meta.tasks.index) > 0:
                    # ä½¿ç”¨ç´¢å¼•ä¸­çš„ç¬¬ä¸€ä¸ªä»»åŠ¡åç§°
                    task_description = dataset.meta.tasks.index[0]
                elif hasattr(dataset.meta.tasks, 'iloc'):
                    # å°è¯•ä»ç¬¬ä¸€è¡Œè·å–
                    first_row = dataset.meta.tasks.iloc[0]
                    if hasattr(first_row, 'name'):
                        task_description = first_row.name
                    elif isinstance(first_row, dict) and 'task' in first_row:
                        task_description = first_row['task']
                    else:
                        task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
                else:
                    task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
                print(f"Using task description from dataset: {task_description}")
            else:
                # ä½¿ç”¨ç”¨æˆ·æä¾›çš„é»˜è®¤ä»»åŠ¡æè¿°
                task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
                print(f"Using default task description: {task_description}")
        except Exception as e:
            print(f"Warning: Failed to get task from dataset metadata: {e}")
            task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
            print(f"Using default task description: {task_description}")
    
    # å­˜å‚¨ç»“æœ
    results = []
    
    # å¤„ç†æ¯ä¸ªepisode
    for episode_idx in tqdm(episode_indices, desc="Processing episodes"):
        try:
            # è·å–episodeçš„å¸§èŒƒå›´
            ep_meta = dataset.meta.episodes[episode_idx]
            ep_start = ep_meta["dataset_from_index"]
            ep_end = ep_meta["dataset_to_index"]
            
            # è·å–ä»»åŠ¡æè¿°ï¼ˆå¦‚æœæœ‰å¤šä¸ªä»»åŠ¡ï¼‰
            ep_task = task_description
            if hasattr(dataset.meta, 'tasks') and 'task_index' in ep_meta:
                try:
                    task_idx = ep_meta.get('task_index', 0)
                    if hasattr(dataset.meta.tasks, 'index') and task_idx < len(dataset.meta.tasks.index):
                        # ä»ç´¢å¼•è·å–ä»»åŠ¡åç§°
                        ep_task = dataset.meta.tasks.index[task_idx]
                    elif hasattr(dataset.meta.tasks, 'iloc'):
                        first_row = dataset.meta.tasks.iloc[task_idx]
                        if hasattr(first_row, 'name'):
                            ep_task = first_row.name
                        elif isinstance(first_row, dict) and 'task' in first_row:
                            ep_task = first_row['task']
                except Exception as e:
                    print(f"Warning: Failed to get task for episode {episode_idx}: {e}")
                    ep_task = task_description
            
            # æ”¶é›†å›¾åƒ
            # ç¡®å®šå¸§èŒƒå›´ï¼ˆç›¸å¯¹äºepisodeçš„èµ·å§‹å¸§ï¼‰
            if img_start_frame is not None and img_end_frame is not None:
                # ä½¿ç”¨æŒ‡å®šçš„å¸§åŒºé—´ï¼ˆç›¸å¯¹äºepisodeå¼€å§‹ï¼‰
                if img_start_frame < 0 or img_end_frame < 0:
                    raise ValueError("img_start_frame and img_end_frame must be non-negative")
                if img_start_frame >= img_end_frame:
                    raise ValueError("img_start_frame must be less than img_end_frame")
                
                # è®¡ç®—å®é™…çš„å¸§ç´¢å¼•
                actual_start = ep_start + img_start_frame
                actual_end = ep_start + img_end_frame
                
                # ç¡®ä¿ä¸è¶…å‡ºepisodeèŒƒå›´
                actual_start = max(ep_start, actual_start)
                actual_end = min(ep_end, actual_end)
                
                if actual_start >= actual_end:
                    raise ValueError(f"Invalid frame range: start={actual_start}, end={actual_end} (episode range: {ep_start}-{ep_end})")
                
                indices = range(actual_start, actual_end)
                print(f"  Using frames {img_start_frame} to {img_end_frame} (absolute: {actual_start} to {actual_end-1})")
            elif img_start_frame is not None:
                # åªæŒ‡å®šäº†èµ·å§‹å¸§
                actual_start = ep_start + img_start_frame
                actual_start = max(ep_start, actual_start)
                actual_end = ep_end
                indices = range(actual_start, actual_end)
                print(f"  Using frames from {img_start_frame} to end (absolute: {actual_start} to {actual_end-1})")
            elif img_end_frame is not None:
                # åªæŒ‡å®šäº†ç»“æŸå¸§
                actual_start = ep_start
                actual_end = ep_start + img_end_frame
                actual_end = min(ep_end, actual_end)
                indices = range(actual_start, actual_end)
                print(f"  Using frames from start to {img_end_frame} (absolute: {actual_start} to {actual_end-1})")
            else:
                # ä½¿ç”¨æ‰€æœ‰å¸§
                indices = range(ep_start, ep_end)
                print(f"  Using all frames (absolute: {ep_start} to {ep_end-1})")
            
            images = []
            for idx in indices:
                try:
                    frame_data = dataset[idx]
                    if camera_key in frame_data:
                        img_tensor = frame_data[camera_key]
                        # è½¬æ¢ä¸º PIL Image
                        img = tensor_to_pil_image(img_tensor)
                        images.append(img)
                except Exception as e:
                    print(f"Warning: Failed to load frame {idx}: {e}")
                    continue
            
            if len(images) == 0:
                print(f"Warning: No images loaded for episode {episode_idx}")
                results.append({
                    "episode": episode_idx,
                    "score": None,
                    "error": "No images loaded",
                    "num_frames": 0,
                    "prompt": prompt_template.format(task_description=ep_task),
                    "images": []
                })
                continue
            
            # è¯„ä¼°episode
            result = evaluate_episode(
                model=model,
                processor=processor,
                images=images,
                task_description=ep_task,
                prompt_template=prompt_template,
                device=next(model.parameters()).device
            )
            
            result["episode"] = episode_idx
            result["task"] = ep_task
            results.append(result)
            
            if result['score'] is not None:
                print(f"Episode {episode_idx}: Score = {result['score']:.3f}, Frames = {result['num_frames']}")
                print(f"  Prompt: {result.get('prompt', 'N/A')[:100]}...")
                print(f"  Images used: {result.get('num_frames', 0)} images")
            else:
                print(f"Episode {episode_idx}: Failed to get score. Response: {result['response'][:100]}...")
                print(f"  Prompt: {result.get('prompt', 'N/A')[:100]}...")
                print(f"  Images used: {result.get('num_frames', 0)} images")
            
        except Exception as e:
            print(f"Error processing episode {episode_idx}: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "episode": episode_idx,
                "score": None,
                "error": str(e),
                "num_frames": 0,
                "prompt": prompt_template.format(task_description=ep_task) if 'ep_task' in locals() else None,
                "images": []
            })
    
    # ä¿å­˜ç»“æœ
    if output_path is None:
        output_path = os.path.join(dataset_path, "reward_scores.json")
    
    output_data = {
        "dataset_path": dataset_path,
        "model_name": model_name,
        "camera_key": camera_key,
        "task_description": task_description,
        "num_episodes": len(results),
        "results": results,
        "statistics": {
            "mean_score": np.mean([r["score"] for r in results if r["score"] is not None]) if len([r for r in results if r["score"] is not None]) > 0 else None,
            "std_score": np.std([r["score"] for r in results if r["score"] is not None]) if len([r for r in results if r["score"] is not None]) > 1 else 0.0,
            "min_score": np.min([r["score"] for r in results if r["score"] is not None]) if len([r for r in results if r["score"] is not None]) > 0 else None,
            "max_score": np.max([r["score"] for r in results if r["score"] is not None]) if len([r for r in results if r["score"] is not None]) > 0 else None,
            "successful_episodes": len([r for r in results if r["score"] is not None]),
            "failed_episodes": len([r for r in results if r["score"] is None]),
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆ HTML å¯è§†åŒ–æŠ¥å‘Š
    html_path = output_path.replace('.json', '.html')
    generate_html_report(output_data, html_path)
    
    print(f"\nResults saved to: {output_path}")
    print(f"HTML report saved to: {html_path}")
    print(f"Statistics:")
    if output_data['statistics']['mean_score'] is not None:
        print(f"  Mean score: {output_data['statistics']['mean_score']:.3f}")
        print(f"  Std score: {output_data['statistics']['std_score']:.3f}")
        print(f"  Min score: {output_data['statistics']['min_score']:.3f}")
        print(f"  Max score: {output_data['statistics']['max_score']:.3f}")
    else:
        print("  âš ï¸  No successful evaluations!")
    print(f"  Successful episodes: {output_data['statistics']['successful_episodes']}")
    print(f"  Failed episodes: {output_data['statistics']['failed_episodes']}")
    
    return results


def create_reward_visualization_video(
    images: List[Image.Image],
    scores: List[float],
    frame_indices: List[int],
    output_path: str,
    fps: int = 5,
    task_description: str = "Task"
) -> str:
    """
    åˆ›å»ºåŒ…å«å›¾åƒåºåˆ—å’Œåˆ†æ•°æ›²çº¿çš„MP4è§†é¢‘
    
    Args:
        images: å›¾åƒåˆ—è¡¨
        scores: å¯¹åº”çš„åˆ†æ•°åˆ—è¡¨
        frame_indices: å¸§ç´¢å¼•åˆ—è¡¨
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        fps: è§†é¢‘å¸§ç‡
        task_description: ä»»åŠ¡æè¿°
    
    Returns:
        è¾“å‡ºè§†é¢‘è·¯å¾„
    """
    if not CV2_AVAILABLE:
        raise ImportError("OpenCV is required for video generation. Install with: pip install opencv-python")
    
    if len(images) != len(scores):
        raise ValueError(f"Number of images ({len(images)}) must match number of scores ({len(scores)})")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜å¸§
    temp_dir = Path(output_path).parent / "temp_frames"
    temp_dir.mkdir(exist_ok=True)
    
    # å‡†å¤‡åˆ†æ•°æ›²çº¿æ•°æ®
    # ç¡®ä¿åˆ†æ•°åˆ—è¡¨ä¸å›¾åƒå¯¹é½
    score_timeline = scores.copy()
    
    # åˆ›å»ºå›¾å½¢ç”¨äºç»˜åˆ¶åˆ†æ•°æ›²çº¿
    fig_width = 12
    fig_height = 6
    dpi = 100
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = None
    
    print(f"Creating reward visualization video with {len(images)} frames...")
    
    try:
        for i, (img, score) in enumerate(tqdm(zip(images, scores), total=len(images), desc="Generating video frames")):
            # åˆ›å»ºå›¾å½¢
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(fig_width, fig_height))
            fig.suptitle(f'Reward Visualization - Frame {frame_indices[i] if i < len(frame_indices) else i}', 
                        fontsize=14, fontweight='bold')
            
            # å·¦ä¾§ï¼šæ˜¾ç¤ºå½“å‰å›¾åƒ
            ax1.imshow(img)
            ax1.axis('off')
            ax1.set_title(f'Current Frame\nScore: {score:.3f}', fontsize=12, fontweight='bold')
            
            # å³ä¾§ï¼šç»˜åˆ¶åˆ†æ•°æ›²çº¿
            current_scores = score_timeline[:i+1]
            current_frames = frame_indices[:i+1] if len(frame_indices) > i else list(range(i+1))
            
            ax2.plot(current_frames, current_scores, 'b-', linewidth=2, label='Reward Score')
            ax2.scatter(current_frames[-1], current_scores[-1], color='red', s=100, zorder=5, 
                       label=f'Current: {current_scores[-1]:.3f}')
            ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Baseline (0.5)')
            ax2.fill_between(current_frames, current_scores, 0.5, where=[s >= 0.5 for s in current_scores], 
                           alpha=0.3, color='green', label='Above Baseline')
            ax2.fill_between(current_frames, current_scores, 0.5, where=[s < 0.5 for s in current_scores], 
                           alpha=0.3, color='red', label='Below Baseline')
            
            ax2.set_xlabel('Frame Index', fontsize=11)
            ax2.set_ylabel('Reward Score', fontsize=11)
            ax2.set_title('Reward Score Timeline', fontsize=12, fontweight='bold')
            ax2.set_ylim(0, 1)
            ax2.grid(True, alpha=0.3)
            ax2.legend(loc='best', fontsize=9)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if len(current_scores) > 0:
                mean_score = np.mean(current_scores)
                max_score = np.max(current_scores)
                min_score = np.min(current_scores)
                stats_text = f'Mean: {mean_score:.3f}\nMax: {max_score:.3f}\nMin: {min_score:.3f}'
                ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, 
                        fontsize=9, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            plt.tight_layout()
            
            # ä¿å­˜ä¸ºå›¾åƒ
            frame_path = temp_dir / f"frame_{i:05d}.png"
            plt.savefig(frame_path, dpi=dpi, bbox_inches='tight')
            plt.close(fig)
            
            # è¯»å–ä¿å­˜çš„å¸§
            frame_img = cv2.imread(str(frame_path))
            if frame_img is None:
                print(f"Warning: Failed to read frame {i}")
                continue
            
            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆä½¿ç”¨ç¬¬ä¸€å¸§çš„å°ºå¯¸ï¼‰
            if video_writer is None:
                height, width = frame_img.shape[:2]
                video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            video_writer.write(frame_img)
        
        if video_writer is not None:
            video_writer.release()
            print(f"âœ… Video saved to: {output_path}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        
        return output_path
        
    except Exception as e:
        if video_writer is not None:
            video_writer.release()
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise e


def evaluate_episode_with_sliding_window(
    model,
    processor: AutoProcessor,
    dataset,
    episode_idx: int,
    camera_key: str,
    task_description: str,
    prompt_template: str = None,
    window_size: int = 20,
    stride: int = 1,
    device: str = "cuda"
) -> Tuple[List[float], List[Image.Image], List[int]]:
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£è¯„ä¼°episodeï¼Œæ¯æ¬¡è¯„ä¼°window_sizeå¸§
    
    Args:
        model: Qwen2.5-VL æ¨¡å‹
        processor: å¤„ç†å™¨
        dataset: LeRobotDataset
        episode_idx: episodeç´¢å¼•
        camera_key: ç›¸æœºæ•°æ®é”®å
        task_description: ä»»åŠ¡æè¿°
        prompt_template: æç¤ºæ¨¡æ¿ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼‰
        window_size: çª—å£å¤§å°ï¼ˆæ¯æ¬¡è¯„ä¼°çš„å¸§æ•°ï¼‰
        stride: æ»‘åŠ¨æ­¥é•¿
        device: è®¾å¤‡
    
    Returns:
        (scores, images, frame_indices): åˆ†æ•°åˆ—è¡¨ã€å›¾åƒåˆ—è¡¨ã€å¸§ç´¢å¼•åˆ—è¡¨
    """
    # å¦‚æœ prompt_template ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if prompt_template is None:
        prompt_template = """ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è¯„ä¼°å™¨ã€‚è¯·è¯„ä¼°æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„å®Œæˆç¨‹åº¦ã€‚

ç›®æ ‡ä»»åŠ¡ï¼š{task_description}

ä¸‹é¢æ˜¯ä¸€ç³»åˆ—æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„è§†è§‰å›¾åƒï¼Œå±•ç¤ºäº†æœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„è¿‡ç¨‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¿™äº›å›¾åƒï¼Œåˆ¤æ–­ä»»åŠ¡å®Œæˆçš„è¿›åº¦ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- è§‚å¯Ÿå›¾åƒåºåˆ—ä¸­çš„è§†è§‰å˜åŒ–
- åˆ¤æ–­æœºå™¨äººæ˜¯å¦åœ¨æœç€ç›®æ ‡å‰è¿›
- è¯„ä¼°ä»»åŠ¡å®Œæˆçš„ç™¾åˆ†æ¯”

è¯·è¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—åˆ†æ•°ï¼š
- 1.0 = ä»»åŠ¡å®Œå…¨æˆåŠŸï¼Œç›®æ ‡å·²è¾¾æˆ
- 0.8-0.9 = ä»»åŠ¡åŸºæœ¬å®Œæˆï¼Œæ¥è¿‘æˆåŠŸ
- 0.5-0.7 = ä»»åŠ¡éƒ¨åˆ†å®Œæˆï¼Œæœ‰ä¸€å®šè¿›å±•
- 0.2-0.4 = ä»»åŠ¡è¿›å±•å¾ˆå°ï¼Œåˆšå¼€å§‹
- 0.0-0.1 = ä»»åŠ¡å®Œå…¨å¤±è´¥ï¼Œæ²¡æœ‰è¿›å±•

è¯·åªè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚ï¼š0.75ï¼‰ï¼Œä¸è¦è¾“å‡ºå…¶ä»–æ–‡å­—ã€‚"""
    
    # è·å–episodeçš„å¸§èŒƒå›´
    ep_meta = dataset.meta.episodes[episode_idx]
    ep_start = ep_meta["dataset_from_index"]
    ep_end = ep_meta["dataset_to_index"]
    num_frames = ep_end - ep_start
    
    print(f"Episode {episode_idx}: {num_frames} frames, using sliding window (size={window_size}, stride={stride})")
    
    scores = []
    all_images = []
    frame_indices = []
    
    # è®¡ç®—éœ€è¦å¤„ç†çš„çª—å£æ•°é‡
    num_windows = (num_frames - window_size) // stride + 1
    print(f"Total windows to process: {num_windows}")
    
    # æ»‘åŠ¨çª—å£å¤„ç†ï¼ˆä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼‰
    for window_idx, window_start in enumerate(tqdm(range(0, num_frames - window_size + 1, stride), 
                                                    desc=f"Sliding window evaluation", 
                                                    total=num_windows)):
        window_end = window_start + window_size
        window_indices = range(ep_start + window_start, ep_start + window_end)
        
        # æ”¶é›†çª—å£å†…çš„å›¾åƒ
        window_images = []
        for idx in window_indices:
            try:
                frame_data = dataset[idx]
                if camera_key in frame_data:
                    img_tensor = frame_data[camera_key]
                    img = tensor_to_pil_image(img_tensor)
                    window_images.append(img)
            except Exception as e:
                print(f"Warning: Failed to load frame {idx}: {e}")
                continue
        
        if len(window_images) == 0:
            print(f"Warning: No images in window [{window_start}, {window_end})")
            continue
        
        # è¯„ä¼°å½“å‰çª—å£ï¼ˆå‡å°‘è°ƒè¯•è¾“å‡ºï¼Œåªåœ¨æ¯10ä¸ªçª—å£æˆ–æœ€åä¸€ä¸ªçª—å£æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰
        verbose = (window_idx % 10 == 0) or (window_idx == num_windows - 1)
        
        result = evaluate_episode(
            model=model,
            processor=processor,
            images=window_images,
            task_description=task_description,
            prompt_template=prompt_template,
            device=device,
            verbose=verbose
        )
        
        score = result.get('score')
        if score is not None:
            scores.append(score)
            # ä½¿ç”¨çª—å£çš„æœ€åä¸€å¸§ä½œä¸ºä»£è¡¨å›¾åƒ
            all_images.append(window_images[-1])
            frame_indices.append(ep_start + window_end - 1)
            
            # åªåœ¨ verbose æ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
            if verbose:
                print(f"Window [{window_start}-{window_end-1}]: Score = {score:.3f}")
        else:
            print(f"Warning: Failed to get score for window [{window_start}, {window_end})")
    
    print(f"\nâœ… Completed sliding window evaluation: {len(scores)} scores collected")
    return scores, all_images, frame_indices


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Qwen2.5-VL è¯„ä¼° lerobot æ•°æ®é›†çš„ä»»åŠ¡å®Œæˆåº¦")
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1125_groot_train_data_with_task_filtered",
        help="æ•°æ®é›†è·¯å¾„"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-VL-7B-Instruct",
        help="æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--camera_key",
        type=str,
        default="observation.images.cam_head",
        help="ç›¸æœºæ•°æ®é”®å"
    )
    parser.add_argument(
        "--task_description",
        type=str,
        default=None,
        help="ä»»åŠ¡æè¿°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»æ•°æ®é›†å…ƒæ•°æ®ä¸­è¯»å–ï¼‰"
    )
    parser.add_argument(
        "--episode",
        type=int,
        default=None,
        help="è¦å¤„ç†çš„episodeç´¢å¼•ï¼ˆNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰episodeï¼‰"
    )
    parser.add_argument(
        "--img_start_frame",
        type=int,
        default=None,
        help="å›¾åƒå¸§çš„èµ·å§‹ç´¢å¼•ï¼ˆç›¸å¯¹äºepisodeå¼€å§‹ï¼ŒNoneè¡¨ç¤ºä»episodeå¼€å§‹ï¼‰"
    )
    parser.add_argument(
        "--img_end_frame",
        type=int,
        default=None,
        help="å›¾åƒå¸§çš„ç»“æŸç´¢å¼•ï¼ˆç›¸å¯¹äºepisodeå¼€å§‹ï¼ŒNoneè¡¨ç¤ºåˆ°episodeç»“æŸï¼‰"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default=None,
        help="è¾“å‡ºç»“æœä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ä¿å­˜åˆ°æ•°æ®é›†ç›®å½•ï¼‰"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¾å¤‡ï¼ˆauto, cuda, cpuï¼‰"
    )
    parser.add_argument(
        "--sliding_window",
        action="store_true",
        help="ä½¿ç”¨æ»‘åŠ¨çª—å£æ¨¡å¼ï¼ˆæ¯æ¬¡è¯„ä¼°20å¸§ï¼Œç”Ÿæˆrewardæ›²çº¿è§†é¢‘ï¼‰"
    )
    parser.add_argument(
        "--window_size",
        type=int,
        default=20,
        help="æ»‘åŠ¨çª—å£å¤§å°ï¼ˆé»˜è®¤20å¸§ï¼‰"
    )
    parser.add_argument(
        "--stride",
        type=int,
        default=1,
        help="æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆé»˜è®¤1å¸§ï¼‰"
    )
    parser.add_argument(
        "--video_fps",
        type=int,
        default=5,
        help="è¾“å‡ºè§†é¢‘çš„å¸§ç‡ï¼ˆé»˜è®¤5fpsï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    if args.sliding_window:
        # æ»‘åŠ¨çª—å£æ¨¡å¼ï¼šç”Ÿæˆrewardå¯è§†åŒ–è§†é¢‘
        if args.episode is None:
            raise ValueError("--episode is required when using --sliding_window mode")
        
        if not CV2_AVAILABLE:
            raise ImportError("OpenCV is required for video generation. Install with: pip install opencv-python")
        
        # åŠ è½½æ•°æ®é›†
        print(f"Loading dataset from: {args.dataset_path}")
        dataset = LeRobotDataset(repo_id=0, root=args.dataset_path)
        
        # æ£€æŸ¥episodeèŒƒå›´
        if args.episode < 0 or args.episode >= dataset.num_episodes:
            raise ValueError(f"Episode index {args.episode} is out of range [0, {dataset.num_episodes-1}]")
        
        # è·å–ä»»åŠ¡æè¿°
        task_description = args.task_description
        if task_description is None:
            try:
                if hasattr(dataset.meta, 'tasks') and len(dataset.meta.tasks) > 0:
                    # tasks æ˜¯ä¸€ä¸ª DataFrameï¼Œä»»åŠ¡åç§°åœ¨ç´¢å¼•ä¸­
                    if hasattr(dataset.meta.tasks, 'iloc'):
                        # å°è¯•ä»ç´¢å¼•è·å–ä»»åŠ¡åç§°
                        task_description = dataset.meta.tasks.index[0]
                    elif hasattr(dataset.meta.tasks, 'index') and len(dataset.meta.tasks.index) > 0:
                        task_description = dataset.meta.tasks.index[0]
                    else:
                        # å¦‚æœç´¢å¼•ä¸å¯ç”¨ï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œè·å–
                        first_row = dataset.meta.tasks.iloc[0] if hasattr(dataset.meta.tasks, 'iloc') else dataset.meta.tasks[0]
                        if hasattr(first_row, 'name'):
                            task_description = first_row.name
                        elif isinstance(first_row, dict) and 'task' in first_row:
                            task_description = first_row['task']
                        else:
                            task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
                else:
                    task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
            except Exception as e:
                print(f"Warning: Failed to get task from dataset metadata: {e}")
                task_description = "æŠŠç°è‰²ç®±å­æ‹‰å¼€ï¼Œå¹¶ä¸”æœºæ¢°è‡‚åŒè¾¹å¤¹çˆªæŠŠç®±å­æŠ¬èµ·æ¥"
        
        print(f"Using task description: {task_description}")
        
        # åŠ è½½æ¨¡å‹
        model, processor = load_model_and_processor(args.model_name, args.device)
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£è¯„ä¼°
        scores, images, frame_indices = evaluate_episode_with_sliding_window(
            model=model,
            processor=processor,
            dataset=dataset,
            episode_idx=args.episode,
            camera_key=args.camera_key,
            task_description=task_description,
            prompt_template=None,
            window_size=args.window_size,
            stride=args.stride,
            device=next(model.parameters()).device
        )
        
        # ç”Ÿæˆè§†é¢‘
        if args.output_path is None:
            video_path = f"./reward_episode_{args.episode}_visualization.mp4"
        else:
            video_path = args.output_path.replace('.json', '.mp4')
        
        create_reward_visualization_video(
            images=images,
            scores=scores,
            frame_indices=frame_indices,
            output_path=video_path,
            fps=args.video_fps,
            task_description=task_description
        )
        
        # ä¿å­˜åˆ†æ•°æ•°æ®
        json_path = video_path.replace('.mp4', '_scores.json')
        score_data = {
            "episode": args.episode,
            "task_description": task_description,
            "window_size": args.window_size,
            "stride": args.stride,
            "num_windows": len(scores),
            "scores": scores,
            "frame_indices": frame_indices,
            "statistics": {
                "mean_score": np.mean(scores) if len(scores) > 0 else None,
                "std_score": np.std(scores) if len(scores) > 1 else 0.0,
                "min_score": np.min(scores) if len(scores) > 0 else None,
                "max_score": np.max(scores) if len(scores) > 0 else None,
            }
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(score_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Reward visualization video saved to: {video_path}")
        print(f"âœ… Score data saved to: {json_path}")
        print(f"Statistics:")
        if score_data['statistics']['mean_score'] is not None:
            print(f"  Mean score: {score_data['statistics']['mean_score']:.3f}")
            print(f"  Std score: {score_data['statistics']['std_score']:.3f}")
            print(f"  Min score: {score_data['statistics']['min_score']:.3f}")
            print(f"  Max score: {score_data['statistics']['max_score']:.3f}")
    else:
        # æ­£å¸¸æ¨¡å¼
        process_dataset(
            dataset_path=args.dataset_path,
            model_name=args.model_name,
            camera_key=args.camera_key,
            task_description=args.task_description,
            episode=args.episode,
            img_start_frame=args.img_start_frame,
            img_end_frame=args.img_end_frame,
            output_path=args.output_path,
            device=args.device
        )


if __name__ == "__main__":
    main()
