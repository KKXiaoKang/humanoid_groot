#!/usr/bin/env python3
"""
å¼‚æ­¥æ¨ç†æ•°æ®é›†è¯„ä¼°è„šæœ¬

è¿™ä¸ªè„šæœ¬ä½¿ç”¨å¼‚æ­¥æ¨ç†æ¶æ„ï¼ˆclient-serveræ¨¡å¼ï¼‰åœ¨LeRobotæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
ä¸eval_on_dataset_lowpass.pyä¸åŒï¼Œè¿™ä¸ªè„šæœ¬ï¼š
- ä½¿ç”¨å¼‚æ­¥æ¨ç†æ¶æ„ï¼ˆpolicy_server + clientï¼‰
- ä¸è¿›è¡Œchunkå†…éƒ¨çº¿æ€§æ’å€¼
- ä¸è¿›è¡Œchunké—´ä½é€šæ»¤æ³¢
- ç›´æ¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„actionè¿›è¡Œè¯¯å·®è®¡ç®—

ä½¿ç”¨æ–¹æ³•ï¼š
1. å¯åŠ¨æœåŠ¡å™¨ï¼š
   python -m lerobot.async_inference.policy_server \
       --host=127.0.0.1 \
       --port=8080 \
       --fps=30 \
       --inference_latency=0.033

2. è¿è¡Œå®¢æˆ·ç«¯ï¼ˆæœ¬è„šæœ¬ï¼‰ï¼š
   python scripts/eval_async_dataset_base.py \
       --server_address=127.0.0.1:8080 \
       --ckpt_path=/path/to/checkpoint \
       --dataset-root=/path/to/dataset \
       --episode=0 \
       --action-chunk-size=16
"""

import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# ç¡®ä¿localhostè¿æ¥ç»•è¿‡ä»£ç†ï¼ˆgRPCè¿æ¥ä¸åº”è¯¥ç»è¿‡HTTPä»£ç†ï¼‰
# è®¾ç½®NO_PROXYç¯å¢ƒå˜é‡ï¼Œè®©localhostå’Œ127.0.0.1ç»•è¿‡ä»£ç†
os.environ.setdefault("NO_PROXY", "localhost,127.0.0.1,0.0.0.0")
os.environ.setdefault("no_proxy", "localhost,127.0.0.1,0.0.0.0")

import time
import threading
import pickle
import argparse
from queue import Queue
from collections import OrderedDict
from typing import Optional
from pathlib import Path

import numpy as np
import torch
import grpc
from tqdm import tqdm

from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.transport import (
    services_pb2,  # type: ignore
    services_pb2_grpc,  # type: ignore
)
from lerobot.transport.utils import grpc_channel_options, send_bytes_in_chunks
from lerobot.async_inference.helpers import (
    TimedObservation,
    TimedAction,
    RawObservation,
    RemotePolicyConfig,
    get_logger,
)

# å¯¼å…¥é…ç½®æ¨¡å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from configs.config import (
        topic_info, TASK_DATA_MODE, get_camera_observation_key, 
        get_camera_names, CAMERA_COMPONENTS, action_names, CAMERA_KEY_MAPPING
    )
    CONFIG_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: configs.config not available. Using defaults.")
    CONFIG_AVAILABLE = False
    topic_info = {}
    TASK_DATA_MODE = "unknown"
    CAMERA_COMPONENTS = []
    action_names = []
    CAMERA_KEY_MAPPING = {}
    def get_camera_observation_key(camera_name, use_image_features=False):
        return f"observation.images.{camera_name}"
    def get_camera_names(camera_components=None):
        return []

# å¯é€‰çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™ç¦ç”¨ï¼‰
try:
    from scripts.visualization_tools.visualizers import RerunVisualizer, KeyboardManager
    RERUN_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: RerunVisualizer not available. Visualization will be disabled.")
    RERUN_AVAILABLE = False
    RerunVisualizer = None
    KeyboardManager = None

logger = get_logger("async_dataset_eval_client")


class DatasetAsyncClient:
    """å¼‚æ­¥æ¨ç†å®¢æˆ·ç«¯ï¼Œç”¨äºåœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    
    def __init__(
        self,
        server_address: str,
        ckpt_path: str,
        dataset: LeRobotDataset,
        action_chunk_size: int = 16,
        task_description: Optional[str] = None,
        fps: float = 30.0,
        chunk_size_threshold: float = 0.5,
    ):
        """
        Args:
            server_address: gRPC æœåŠ¡å™¨åœ°å€ (æ ¼å¼: "host:port")
            ckpt_path: GROOT æ¨¡å‹ checkpoint è·¯å¾„
            dataset: LeRobotDataset å®ä¾‹
            action_chunk_size: åŠ¨ä½œå—å¤§å°
            task_description: ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ï¼ˆå¦‚æœæä¾›åˆ™è¦†ç›–æ•°æ®é›†ä¸­çš„taskï¼‰
            fps: æ§åˆ¶é¢‘ç‡
            chunk_size_threshold: åŠ¨ä½œé˜Ÿåˆ—é˜ˆå€¼
        """
        self.server_address = server_address
        self.ckpt_path = ckpt_path
        self.dataset = dataset
        self.action_chunk_size = action_chunk_size
        self.task_description = task_description
        self.fps = fps
        self.environment_dt = 1.0 / fps
        self.chunk_size_threshold = chunk_size_threshold
        
        # è·å–æ•°æ®é›†ç‰¹å¾æ˜ å°„ï¼ˆç”¨äºæœåŠ¡å™¨ç«¯è½¬æ¢ï¼‰
        self.lerobot_features = self._get_lerobot_features_from_dataset()
        
        # è¿æ¥ gRPC æœåŠ¡å™¨
        logger.info(f"Connecting to policy server at {server_address}...")
        self.channel = grpc.insecure_channel(
            server_address,
            grpc_channel_options(initial_backoff=f"{self.environment_dt:.4f}s")
        )
        self.stub = services_pb2_grpc.AsyncInferenceStub(self.channel)
        
        # å‘é€ Ready ä¿¡å·
        try:
            self.stub.Ready(services_pb2.Empty())
            logger.info("Connected to policy server")
        except grpc.RpcError as e:
            logger.error(f"Failed to connect to server: {e}")
            raise
        
        # å‘é€ç­–ç•¥é…ç½®
        policy_config = RemotePolicyConfig(
            policy_type="groot",
            pretrained_name_or_path=ckpt_path,
            lerobot_features=self.lerobot_features,
            actions_per_chunk=action_chunk_size,
            device="cuda:0" if torch.cuda.is_available() else "cpu",
        )
        policy_config_bytes = pickle.dumps(policy_config)
        policy_setup = services_pb2.PolicySetup(data=policy_config_bytes)
        self.stub.SendPolicyInstructions(policy_setup)
        logger.info("Policy configuration sent to server")
        
        # åŠ¨ä½œé˜Ÿåˆ—å’ŒåŒæ­¥
        self.action_queue = Queue()
        self.action_queue_lock = threading.Lock()
        self.latest_action = -1
        self.latest_action_lock = threading.Lock()
        self.action_chunk_size_received = action_chunk_size
        
        # çº¿ç¨‹æ§åˆ¶
        self.shutdown_event = threading.Event()
        self.must_go = threading.Event()
        self.must_go.set()  # åˆå§‹è®¾ç½®ä¸ºå¯å‘é€è§‚æµ‹
        
        logger.info("DatasetAsyncClient initialized")
    
    def _get_lerobot_features_from_dataset(self) -> dict[str, dict]:
        """ä»æ•°æ®é›†è·å–LeRobotç‰¹å¾æ˜ å°„
        
        è¿”å›æ ¼å¼: dict[str, dict] - LeRobot æ•°æ®é›†ç‰¹å¾æ ¼å¼ï¼ˆå¸¦ "observation." å‰ç¼€ï¼‰
        """
        # ä»æ•°æ®é›†metaä¸­è·å–ç‰¹å¾å®šä¹‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        features = {}
        
        # å°è¯•ä»æ•°æ®é›†metaä¸­è·å–ç‰¹å¾å®šä¹‰
        if hasattr(self.dataset, 'meta') and hasattr(self.dataset.meta, 'features'):
            # å¦‚æœæœ‰meta.featuresï¼Œç›´æ¥ä½¿ç”¨ï¼Œä½†éœ€è¦å¤„ç†å­—å…¸æ ¼å¼çš„names
            for key, feat_def in self.dataset.meta.features.items():
                if key.startswith("observation"):
                    # å¤åˆ¶ç‰¹å¾å®šä¹‰
                    feat_def_copy = feat_def.copy()
                    
                    # å¦‚æœnamesæ˜¯å­—å…¸æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆbuild_dataset_frameæœŸæœ›åˆ—è¡¨ï¼‰
                    if "names" in feat_def_copy and isinstance(feat_def_copy["names"], dict):
                        # å°†å­—å…¸æ ¼å¼çš„namesæ‰å¹³åŒ–ä¸ºåˆ—è¡¨
                        # ä¾‹å¦‚: {"motors": ["arm_joint_1", ...]} -> ["arm_joint_1", ...]
                        state_names = []
                        for dict_key, value_list in feat_def_copy["names"].items():
                            if isinstance(value_list, list):
                                state_names.extend(value_list)
                            else:
                                state_names.append(value_list)
                        feat_def_copy["names"] = state_names
                        logger.debug(f"Converted dictionary format names to list for {key}: {len(state_names)} names")
                    
                    features[key] = feat_def_copy
            logger.info(f"Loaded lerobot features from dataset meta: {list(features.keys())}")
            return features
        
        # å¦‚æœæ²¡æœ‰meta.featuresï¼Œä»æ•°æ®ä¸­æ¨æ–­
        # æ·»åŠ stateç‰¹å¾
        if "observation.state" in self.dataset.hf_dataset.column_names:
            sample_state = self.dataset[0]["observation.state"]
            if isinstance(sample_state, torch.Tensor):
                state_dim = sample_state.shape[0] if sample_state.ndim == 1 else sample_state.shape[-1]
            else:
                state_dim = len(sample_state)
            
            features["observation.state"] = {
                "dtype": "float32",
                "shape": (state_dim,),
                "names": [f"state_{i}" for i in range(state_dim)],
            }
        
        # æ·»åŠ å›¾åƒç‰¹å¾
        for key in self.dataset.hf_dataset.column_names:
            if key.startswith("observation.images."):
                sample_img = self.dataset[0][key]
                if isinstance(sample_img, torch.Tensor):
                    if sample_img.ndim == 3:  # (C, H, W)
                        img_shape = sample_img.shape
                    elif sample_img.ndim == 4:  # (B, C, H, W)
                        img_shape = sample_img.shape[1:]
                    else:
                        continue
                else:
                    continue
                
                # è½¬æ¢ä¸º(H, W, C)æ ¼å¼ï¼ˆæœåŠ¡å™¨ç«¯æœŸæœ›çš„æ ¼å¼ï¼‰
                if len(img_shape) == 3:  # (C, H, W)
                    img_shape_hwc = (img_shape[1], img_shape[2], img_shape[0])  # (H, W, C)
                else:
                    img_shape_hwc = img_shape
                
                features[key] = {
                    "dtype": "image",
                    "shape": img_shape_hwc,  # (H, W, C)
                    "names": ["height", "width", "channels"],
                }
        
        logger.info(f"Extracted lerobot features from data: {list(features.keys())}")
        return features
    
    def _convert_dataset_obs_to_raw_obs(self, batch: dict, frame_idx: int) -> RawObservation:
        """å°†æ•°æ®é›†ä¸­çš„è§‚æµ‹è½¬æ¢ä¸ºRawObservationæ ¼å¼
        
        Args:
            batch: ä»DataLoaderè·å–çš„batchï¼ˆbatch_size=1ï¼‰
            frame_idx: å½“å‰å¸§ç´¢å¼•
            
        Returns:
            RawObservation: åŸå§‹è§‚æµ‹å­—å…¸ï¼Œæ ¼å¼ï¼š
                - çŠ¶æ€ç»„ä»¶ï¼šé”®åä¸ºlerobot_featuresä¸­"observation.state"çš„"names"å­—æ®µä¸­çš„å€¼
                - å›¾åƒï¼šé”®åä¸ºå»æ‰"observation.images."å‰ç¼€åçš„ç›¸æœºåç§°ï¼ˆå¦‚cam_headï¼‰
        """
        raw_obs = {}
        
        # è½¬æ¢stateï¼šéœ€è¦åˆ†è§£ä¸ºå•ç‹¬ç»„ä»¶ï¼ˆæ ¹æ®lerobot_featuresä¸­çš„namesï¼‰
        if "observation.state" in batch:
            state = batch["observation.state"][0]  # (state_dim,)
            if isinstance(state, torch.Tensor):
                state = state.cpu().numpy()
            
            # ä»lerobot_featuresè·å–çŠ¶æ€åç§°åˆ—è¡¨
            from lerobot.utils.constants import OBS_STR
            state_key = f"{OBS_STR}.state"
            
            if state_key in self.lerobot_features and "names" in self.lerobot_features[state_key]:
                state_names_raw = self.lerobot_features[state_key]["names"]
                
                # å¤„ç†nameså­—æ®µï¼šå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼ˆæ—§æ ¼å¼ï¼‰
                if isinstance(state_names_raw, dict):
                    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆå¦‚ {"motors": [...]}ï¼‰ï¼Œæå–æ‰€æœ‰å€¼å¹¶æ‰å¹³åŒ–
                    state_names = []
                    for key, value_list in state_names_raw.items():
                        if isinstance(value_list, list):
                            state_names.extend(value_list)
                        else:
                            state_names.append(value_list)
                    logger.debug(f"Converted dictionary format names to list: {len(state_names)} names")
                elif isinstance(state_names_raw, list):
                    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                    state_names = state_names_raw
                else:
                    # å¦‚æœä¸æ˜¯åˆ—è¡¨ä¹Ÿä¸æ˜¯å­—å…¸ï¼Œä½¿ç”¨é€šç”¨å‘½å
                    logger.warning(f"Unexpected names format: {type(state_names_raw)}. Using fallback.")
                    state_names = [f"state_{i}" for i in range(len(state))]
                
                # å°†çŠ¶æ€åˆ†è§£ä¸ºå•ç‹¬ç»„ä»¶
                for idx, state_name in enumerate(state_names):
                    if idx < len(state):
                        raw_obs[state_name] = float(state[idx])
                    else:
                        raw_obs[state_name] = 0.0
            else:
                # å¦‚æœæ²¡æœ‰ç‰¹å¾å®šä¹‰ï¼Œä½¿ç”¨é€šç”¨å‘½å
                for idx in range(len(state)):
                    raw_obs[f"state_{idx}"] = float(state[idx])
        
        # è½¬æ¢å›¾åƒï¼ˆéœ€è¦å»æ‰"observation.images."å‰ç¼€ï¼‰
        # æ³¨æ„ï¼šæœåŠ¡å™¨ç«¯çš„resize_robot_observation_imageæœŸæœ›è¾“å…¥æ˜¯(H, W, C)æ ¼å¼ï¼ˆä¼šåœ¨å†…éƒ¨è½¬æ¢ä¸º(C, H, W)ï¼‰
        # build_dataset_frameä»valuesä¸­ç›´æ¥å–numpy arrayï¼Œæ‰€ä»¥RawObservationä¸­çš„å›¾åƒåº”è¯¥æ˜¯(H, W, C)æ ¼å¼çš„numpy array
        # ä»eval_depalletize_async.pyæ¥çœ‹ï¼Œå®ƒä½¿ç”¨(H, W, C)æ ¼å¼çš„numpy arrayï¼ˆfloat32ï¼‰
        # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨(H, W, C)æ ¼å¼çš„float32 numpy array [0, 1]èŒƒå›´
        for key in batch.keys():
            if key.startswith("observation.images."):
                camera_base_name = key.replace("observation.images.", "")
                img = batch[key][0]  # æ•°æ®é›†ä¸­çš„æ ¼å¼é€šå¸¸æ˜¯(C, H, W)ï¼Œå€¼åœ¨[0, 1]èŒƒå›´
                
                if isinstance(img, torch.Tensor):
                    img = img.cpu().numpy()
                
                # ç¡®ä¿è½¬æ¢ä¸º(H, W, C)æ ¼å¼
                if img.ndim == 3:
                    if img.shape[0] == 3 or img.shape[0] == 1:  # (C, H, W)
                        img = img.transpose(1, 2, 0)  # (H, W, C)
                    # å¦‚æœå·²ç»æ˜¯(H, W, C)æ ¼å¼ï¼Œä¿æŒä¸å˜
                
                # è½¬æ¢ä¸ºfloat32æ ¼å¼ï¼ˆä¸eval_depalletize_async.pyä¿æŒä¸€è‡´ï¼‰
                # å¦‚æœå›¾åƒå€¼åœ¨[0, 1]èŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœåœ¨[0, 255]èŒƒå›´ï¼Œéœ€è¦å½’ä¸€åŒ–
                if img.dtype == np.uint8 or (img.dtype in [np.float32, np.float64] and img.max() > 1.0):
                    # å¦‚æœæ˜¯uint8æˆ–åœ¨[0, 255]èŒƒå›´ï¼Œå½’ä¸€åŒ–åˆ°[0, 1]
                    img = img.astype(np.float32) / 255.0
                else:
                    # å¦‚æœå·²ç»æ˜¯[0, 1]èŒƒå›´çš„float32/float64ï¼Œè½¬æ¢ä¸ºfloat32
                    img = img.astype(np.float32)
                
                raw_obs[camera_base_name] = img
        
        # æ·»åŠ taskï¼ˆå¦‚æœæä¾›æˆ–ä»æ•°æ®é›†ä¸­è·å–ï¼‰
        if self.task_description is not None:
            raw_obs["task"] = self.task_description
        elif "task" in batch:
            task = batch["task"]
            if isinstance(task, (list, tuple)) and len(task) > 0:
                raw_obs["task"] = task[0]
            elif isinstance(task, str):
                raw_obs["task"] = task
            else:
                raw_obs["task"] = str(task) if task is not None else ""
        else:
            raw_obs["task"] = ""
        
        return raw_obs
    
    def receive_actions(self):
        """æ¥æ”¶åŠ¨ä½œçš„çº¿ç¨‹å‡½æ•°"""
        logger.info("Action receiving thread starting")
        
        while not self.shutdown_event.is_set():
            try:
                actions_chunk = self.stub.GetActions(services_pb2.Empty())
                if len(actions_chunk.data) == 0:
                    continue  # æ”¶åˆ°ç©ºå“åº”ï¼Œç»§ç»­ç­‰å¾…
                
                # ååºåˆ—åŒ–åŠ¨ä½œ
                timed_actions = pickle.loads(actions_chunk.data)  # nosec
                
                # æ›´æ–°é˜Ÿåˆ—
                with self.action_queue_lock:
                    for action in timed_actions:
                        self.action_queue.put(action)
                
                self.action_chunk_size_received = max(self.action_chunk_size_received, len(timed_actions))
                self.must_go.set()  # æ”¶åˆ°åŠ¨ä½œåï¼Œå…è®¸å‘é€æ–°è§‚æµ‹
                
                if len(timed_actions) > 0:
                    logger.debug(f"Received {len(timed_actions)} actions, queue size: {self.action_queue.qsize()}")
            
            except grpc.RpcError as e:
                if not self.shutdown_event.is_set():
                    logger.error(f"Error receiving actions: {e}")
    
    def actions_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨ä½œå¯ç”¨"""
        with self.action_queue_lock:
            return not self.action_queue.empty()
    
    def _ready_to_send_observation(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å‡†å¤‡å¥½å‘é€æ–°è§‚æµ‹"""
        with self.action_queue_lock:
            queue_size = self.action_queue.qsize()
            return queue_size / self.action_chunk_size_received <= self.chunk_size_threshold
    
    def send_observation(self, obs: TimedObservation) -> bool:
        """å‘é€è§‚æµ‹åˆ°æœåŠ¡å™¨"""
        if not isinstance(obs, TimedObservation):
            raise ValueError("Input observation needs to be a TimedObservation!")
        
        try:
            observation_bytes = pickle.dumps(obs)
            observation_iterator = send_bytes_in_chunks(
                observation_bytes,
                services_pb2.Observation,
                log_prefix="[CLIENT] Observation",
                silent=True,
            )
            _ = self.stub.SendObservations(observation_iterator)
            logger.debug(f"Sent observation #{obs.get_timestep()}")
            return True
        except grpc.RpcError as e:
            logger.error(f"Error sending observation #{obs.get_timestep()}: {e}")
            return False
    
    def get_action(self) -> Optional[TimedAction]:
        """ä»é˜Ÿåˆ—è·å–å•ä¸ªåŠ¨ä½œ"""
        with self.action_queue_lock:
            if self.action_queue.empty():
                return None
            action = self.action_queue.get_nowait()
            with self.latest_action_lock:
                self.latest_action = action.get_timestep()
            return action
    
    def get_action_chunk(self, chunk_size: Optional[int] = None) -> Optional[list[TimedAction]]:
        """ä»é˜Ÿåˆ—è·å–åŠ¨ä½œchunkï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        
        Args:
            chunk_size: è¦è·å–çš„chunkå¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.action_chunk_size
            
        Returns:
            åŠ¨ä½œchunkåˆ—è¡¨ï¼Œå¦‚æœé˜Ÿåˆ—ä¸­æ²¡æœ‰è¶³å¤Ÿçš„åŠ¨ä½œåˆ™è¿”å›None
        """
        if chunk_size is None:
            chunk_size = self.action_chunk_size
        
        with self.action_queue_lock:
            if self.action_queue.qsize() < chunk_size:
                return None
            
            action_chunk = []
            for _ in range(chunk_size):
                if self.action_queue.empty():
                    # å¦‚æœé˜Ÿåˆ—åœ¨è·å–è¿‡ç¨‹ä¸­å˜ç©ºï¼Œè¿”å›å·²æ”¶é›†çš„éƒ¨åˆ†
                    return action_chunk if len(action_chunk) > 0 else None
                action = self.action_queue.get_nowait()
                action_chunk.append(action)
            
            # æ›´æ–°latest_actionä¸ºchunkä¸­æœ€åä¸€ä¸ªactionçš„timestep
            if len(action_chunk) > 0:
                with self.latest_action_lock:
                    self.latest_action = action_chunk[-1].get_timestep()
            
            return action_chunk
    
    def stop(self):
        """åœæ­¢å®¢æˆ·ç«¯"""
        self.shutdown_event.set()
        self.channel.close()
        logger.info("Client stopped")


def eval_async_on_dataset(
    server_address: str,
    ckpt_path: str,
    dataset_root: str,
    episode: int,
    action_chunk_size: int = 16,
    task_description: Optional[str] = None,
    show_progress: bool = True,
    enable_visualization: bool = True,
):
    """
    ä½¿ç”¨å¼‚æ­¥æ¨ç†åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
    
    Args:
        server_address: æœåŠ¡å™¨åœ°å€ (æ ¼å¼: "host:port")
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        episode: episodeç¼–å·
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        task_description: ä»»åŠ¡æè¿°ï¼ˆå¦‚æœæä¾›åˆ™è¦†ç›–æ•°æ®é›†ä¸­çš„taskï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
    """
    # åŠ è½½æ•°æ®é›†
    print(f"\n{'='*80}")
    print(f"ğŸ“‚ Loading dataset from {dataset_root}")
    print(f"ğŸ“¹ Episode: {episode}")
    
    # å¯¹äºæœ¬åœ°æ•°æ®é›†ï¼Œrepo_idåº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ ‡è¯†ç¬¦ï¼ˆä¸åŒ…å«"/"ï¼‰
    # ä½¿ç”¨æ•°æ®é›†è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ ‡è¯†ç¬¦ï¼Œæˆ–è€…ä½¿ç”¨"local"
    dataset_name = Path(dataset_root).name if dataset_root else "local"
    dataset = LeRobotDataset(repo_id=dataset_name, root=dataset_root, episodes=[episode])
    
    # è¿‡æ»¤åˆ°æŒ‡å®šepisode
    if episode >= len(dataset.meta.episodes):
        raise ValueError(f"Episode {episode} out of range. Available episodes: 0-{len(dataset.meta.episodes)-1}")
    
    ep_meta = dataset.meta.episodes[episode]
    ep_start = ep_meta["dataset_from_index"]
    ep_end = ep_meta["dataset_to_index"]
    dataset.hf_dataset = dataset.hf_dataset.select(range(ep_start, ep_end))
    print(f"âœ… Filtered dataset. Total frames in episode {episode}: {len(dataset.hf_dataset)} (indices {ep_start}-{ep_end-1})")
    
    # åˆ›å»ºDataLoader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=False,
        drop_last=False,
    )
    
    # è·å–actionç»´åº¦
    first_batch = next(iter(dataloader))
    action_dim = first_batch['action'].shape[1]
    print(f"ğŸ“Š Action dimension: {action_dim}")
    
    # é‡æ–°åˆ›å»ºdataloader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=False,
        drop_last=False,
    )
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    print(f"\nğŸ”§ Initializing async client...")
    client = DatasetAsyncClient(
        server_address=server_address,
        ckpt_path=ckpt_path,
        dataset=dataset,
        action_chunk_size=action_chunk_size,
        task_description=task_description,
    )
    
    # å¯åŠ¨æ¥æ”¶åŠ¨ä½œçš„çº¿ç¨‹
    action_receiver_thread = threading.Thread(target=client.receive_actions, daemon=True)
    action_receiver_thread.start()
    
    # ç­‰å¾…æœåŠ¡å™¨å‡†å¤‡
    time.sleep(1.0)
    
    # ------------- åˆå§‹åŒ–visualizer (å¯é€‰) -------------
    vizer = None
    kb = None
    if enable_visualization and RERUN_AVAILABLE:
        vizer = RerunVisualizer()
        kb = KeyboardManager()
        print("âœ… RerunVisualizer initialized")
    elif enable_visualization:
        print("âš ï¸  Running without RerunVisualizer (not available)")
    
    # ========= å¯è§†åŒ–æ•°æ®é›†é‡Œçš„groundtruth (å¦‚æœå¯ç”¨RerunVisualizer) =========
    if vizer is not None:
        print(f"\nğŸ“Š Visualizing ground truth data...")
        # åŠ è½½æ‰€æœ‰ground truth actionsç”¨äºå¯è§†åŒ–
        all_gt_actions = []
        all_gt_states = []
        
        temp_dataloader = torch.utils.data.DataLoader(
            dataset, num_workers=0, batch_size=1, shuffle=False, drop_last=False
        )
        
        for batch in temp_dataloader:
            all_gt_actions.append(batch['action'][0].cpu().numpy())
            if 'observation.state' in batch:
                all_gt_states.append(batch['observation.state'][0].cpu().numpy())
        
        all_gt_actions = np.array(all_gt_actions)
        all_gt_states = np.array(all_gt_states) if all_gt_states else None
        
        # å¯è§†åŒ–ground truth actionsï¼ˆä¸eval_on_dataset_lowpass.pyä¿æŒä¸€è‡´ï¼‰
        for dim in range(action_dim):
            vizer.visualize_chunk(
                name=f"chunk/action_dim_{dim}/gt",
                chunk_data=all_gt_actions[:, dim],
                step_id=0,
                width=3.0
            )
        
        # å¯è§†åŒ–observations (å¦‚æœå¯ç”¨)
        if all_gt_states is not None and len(all_gt_states) > 0:
            obs_dim = all_gt_states.shape[1]
            for dim in range(obs_dim):
                vizer.visualize_chunk(
                    name=f"obs/obs_{dim}",
                    chunk_data=all_gt_states[:, dim],
                    step_id=0,
                    width=3.0
                )
        
        print(f"âœ… Ground truth visualization ready")
    
    # å¼€å§‹è¯„ä¼°
    print("\n" + "="*80)
    print("ğŸš€ Starting evaluation...")
    print("="*80 + "\n")
    
    mse_per_action_dim = OrderedDict()
    mae_per_action_dim = OrderedDict()
    predictions = []
    ground_truths = []
    last_data_step = -1  # ç”¨äºè·Ÿè¸ªä¸Šæ¬¡å¯è§†åŒ–çš„step
    cached_action_chunk = []  # ç¼“å­˜çš„action chunkï¼ˆnumpyæ•°ç»„æ ¼å¼ï¼‰
    cached_chunk_start_frame = -1  # ç¼“å­˜çš„chunkèµ·å§‹frame
    
    iterator = tqdm(enumerate(dataloader), total=len(dataset.hf_dataset), desc="Processing") if show_progress else enumerate(dataloader)
    
    for frame_idx, batch in iterator:
        # æš‚åœæ§åˆ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vizer is not None and kb is not None:
            time.sleep(0.05)
            if kb.paused:
                print(f'===== æš‚åœä¸­ï¼ŒæŒ‰ä¸‹ç©ºæ ¼å¼€å§‹ =====')
            while kb.paused:
                time.sleep(0.1)
        
        # å‡†å¤‡è§‚æµ‹
        raw_obs = client._convert_dataset_obs_to_raw_obs(batch, frame_idx)
        
        # åˆ›å»ºTimedObservation
        with client.latest_action_lock:
            latest_action = client.latest_action
        
        timed_obs = TimedObservation(
            timestamp=time.time(),
            observation=raw_obs,
            timestep=max(latest_action + 1, frame_idx),
        )
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€è§‚æµ‹
        with client.action_queue_lock:
            timed_obs.must_go = client.must_go.is_set() and client.action_queue.empty()
            current_queue_size = client.action_queue.qsize()
        
        if client._ready_to_send_observation() or timed_obs.must_go:
            client.send_observation(timed_obs)
            if timed_obs.must_go:
                client.must_go.clear()
        
        # å¦‚æœç¼“å­˜çš„chunkå·²ç»ç”¨å®Œäº†ï¼Œå°è¯•è·å–æ–°çš„chunk
        if cached_chunk_start_frame >= 0:
            chunk_idx_in_cache = frame_idx - cached_chunk_start_frame
        else:
            chunk_idx_in_cache = -1  # æ²¡æœ‰ç¼“å­˜çš„chunk
        
        if len(cached_action_chunk) == 0 or chunk_idx_in_cache < 0 or chunk_idx_in_cache >= len(cached_action_chunk):
            # å°è¯•ä»é˜Ÿåˆ—è·å–æ–°çš„chunk
            action_chunk = client.get_action_chunk(action_chunk_size)
            if action_chunk is not None and len(action_chunk) > 0:
                # å°†TimedActionåˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
                pred_chunk = np.array([
                    ta.get_action().cpu().numpy() if isinstance(ta.get_action(), torch.Tensor) 
                    else ta.get_action() 
                    for ta in action_chunk
                ])
                if pred_chunk.ndim == 2:  # (chunk_size, action_dim)
                    cached_action_chunk = pred_chunk
                    cached_chunk_start_frame = frame_idx
                    chunk_idx_in_cache = 0
                else:
                    logger.warning(f"Unexpected chunk shape: {pred_chunk.shape}")
                    cached_action_chunk = []
                    cached_chunk_start_frame = -1
        
        # ä»ç¼“å­˜çš„chunkä¸­è·å–å½“å‰frameå¯¹åº”çš„action
        pred_action = None
        if len(cached_action_chunk) > 0 and 0 <= chunk_idx_in_cache < len(cached_action_chunk):
            # ä½¿ç”¨ç¼“å­˜çš„chunkä¸­çš„å¯¹åº”action
            pred_action = cached_action_chunk[chunk_idx_in_cache]
        else:
            # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»é˜Ÿåˆ—è·å–å•ä¸ªactionï¼ˆfallbackï¼‰
            timed_action = client.get_action()
            if timed_action is not None:
                pred_action = timed_action.get_action()
                if isinstance(pred_action, torch.Tensor):
                    pred_action = pred_action.cpu().numpy()
                if pred_action.ndim > 1:
                    pred_action = pred_action[0]
        
        if pred_action is not None:
            # è·å–ground truth
            gt_action = batch['action'][0].cpu().numpy()  # (action_dim,)
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            if pred_action.shape[0] != action_dim:
                logger.warning(f"Action dimension mismatch: pred={pred_action.shape[0]}, gt={action_dim}")
                continue
            
            # ä¿å­˜é¢„æµ‹å’ŒçœŸå®å€¼
            predictions.append(pred_action)
            ground_truths.append(gt_action)
            
            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„MSEå’ŒMAE
            for dim in range(action_dim):
                error = pred_action[dim] - gt_action[dim]
                mse = error ** 2
                mae = abs(error)
                
                if dim not in mse_per_action_dim:
                    mse_per_action_dim[dim] = []
                    mae_per_action_dim[dim] = []
                
                mse_per_action_dim[dim].append(mse)
                mae_per_action_dim[dim].append(mae)
            
            # å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰- ä¸eval_on_dataset_lowpass.pyå¯¹é½
            if vizer is not None:
                # æ˜¾ç¤ºå›¾åƒ - åŠ¨æ€æŸ¥æ‰¾å¯ç”¨çš„ç›¸æœºå›¾åƒï¼ˆä¸eval_on_dataset_lowpass.pyä¿æŒä¸€è‡´ï¼‰
                if CONFIG_AVAILABLE:
                    camera_names = get_camera_names(CAMERA_COMPONENTS)
                    for camera_name in camera_names:
                        obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                        fallback_key = f"observation.images.{camera_name}"
                        
                        # ä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ—§æ ¼å¼
                        key_to_use = obs_key if obs_key in batch else fallback_key
                        if key_to_use in batch:
                            img = batch[key_to_use][0]  # (C, H, W)
                            # ä»obs_keyä¸­æå–ç»„ä»¶åï¼ˆå¦‚ observation.images.cam_head -> cam_headï¼‰
                            # æˆ–è€…ä»camera_nameæ˜ å°„åˆ°ç»„ä»¶å
                            if obs_key in batch:
                                # ä½¿ç”¨æ–°æ ¼å¼ï¼šä» observation.images.cam_head æå– cam_head
                                display_name = obs_key.replace('observation.images.', '')
                            else:
                                # ä½¿ç”¨æ—§æ ¼å¼ï¼šä» camera_name æ˜ å°„åˆ°ç»„ä»¶å
                                display_name = CAMERA_KEY_MAPPING.get(camera_name, camera_name)
                            vizer.show_img(
                                name=f"images.{display_name}",
                                image_data=img.to("cpu"),
                                step_id=frame_idx
                            )
                else:
                    # å¦‚æœæ²¡æœ‰configï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•
                    for key in batch.keys():
                        if 'image' in key.lower() and key.startswith('observation'):
                            img = batch[key][0]  # (C, H, W)
                            camera_name = key.replace('observation.', '').replace('observation.images.', '')
                            vizer.show_img(
                                name=camera_name,
                                image_data=img.to("cpu"),
                                step_id=frame_idx
                            )
                
                # å¯è§†åŒ–é¢„æµ‹çš„action chunkï¼ˆä¸eval_on_dataset_lowpass.pyå¯¹é½ï¼‰
                for dim in range(action_dim):
                    # å¯è§†åŒ–MSEï¼ˆä¸eval_on_dataset_lowpass.pyä¿æŒä¸€è‡´ï¼‰
                    vizer.visualize_chunk(
                        name=f"mse/action_dim_{dim}",
                        chunk_data=mse_per_action_dim[dim][-1],
                        step_id=frame_idx,
                        width=3.0,
                    )
                
                # å¦‚æœæœ‰å®Œæ•´çš„chunkï¼Œå¯è§†åŒ–æ•´ä¸ªchunkï¼ˆä¸eval_on_dataset_lowpass.pyå¯¹é½ï¼‰
                # åªåœ¨chunkçš„ç¬¬ä¸€ä¸ªframeæ—¶å¯è§†åŒ–æ•´ä¸ªchunk
                should_visualize_chunk = (len(cached_action_chunk) > 0 and 
                                         cached_chunk_start_frame == frame_idx)
                
                if should_visualize_chunk:
                    for dim in range(action_dim):
                        # åˆ é™¤ä¹‹å‰çš„é¢„æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if last_data_step != frame_idx and last_data_step >= 0:
                            vizer.del_chunk(
                                name=f"chunk/action_dim_{dim}/pred_seg_{last_data_step}",
                                chunk_data=np.array([0.0]),  # å ä½æ•°æ®
                                step_id=last_data_step,
                                width=0.5
                            )
                        
                        # å¯è§†åŒ–é¢„æµ‹çš„æ•´ä¸ªchunkï¼ˆä¸eval_on_dataset_lowpass.pyå¯¹é½ï¼‰
                        vizer.visualize_chunk(
                            name=f"chunk/action_dim_{dim}/pred_seg_{frame_idx}",
                            chunk_data=cached_action_chunk[:, dim],
                            step_id=frame_idx,
                            width=2.0,
                        )
        else:
            # å¦‚æœæ²¡æœ‰æ”¶åˆ°åŠ¨ä½œï¼Œä½¿ç”¨é›¶å‘é‡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
            logger.warning(f"No action received for frame {frame_idx}, using zeros")
            gt_action = batch['action'][0].cpu().numpy()
            pred_action = np.zeros(action_dim)
            
            predictions.append(pred_action)
            ground_truths.append(gt_action)
        
        # æ›´æ–°last_data_stepç”¨äºå¯è§†åŒ–
        if pred_action is not None:
            last_data_step = frame_idx
        
        # æ§åˆ¶å¾ªç¯é¢‘ç‡
        time.sleep(max(0, client.environment_dt - 0.001))
    
    # åœæ­¢å®¢æˆ·ç«¯
    client.stop()
    action_receiver_thread.join(timeout=5.0)
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š Final Statistics")
    print("="*80)
    
    # Actionåç§°å®šä¹‰
    if CONFIG_AVAILABLE and action_names and len(action_names) == action_dim:
        eval_action_names = action_names
    elif action_dim == 16:
        eval_action_names = [f"Arm_joint_{i+1}" for i in range(14)] + ["Left_claw", "Right_claw"]
    elif action_dim == 18:
        eval_action_names = (
            [f"arm_joint_{i+1}" for i in range(7)] +
            [f"arm_joint_{i+8}" for i in range(7)] +
            ["left_claw_position", "right_claw_position", "cmd_pose_z", "cmd_pose_pitch"]
        )
    elif action_dim == 24:
        eval_action_names = (
            ["COM_dx", "COM_dy", "COM_dz", "COM_dR11", "COM_dR21", "COM_dR31", "COM_dR12", "COM_dR22", "COM_dR32"] +
            [f"Arm_joint_{i+1}" for i in range(14)] +
            ["Gait_mode"]
        )
    else:
        eval_action_names = [f"Action_dim_{i}" for i in range(action_dim)]
    
    print(f"\n{'Dimension':<20} {'MSE':<15} {'MAE':<15}")
    print("-" * 80)
    
    for dim in range(action_dim):
        if dim in mse_per_action_dim and len(mse_per_action_dim[dim]) > 0:
            mse_mean = np.mean(mse_per_action_dim[dim])
            mae_mean = np.mean(mae_per_action_dim[dim])
            dim_name = eval_action_names[dim] if dim < len(eval_action_names) else f"Dim_{dim}"
            print(f'{dim_name:<20} {mse_mean:<15.8f} {mae_mean:<15.8f}')
    
    if len(mse_per_action_dim) > 0:
        overall_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in mse_per_action_dim.keys()])
        overall_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in mae_per_action_dim.keys()])
        
        print("-" * 80)
        print(f'{"Overall":<20} {overall_mse:<15.8f} {overall_mae:<15.8f}')
    
    print("="*80)
    print("\nâœ… Evaluation completed!")
    
    if vizer is not None:
        print("\n[Offline Eval] Visualization active. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\nâœ… Exiting...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Evaluate GrootPolicy Model on Dataset using Async Inference',
        epilog='Evaluates a trained GrootPolicy model on a LeRobot dataset using async inference architecture.'
    )
    parser.add_argument('--server_address', type=str, required=True,
                       help='Server address (format: host:port)')
    parser.add_argument('--ckpt-path', '--ckpt_path', type=str, required=True, dest='ckpt_path',
                       help='Path to the model checkpoint directory')
    parser.add_argument('--dataset-root', '--dataset_root', type=str, required=True, dest='dataset_root',
                       help='Path to the LeRobot dataset root directory')
    parser.add_argument('--episode', type=int, default=0,
                       help='Episode number to evaluate (default: 0)')
    parser.add_argument('--action-chunk-size', '--action_chunk_size', type=int, default=16, dest='action_chunk_size',
                       help='Action chunk size (default: 16, should match training config)')
    parser.add_argument('--task-description', '--task_description', type=str, default=None, dest='task_description',
                       help='Task description (language instruction) to override the task from dataset')
    parser.add_argument('--no-progress', '--no_progress', action='store_true', dest='no_progress',
                       help='Disable progress bar')
    parser.add_argument('--no-visualization', '--no_visualization', action='store_true', dest='no_visualization',
                       help='Disable Rerun visualization')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ğŸ¯ GrootPolicy Async Inference Dataset Evaluation")
    print("="*80)
    print(f"Server: {args.server_address}")
    print(f"Checkpoint: {args.ckpt_path}")
    print(f"Dataset: {args.dataset_root}")
    print(f"Episode: {args.episode}")
    print(f"Action Chunk Size: {args.action_chunk_size}")
    if args.task_description:
        print(f"Task Description (overridden): '{args.task_description}'")
    else:
        print(f"Task Description: Will use task from dataset")
    print("="*80)
    
    eval_async_on_dataset(
        server_address=args.server_address,
        ckpt_path=args.ckpt_path,
        dataset_root=args.dataset_root,
        episode=args.episode,
        action_chunk_size=args.action_chunk_size,
        task_description=args.task_description,
        show_progress=not args.no_progress,
        enable_visualization=not args.no_visualization,
    )
