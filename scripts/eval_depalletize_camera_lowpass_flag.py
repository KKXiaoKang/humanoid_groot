import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import cv2
import numpy as np

# Initialize GUI windows if requested
def init_gui_windows(enable_gui=False, camera_config=None):
    """
    Initialize GUI windows if enabled
    
    Args:
        enable_gui: Whether to enable GUI windows
        camera_config: Dictionary of camera names from topic_info (e.g., {'image': ..., 'chest_image': ...})
    """
    if not enable_gui:
        print(" ======================  GUI windows disabled ====================== ")
        return
    
    print(" ======================  Initializing GUI windows ====================== ")
    
    # æ ¹æ®ç›¸æœºé…ç½®åŠ¨æ€åˆ›å»ºçª—å£
    if camera_config is None:
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤3ç›¸æœºé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        from configs.config import topic_info
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    
    # ç›¸æœºåç§°åˆ°çª—å£åç§°çš„æ˜ å°„
    camera_window_map = {
        'image': 'head Camera',
        'chest_image': 'chest Camera',
        'left_shoulder_image': 'left_shoulder Camera',
        'right_shoulder_image': 'right_shoulder Camera'
    }
    
    # åˆ›å»ºç›¸æœºçª—å£
    for camera_name in camera_config.keys():
        if camera_name in camera_window_map:
            window_name = camera_window_map[camera_name]
            cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(window_name, 640, 480)
            print(f"   Created window: {window_name}")
    
    print(f" ======================  GUI windows ready ({len(camera_config)} cameras) ====================== ")

# GUIçª—å£å°†åœ¨è§£æå‘½ä»¤è¡Œå‚æ•°ååˆå§‹åŒ–

from collections import deque
from typing import Optional
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "robot_envs")))
from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, ACTION_COMPONENTS

# ä½¿ç”¨GrootPolicyæ¨¡å‹
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# import torchvision
# import matplotlib.pyplot as plt
from pathlib import Path
import torch
import time
import argparse
import rospy
from std_msgs.msg import Float64MultiArray
from kuavo_humanoid_sdk.kuavo_strategy_pytree.common.robot_sdk import RobotSDK
from kuavo_humanoid_sdk.msg.kuavo_msgs.srv import (changeArmCtrlMode, changeArmCtrlModeRequest)

MODEL_ACTION_DT = 0.1  # seconds between predicted actions during training
MODEL_ACTION_FREQUENCY = 1.0 / MODEL_ACTION_DT
TARGET_CONTROL_FREQUENCY = 100.0
TARGET_CONTROL_DT = 1.0 / TARGET_CONTROL_FREQUENCY
CHUNK_TRANSITION_DURATION_S = 0.2  # seconds of low-pass smoothing at chunk boundary
LOWPASS_ALPHA = 0.85  # closer to 1 => smoother (slower) transitions
ENABLE_CHUNK_TRANSITION_LOWPASS = False  # Enable/disable low-pass filtering at chunk boundaries (default: False, only linear interpolation within chunks)


def resample_action_chunk(action_chunk: np.ndarray,
                          source_dt: float = MODEL_ACTION_DT,
                          target_dt: float = TARGET_CONTROL_DT) -> np.ndarray:
    """
    Resample an action chunk predicted at a lower frequency to a higher control frequency.

    Args:
        action_chunk: Array of shape (N, action_dim) predicted at intervals of source_dt.
        source_dt: Time interval between successive actions in the chunk.
        target_dt: Desired time interval for control commands.

    Returns:
        Array of shape (M, action_dim) where M approximates (N-1)*source_dt/target_dt + 1,
        interpolated with linear interpolation along time.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if action_chunk.shape[0] <= 1 or np.isclose(source_dt, target_dt):
        # Nothing to resample, either single action or already at target frequency
        return action_chunk

    total_duration = source_dt * (action_chunk.shape[0] - 1)
    if total_duration <= 0:
        repeat_factor = max(int(round(source_dt / target_dt)), 1)
        return np.repeat(action_chunk, repeats=repeat_factor, axis=0)

    num_target_steps = int(round(total_duration / target_dt)) + 1
    source_times = np.linspace(0.0, total_duration, num=action_chunk.shape[0])
    target_times = np.linspace(0.0, total_duration, num=num_target_steps)

    interpolated = np.empty((num_target_steps, action_chunk.shape[1]), dtype=action_chunk.dtype)
    for dim in range(action_chunk.shape[1]):
        interpolated[:, dim] = np.interp(target_times, source_times, action_chunk[:, dim])

    return interpolated


def apply_lowpass_transition(actions: np.ndarray,
                             previous_action: Optional[np.ndarray],
                             alpha: float = LOWPASS_ALPHA,
                             transition_steps: Optional[int] = None,
                             smooth_slice: slice | tuple | np.ndarray = slice(None)) -> np.ndarray:
    """
    Smooth the beginning of a resampled chunk with an exponential low-pass filter
    to reduce discontinuities at chunk boundaries.

    Args:
        actions: Resampled action chunk at control frequency, shape (N, action_dim).
        previous_action: Last action that was executed on the robot. If None, no smoothing applied.
        alpha: Low-pass smoothing coefficient (0 < alpha < 1). Larger alpha = smoother/slower response.
        transition_steps: Number of control steps over which to apply smoothing. If None, smooth entire chunk.
        smooth_slice: Indices/slice specifying which action dimensions to smooth (e.g., only arm joints).

    Returns:
        Smoothed action chunk (same shape as input).
    """
    if previous_action is None:
        return actions

    actions = np.asarray(actions)
    if actions.ndim == 1:
        actions = actions.reshape(1, -1)

    smoothed = actions.copy()
    prev = np.asarray(previous_action, dtype=smoothed.dtype)
    if prev.ndim == 1:
        prev = prev.reshape(1, -1)
    prev = prev[0]

    num_steps = smoothed.shape[0]
    if transition_steps is None or transition_steps > num_steps:
        transition_steps = num_steps
    transition_steps = max(1, transition_steps)

    if isinstance(smooth_slice, slice) or isinstance(smooth_slice, tuple):
        smooth_indices = smooth_slice
    else:
        smooth_indices = smooth_slice

    for idx in range(transition_steps):
        prev_slice = prev[smooth_indices]
        smoothed_slice = smoothed[idx][smooth_indices]
        filtered = alpha * prev_slice + (1.0 - alpha) * smoothed_slice
        prev[smooth_indices] = filtered
        smoothed[idx][smooth_indices] = filtered

    return smoothed


def resample_chunk_with_claw_hold(action_chunk: np.ndarray,
                                  previous_action: Optional[np.ndarray],
                                  control_frequency: float,
                                  source_dt: float = MODEL_ACTION_DT,
                                  arm_dims: slice = slice(0, 14),
                                  claw_dims: slice = slice(14, 16)) -> np.ndarray:
    """
    Resample an action chunk so that arm joints are interpolated to the control frequency
    while claw positions are held at the original (low) frequency.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if previous_action is not None:
        chunk_with_bridge = np.vstack([previous_action, action_chunk])
        resampled = resample_action_chunk(
            chunk_with_bridge,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )[1:]
        source_array = chunk_with_bridge
    else:
        resampled = resample_action_chunk(
            action_chunk,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )
        source_array = action_chunk

    # Zero-order hold for claw dimensions (keep 10Hz updates)
    if source_array.shape[0] > 0 and resampled.shape[0] > 0:
        total_duration = source_dt * max(source_array.shape[0] - 1, 1)
        if total_duration <= 0:
            hold_indices = np.zeros(resampled.shape[0], dtype=int)
        else:
            target_times = np.linspace(0.0, total_duration, num=resampled.shape[0], endpoint=True)
            source_times = np.linspace(0.0, total_duration, num=source_array.shape[0], endpoint=True)
            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
            hold_indices = np.clip(hold_indices, 0, source_array.shape[0] - 1)
        resampled[:, claw_dims] = source_array[hold_indices][:, claw_dims]

    return resampled


def direct_to_wbc(control_mode):
    """
        åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
        Args:
            control_mode: æ§åˆ¶æ¨¡å¼
                0: ç¦ç”¨wbcæ§åˆ¶è½¨è¿¹æ¨¡å¼
                1: wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
    """
    rospy.wait_for_service('/enable_wbc_arm_trajectory_control', timeout=5)
    try:
        change_mode = rospy.ServiceProxy('/enable_wbc_arm_trajectory_control', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)


def replay(lerobot_dataset_path, episode, control_arm=True, control_claw=True):
    """
    ç›´æ¥replayæ•°æ®é›†é‡Œçš„è½¨è¿¹ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    """
    repo_id = 0

    dataset = LeRobotDataset(repo_id=repo_id, root=lerobot_dataset_path, episodes=[episode])
    actions = dataset.hf_dataset.select_columns("action")
    env = GrabBoxMpcEnv()
    env.obs_buffer.wait_buffer_ready()
    time.sleep(1)

    for idx in range(dataset.num_frames):
        action = actions[idx]["action"]
        action = np.expand_dims(action, axis=0)

        env.exec_actions(actions=action,
                         control_arm=control_arm,
                         control_claw=control_claw,
                         )

def publish_joint_positions(action_chunk,
                            joint_pub,
                            source_frequency_hz: float,
                            target_frequency_hz: Optional[float] = None):
    """
    ä»åŠ¨ä½œå—ä¸­æå–å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®å¹¶åˆå¹¶å‘å¸ƒåˆ°ROSè¯é¢˜ï¼Œ
    å¯é€‰åœ°å°†åŠ¨ä½œæ’å€¼åˆ°æ›´é«˜çš„æ§åˆ¶é¢‘ç‡åå†å‘å¸ƒã€‚
    
    Args:
        action_chunk: shapeä¸º(N, action_dim)çš„åŠ¨ä½œå—
                     æ”¯æŒæ ¼å¼:
                     - 16ç»´: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
                     - 18ç»´: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
        joint_pub: å…³èŠ‚ä½ç½®å‘å¸ƒå™¨
        source_frequency_hz: åŸå§‹åŠ¨ä½œå—çš„é¢‘ç‡ï¼ˆHzï¼‰
        target_frequency_hz: å¦‚æœæä¾›ï¼Œåˆ™å°†åŠ¨ä½œå—æ’å€¼åˆ°è¯¥é¢‘ç‡åå†å‘å¸ƒ
    """
    try:
        action_chunk = np.asarray(action_chunk)
        if action_chunk.ndim == 1:
            action_chunk = action_chunk.reshape(1, -1)

        if target_frequency_hz is not None and target_frequency_hz > source_frequency_hz:
            action_chunk = resample_action_chunk(
                action_chunk,
                source_dt=1.0 / source_frequency_hz,
                target_dt=1.0 / target_frequency_hz
            )

        action_dim = action_chunk.shape[1]
        
        # æ”¯æŒ16ç»´å’Œ18ç»´åŠ¨ä½œæ ¼å¼
        if action_dim == 16:
            # 16ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
            rospy.logdebug(f"Using depalletize 16-dim action format")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            # åˆå¹¶å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œæœ€åæ˜¯å¤¹çˆªä½ç½®
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions], axis=1)  # shape: (action_chunk_size, 16)
            
        elif action_dim == 18:
            # 18ç»´æ ¼å¼: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
            # å‘é€å®Œæ•´çš„18ç»´æ•°æ®
            rospy.logdebug(f"Using depalletize 18-dim action format (including cmd_pose dimensions)")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            cmd_pose = action_chunk[:, 16:18]  # shape: (action_chunk_size, 2)
            # åˆå¹¶æ‰€æœ‰ç»„ä»¶ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œç„¶åæ˜¯å¤¹çˆªï¼Œæœ€åæ˜¯cmd_pose
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions, cmd_pose], axis=1)  # shape: (action_chunk_size, 18)
            
        else:
            rospy.logwarn(f"Action chunk dimension {action_dim} not supported (expected 16 or 18 for depalletize task)")
            return

        # å‘å¸ƒåˆå¹¶åçš„å…³èŠ‚ä½ç½®ï¼ˆå®Œæ•´çš„action_dimç»´åº¦ï¼‰
        joint_msg = Float64MultiArray()
        joint_msg.data = combined_joints.flatten().tolist()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        joint_pub.publish(joint_msg)
        
        rospy.logdebug(f"Published combined joint positions: {combined_joints.shape} (action_dim={action_dim})")
        
    except Exception as e:
        rospy.logerr(f"Error publishing joint positions: {str(e)}")


def eval(ckpt_path, model_type, control_arm=True, control_claw=True, action_chunk_size=50, lerobot_dataset_path=None, enable_gui=False):
    """
    åœ¨è¿™é‡Œå’Œå®æœº/ä»¿çœŸäº¤äº’ï¼Œåšç½‘ç»œæ¨ç†ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨åªä½¿ç”¨GrootPolicyï¼‰
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        lerobot_dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆç”¨äºåŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯é€‰ï¼‰
        enable_gui: æ˜¯å¦å¯ç”¨GUIçª—å£æ˜¾ç¤ºç›¸æœºå›¾åƒ
    """

    # ---------- 1. load GrootPolicy from checkpoint ---------------
    device = "cuda:0"
    print(" =================== Loading GrootPolicy =================== ")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = action_chunk_size
    
    # Load dataset statistics for normalization
    print(f"\nğŸ“‚ Loading dataset for statistics...")
    dataset_stats = None
    if lerobot_dataset_path:
        try:
            dataset_for_stats = LeRobotDataset(repo_id=0, root=lerobot_dataset_path)
            dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
            print(f"âœ… Dataset statistics loaded: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load dataset statistics: {e}")
            print("   This may cause normalization issues during inference")
    else:
        print("âš ï¸ Warning: No dataset path provided. Using default dataset for statistics.")
        try:
            dataset_for_stats = LeRobotDataset(repo_id=0, root='/home/lab/lerobot_groot/lerobot_data/new_demo/1118_sim_depalletize')
            dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
            print(f"âœ… Dataset statistics loaded from default path: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load default dataset statistics: {e}")
    
    # Create preprocessor and postprocessor
    print(f"\nğŸ”§ Creating preprocessor and postprocessor...")
    preprocessor, postprocessor = make_groot_pre_post_processors(
        config=policy.config,
        dataset_stats=dataset_stats,
    )
    print("âœ… Preprocessor and postprocessor created")
    
    # Debug: Print model configuration
    print(f"ğŸ” Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"ğŸ” Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    # Print action mode configuration
    print("\n" + "="*80)
    print("ğŸ¯ DEPALLETIZE TASK CONFIGURATION (GrootPolicy)")
    print("="*80)
    print(f"ğŸ¤– Control arm: {control_arm}")
    print(f"ğŸ¦¾ Control claw: {control_claw}")
    print(f"ğŸ“Š ACTION_COMPONENTS: {ACTION_COMPONENTS}")
    print(f"ğŸ“Š Action dimension: Will be determined by model output (expected: {len(ACTION_COMPONENTS) * 7 if 'Left_arm' in ACTION_COMPONENTS and 'Right_arm' in ACTION_COMPONENTS else 'varies'}D based on config)")
    print(f"ğŸ“¦ Action chunk size: {action_chunk_size}")
    # æ ¹æ®ACTION_COMPONENTSåˆ¤æ–­æ˜¯å¦åŒ…å«cmd_pose
    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
    print(f"ğŸ¯ Cmd_pose control: {'Enabled' if has_cmd_pose else 'Disabled'} (based on ACTION_COMPONENTS)")
    print("="*80 + "\n")
    
    policy.eval()
    policy.to(device)
    policy.reset()
    
    step_counter = 0

    # Initialize ROS publishers for action visualization
    # rospy.init_node('act_eval_visualizer', anonymous=True)
    joint_pub = rospy.Publisher('/policy/action/eef_pose_marker_all', Float64MultiArray, queue_size=10)
    
    rospy.loginfo(f"Initialized ROS publishers for action visualization with chunk size: {action_chunk_size}")

    # Initialize real-time environment
    env = GrabBoxMpcEnv()
    print(f"ğŸ¤– Environment initialized for depalletize task")
    print(" ======================  Waiting for buffer ready ====================== ")
    env.obs_buffer.wait_buffer_ready()
    print(" ======================  Buffer ready ====================== ")
    time.sleep(1)
    
    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

    # TODO: æ£€æŸ¥æ—¶é—´åŒæ­¥æƒ…å†µ
    # TODO: æ˜¾ç¤ºå›¾åƒ
    rospy.loginfo(f"Initialized action visualization with chunk size: {action_chunk_size}")
    
    # ---------- 2. æ¨¡å‹æ¨ç†ï¼ˆå®æ—¶æ¨¡å¼ï¼‰ ----------------------
    # Real-time environment evaluation loop
    direct_to_wbc(1)
    input(f"direct_to_wbc ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== \n")
    time.sleep(1.0)
    resampled_action_queue: deque[np.ndarray] = deque()
    last_executed_action: Optional[np.ndarray] = None

    while True:
        try:
            state = torch.from_numpy(obs_data["state"]).float()
            # print(f" ==== state ==== {state.shape} ==== ")
            
            # æ ¹æ®topic_infoåŠ¨æ€å¤„ç†æ‰€æœ‰ç›¸æœºå›¾åƒ
            # å¡«å……ç½‘ç»œçš„obs
            observation = {}
            
            # åŠ¨æ€å¤„ç†æ‰€æœ‰ç›¸æœºè§‚æµ‹ - ä½¿ç”¨æ–°çš„keyæ ¼å¼
            for camera_name in topic_info.keys():
                if 'image' in camera_name and camera_name in obs_data:
                    camera_images = torch.from_numpy(np.moveaxis(obs_data[camera_name], 3, 1)).float() / 255
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼: observation.images.cam_*
                    obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                    observation[obs_key] = camera_images.to('cuda:0')

            # observation['observation.environment_state'] = environment_state
            observation['observation.state'] = state.to('cuda:0')

            if not resampled_action_queue:
                # ä½¿ç”¨GrootPolicyçš„predict_action_chunk
                # é¦–å…ˆä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†è¾“å…¥
                processed_observation = preprocessor(observation)
                
                # æ¨¡å‹æ¨ç†
                with torch.inference_mode():
                    pred_actions = policy.predict_action_chunk(processed_observation)
                
                # pred_actions shape: (batch_size, chunk_size, action_dim)
                # æ³¨æ„ï¼špred_actionsæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ŒèŒƒå›´åœ¨[-1, 1]
                # éœ€è¦æ‰‹åŠ¨åå½’ä¸€åŒ–åˆ°çœŸå®å•ä½
                
                # æ‰‹åŠ¨åå½’ä¸€åŒ–æ•´ä¸ªchunk
                if dataset_stats and 'action' in dataset_stats:
                    action_stats = dataset_stats['action']
                    if 'min' in action_stats and 'max' in action_stats:
                        action_min = torch.as_tensor(action_stats['min'], dtype=torch.float32, device=pred_actions.device)
                        action_max = torch.as_tensor(action_stats['max'], dtype=torch.float32, device=pred_actions.device)
                        
                        # ç¡®ä¿ç»´åº¦åŒ¹é…
                        action_dim = pred_actions.shape[-1]
                        if action_min.numel() < action_dim:
                            action_min = torch.nn.functional.pad(action_min.flatten()[:action_dim], (0, max(0, action_dim - action_min.numel())))
                        if action_max.numel() < action_dim:
                            action_max = torch.nn.functional.pad(action_max.flatten()[:action_dim], (0, max(0, action_dim - action_max.numel())))
                        
                        action_min = action_min[:action_dim]
                        action_max = action_max[:action_dim]
                        
                        # åå½’ä¸€åŒ–å…¬å¼ï¼šx = (y + 1) / 2 * (max - min) + min
                        denom = action_max - action_min
                        mask = denom != 0
                        safe_denom = torch.where(mask, denom, torch.ones_like(denom))
                        
                        # åå½’ä¸€åŒ–æ•´ä¸ªchunk
                        pred_actions_unnorm = (pred_actions + 1.0) * 0.5 * safe_denom + action_min
                        pred_actions_unnorm = torch.where(mask, pred_actions_unnorm, action_min)
                        
                        # è½¬æ¢ä¸ºnumpy
                        action_chunk = pred_actions_unnorm[0].cpu().numpy()  # (chunk_size, action_dim)
                    else:
                        # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼ï¼ˆå¯èƒ½å·²ç»æ˜¯åå½’ä¸€åŒ–çš„ï¼‰
                        action_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
                        rospy.logwarn("âš ï¸ Warning: No action min/max stats found. Using raw predictions (may be normalized).")
                else:
                    # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼
                    action_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
                    rospy.logwarn("âš ï¸ Warning: No dataset stats found. Using raw predictions (may be normalized).")

                # æ ¹æ®åŠ¨ä½œç»´åº¦åŠ¨æ€ç¡®å®šclawç»´åº¦
                action_dim = action_chunk.shape[1]
                if action_dim == 16:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                elif action_dim == 18:
                    # 18ç»´æ ¼å¼: å‰14ç»´æ˜¯æ‰‹è‡‚å…³èŠ‚ï¼Œ14-16æ˜¯å¤¹çˆªï¼Œ16-18æ˜¯cmd_poseï¼ˆä¿ç•™å®Œæ•´18ç»´ï¼‰
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                    # æ³¨æ„ï¼šcmd_poseç»´åº¦(16-18)ä¼šåœ¨resampleæ—¶ä¸€èµ·å¤„ç†ï¼Œä¸éœ€è¦å•ç‹¬å¤„ç†
                else:
                    # é»˜è®¤ä½¿ç”¨å‰14ç»´ä½œä¸ºæ‰‹è‡‚ï¼Œ14-16ä½œä¸ºå¤¹çˆª
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, min(16, action_dim))
                    rospy.logwarn(f"Unknown action dimension {action_dim}, using default arm/claw split")
                
                resampled_chunk = resample_chunk_with_claw_hold(
                    action_chunk,
                    previous_action=last_executed_action,
                    control_frequency=env.control_frequency,
                    source_dt=MODEL_ACTION_DT,
                    arm_dims=arm_dims,
                    claw_dims=claw_dims
                )

                # Apply low-pass filtering at chunk boundaries only if enabled
                if ENABLE_CHUNK_TRANSITION_LOWPASS:
                    if last_executed_action is not None:
                        transition_steps = max(
                            1,
                            int(round(env.control_frequency * CHUNK_TRANSITION_DURATION_S))
                        )
                    else:
                        transition_steps = None
                    
                    resampled_chunk = apply_lowpass_transition(
                        resampled_chunk,
                        previous_action=last_executed_action,
                        alpha=LOWPASS_ALPHA,
                        transition_steps=transition_steps,
                        smooth_slice=arm_dims  # åªå¯¹æ‰‹è‡‚å…³èŠ‚è¿›è¡Œä½é€šæ»¤æ³¢
                    )

                publish_joint_positions(
                    resampled_chunk,
                    joint_pub,
                    source_frequency_hz=env.control_frequency,
                    target_frequency_hz=None
                )
                rospy.loginfo(f"Prepared resampled chunk of size {resampled_chunk.shape[0]} for execution")

                resampled_action_queue = deque(np.array(step, copy=True) for step in resampled_chunk)

            current_action = resampled_action_queue.popleft()
            # æ ¹æ®ACTION_COMPONENTSé…ç½®å†³å®šæ˜¯å¦æ§åˆ¶cmd_pose
            # å¦‚æœACTION_COMPONENTSåŒ…å«Cmd_pose_zæˆ–Cmd_pose_pitchï¼Œåˆ™å¯ç”¨cmd_poseæ§åˆ¶
            control_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
            
            env.exec_actions(actions=current_action,
                             control_arm=control_arm,
                             control_claw=control_claw,
                             control_cmd_pose=control_cmd_pose)
            step_counter += 1
            last_executed_action = current_action.copy()

            obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

            if enable_gui:
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' or ESC to quit
                    print("\n[GUI] Exiting by user request (q or ESC pressed)")
                    break

        except KeyboardInterrupt:
            print("\n[Interrupted] Exiting by user Ctrl+C.")
            break
    
    # Cleanup GUI windows
    if enable_gui:
        cv2.destroyAllWindows()




if __name__ == '__main__':
    # æœºå™¨äººä½å¤´
    robot_sdk = RobotSDK()
    robot_sdk.control.control_head(0, np.deg2rad(10))
    robot_sdk.control.set_external_control_arm_mode()  # åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼
    print(" ==== æœºå™¨äººå¤´éƒ¨ä¿¯ä»°è°ƒèŠ‚è§’åº¦: 10 æˆåŠŸ ==== ")
    print(" ==== åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== ")
    
    # python å‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description='Depalletize Task Evaluation Script',
        epilog='This script evaluates models for the depalletize task (16-dim actions: 14 arm joints + 2 claw positions).'
    )
    parser.add_argument('--ckpt-path', type=str, default='/home/lab/kuavo-manip/outputs/train/box_only_vel_obs/checkpoints/080000/pretrained_model',
                        help='Path to the checkpoint directory')
    parser.add_argument('--model-type', type=str, default='groot', choices=['groot', 'act', 'dp'],
                        help='Type of model to use (now only groot is supported, act/dp are deprecated)')
    parser.add_argument('--eval', action='store_true', help='Evaluate the model in real-time environment')
    parser.add_argument('--replay', action='store_true', help='Replay the model')
    parser.add_argument('--action_chunk_size', type=int, default=20, help='Number of action steps')
    parser.add_argument('--lerobot_dataset_path', type=str, default=None, help='Path to the LeRobot dataset for loading statistics (optional)')
    parser.add_argument('--enable_gui', action='store_true',
                        help='Enable GUI windows for camera display (default: disabled)')
    
    args = parser.parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å’Œç›¸æœºé…ç½®åˆå§‹åŒ–GUIçª—å£
    camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    init_gui_windows(enable_gui=args.enable_gui, camera_config=camera_config)
    
    # æ‰“å°ç›¸æœºé…ç½®ä¿¡æ¯
    print(f"\nğŸ“· Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
    print(f"   Detected {len(camera_config)} cameras: {list(camera_config.keys())}")
    
    print("\n" + "="*80)
    print("ğŸ¯ Depalletize Task Evaluation (GrootPolicy)")
    print("="*80)
    print(f"ğŸ“‚ Checkpoint: {args.ckpt_path}")
    print(f"ğŸ¤– Model type: {args.model_type} (using GrootPolicy)")
    if args.model_type != 'groot':
        print(f"âš ï¸  Warning: model-type '{args.model_type}' is deprecated. Using GrootPolicy instead.")
    print(f"ğŸ“Š Action chunk size: {args.action_chunk_size}")
    print(f"ğŸ“¦ Action dimension: Supports 16 or 18 (14 arm joints + 2 claw positions [+ 2 cmd_pose])")
    print(f"ğŸ–¼ï¸  Enable GUI: {args.enable_gui}")
    if args.lerobot_dataset_path:
        print(f"ğŸ“ Dataset path (for stats): {args.lerobot_dataset_path}")
    print("="*80 + "\n")

    if args.eval:
        print("ğŸš€ Starting real-time evaluation...")
        eval(args.ckpt_path, model_type=args.model_type, control_arm=True, control_claw=True, 
             action_chunk_size=args.action_chunk_size, 
             lerobot_dataset_path=args.lerobot_dataset_path,
             enable_gui=args.enable_gui)
    elif args.replay:
        print("Replaying the model")
        lerobot_dataset_path = '/home/lab/kuavo-manip/lerobot_data/vel_wrend_box_613'
        replay(lerobot_dataset_path, episode=0, control_arm=True, control_claw=True)
    else:
        print("Please specify either --eval or --replay")
        exit(1)

    # --------------------------------------- #

