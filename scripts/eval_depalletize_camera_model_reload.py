import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import cv2
import numpy as np
import rosbag
from sensor_msgs.msg import JointState
import json
from std_srvs.srv import Trigger, TriggerRequest, SetBool, SetBoolRequest

# Initialize GUI windows if requested
def init_gui_windows(enable_gui=False, camera_config=None):
    """
    Initialize GUI windows if enabled
    
    Args:
        enable_gui: Whether to enable GUI windows
        camera_config: Dictionary of camera names from topic_info (e.g., {'image': ..., 'chest_image': ...})
    """
    if not enable_gui:
        print(" ======================  GUI windows disabled ====================== ")
        return
    
    print(" ======================  Initializing GUI windows ====================== ")
    
    # æ ¹æ®ç›¸æœºé…ç½®åŠ¨æ€åˆ›å»ºçª—å£
    if camera_config is None:
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤3ç›¸æœºé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        from configs.config import topic_info
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    
    # ç›¸æœºåç§°åˆ°çª—å£åç§°çš„æ˜ å°„
    camera_window_map = {
        'image': 'head Camera',
        'chest_image': 'chest Camera',
        'left_shoulder_image': 'left_shoulder Camera',
        'right_shoulder_image': 'right_shoulder Camera'
    }
    
    # åˆ›å»ºç›¸æœºçª—å£
    for camera_name in camera_config.keys():
        if camera_name in camera_window_map:
            window_name = camera_window_map[camera_name]
            cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(window_name, 640, 480)
            print(f"   Created window: {window_name}")
    
    print(f" ======================  GUI windows ready ({len(camera_config)} cameras) ====================== ")

# GUIçª—å£å°†åœ¨è§£æå‘½ä»¤è¡Œå‚æ•°ååˆå§‹åŒ–

from collections import deque
from typing import Optional
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "robot_envs")))
from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, get_camera_names, CAMERA_COMPONENTS, ACTION_COMPONENTS
from configs.config import ROBOT_VERSION
# ä½¿ç”¨GrootPolicyæ¨¡å‹
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# import torchvision
# import matplotlib.pyplot as plt
from pathlib import Path
import torch
import time
import argparse
import rospy
from std_msgs.msg import Float64MultiArray
from kuavo_humanoid_sdk.kuavo_strategy_pytree.common.robot_sdk import RobotSDK
from kuavo_humanoid_sdk.msg.kuavo_msgs.srv import (changeArmCtrlMode, changeArmCtrlModeRequest)

# Default MODEL_ACTION_DT - can be overridden by command line argument
# This represents the time interval between predicted actions during training
# Smaller values = higher inference frequency (e.g., 0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz)
DEFAULT_MODEL_ACTION_DT = 0.1
MODEL_ACTION_DT = DEFAULT_MODEL_ACTION_DT  # Will be updated by command line argument if provided
MODEL_ACTION_FREQUENCY = 1.0 / MODEL_ACTION_DT
TARGET_CONTROL_FREQUENCY = 100.0
TARGET_CONTROL_DT = 1.0 / TARGET_CONTROL_FREQUENCY
CHUNK_TRANSITION_DURATION_S = 0.2  # seconds of low-pass smoothing at chunk boundary
LOWPASS_ALPHA = 0.85  # closer to 1 => smoother (slower) transitions
ENABLE_CHUNK_TRANSITION_LOWPASS = True  # Enable/disable low-pass filtering at chunk boundaries (default: False, only linear interpolation within chunks)


def resample_action_chunk(action_chunk: np.ndarray,
                          source_dt: float = MODEL_ACTION_DT,
                          target_dt: float = TARGET_CONTROL_DT) -> np.ndarray:
    """
    Resample an action chunk predicted at a lower frequency to a higher control frequency.

    Args:
        action_chunk: Array of shape (N, action_dim) predicted at intervals of source_dt.
        source_dt: Time interval between successive actions in the chunk.
        target_dt: Desired time interval for control commands.

    Returns:
        Array of shape (M, action_dim) where M approximates (N-1)*source_dt/target_dt + 1,
        interpolated with linear interpolation along time.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if action_chunk.shape[0] <= 1 or np.isclose(source_dt, target_dt):
        # Nothing to resample, either single action or already at target frequency
        return action_chunk

    total_duration = source_dt * (action_chunk.shape[0] - 1)
    if total_duration <= 0:
        repeat_factor = max(int(round(source_dt / target_dt)), 1)
        return np.repeat(action_chunk, repeats=repeat_factor, axis=0)

    num_target_steps = int(round(total_duration / target_dt)) + 1
    source_times = np.linspace(0.0, total_duration, num=action_chunk.shape[0])
    target_times = np.linspace(0.0, total_duration, num=num_target_steps)

    interpolated = np.empty((num_target_steps, action_chunk.shape[1]), dtype=action_chunk.dtype)
    for dim in range(action_chunk.shape[1]):
        interpolated[:, dim] = np.interp(target_times, source_times, action_chunk[:, dim])

    return interpolated


def apply_lowpass_transition(actions: np.ndarray,
                             previous_action: Optional[np.ndarray],
                             alpha: float = LOWPASS_ALPHA,
                             transition_steps: Optional[int] = None,
                             smooth_slice: slice | tuple | np.ndarray = slice(None)) -> np.ndarray:
    """
    Smooth the beginning of a resampled chunk with an exponential low-pass filter
    to reduce discontinuities at chunk boundaries.

    Args:
        actions: Resampled action chunk at control frequency, shape (N, action_dim).
        previous_action: Last action that was executed on the robot. If None, no smoothing applied.
        alpha: Low-pass smoothing coefficient (0 < alpha < 1). Larger alpha = smoother/slower response.
        transition_steps: Number of control steps over which to apply smoothing. If None, smooth entire chunk.
        smooth_slice: Indices/slice specifying which action dimensions to smooth (e.g., only arm joints).

    Returns:
        Smoothed action chunk (same shape as input).
    """
    if previous_action is None:
        return actions

    actions = np.asarray(actions)
    if actions.ndim == 1:
        actions = actions.reshape(1, -1)

    smoothed = actions.copy()
    prev = np.asarray(previous_action, dtype=smoothed.dtype)
    if prev.ndim == 1:
        prev = prev.reshape(1, -1)
    prev = prev[0]

    num_steps = smoothed.shape[0]
    if transition_steps is None or transition_steps > num_steps:
        transition_steps = num_steps
    transition_steps = max(1, transition_steps)

    if isinstance(smooth_slice, slice) or isinstance(smooth_slice, tuple):
        smooth_indices = smooth_slice
    else:
        smooth_indices = smooth_slice

    for idx in range(transition_steps):
        prev_slice = prev[smooth_indices]
        smoothed_slice = smoothed[idx][smooth_indices]
        filtered = alpha * prev_slice + (1.0 - alpha) * smoothed_slice
        prev[smooth_indices] = filtered
        smoothed[idx][smooth_indices] = filtered

    return smoothed


def resample_chunk_with_claw_hold(action_chunk: np.ndarray,
                                  previous_action: Optional[np.ndarray],
                                  control_frequency: float,
                                  source_dt: float = MODEL_ACTION_DT,
                                  arm_dims: slice = slice(0, 14),
                                  claw_dims: slice = slice(14, 16)) -> np.ndarray:
    """
    Resample an action chunk so that arm joints are interpolated to the control frequency
    while claw positions are held at the original (low) frequency.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if previous_action is not None:
        chunk_with_bridge = np.vstack([previous_action, action_chunk])
        resampled = resample_action_chunk(
            chunk_with_bridge,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )[1:]
        source_array = chunk_with_bridge
    else:
        resampled = resample_action_chunk(
            action_chunk,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )
        source_array = action_chunk

    # Zero-order hold for claw dimensions (keep 10Hz updates)
    if source_array.shape[0] > 0 and resampled.shape[0] > 0:
        total_duration = source_dt * max(source_array.shape[0] - 1, 1)
        if total_duration <= 0:
            hold_indices = np.zeros(resampled.shape[0], dtype=int)
        else:
            target_times = np.linspace(0.0, total_duration, num=resampled.shape[0], endpoint=True)
            source_times = np.linspace(0.0, total_duration, num=source_array.shape[0], endpoint=True)
            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
            hold_indices = np.clip(hold_indices, 0, source_array.shape[0] - 1)
        resampled[:, claw_dims] = source_array[hold_indices][:, claw_dims]

    return resampled

def change_arm_ctrl_mode(control_mode):
    rospy.wait_for_service('/humanoid_change_arm_ctrl_mode')
    try:
        change_mode = rospy.ServiceProxy('/humanoid_change_arm_ctrl_mode', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("æ‰‹è‡‚æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†æ‰‹è‡‚æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)

def direct_to_wbc(control_mode):
    """
        åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
        Args:
            control_mode: æ§åˆ¶æ¨¡å¼
                0: ç¦ç”¨wbcæ§åˆ¶è½¨è¿¹æ¨¡å¼
                1: wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
    """
    rospy.wait_for_service('/enable_wbc_arm_trajectory_control', timeout=5)
    try:
        change_mode = rospy.ServiceProxy('/enable_wbc_arm_trajectory_control', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)


def replay(lerobot_dataset_path, episode, control_arm=True, control_claw=True):
    """
    ç›´æ¥replayæ•°æ®é›†é‡Œçš„è½¨è¿¹ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    """
    repo_id = 0

    dataset = LeRobotDataset(repo_id=repo_id, root=lerobot_dataset_path, episodes=[episode])
    actions = dataset.hf_dataset.select_columns("action")
    env = GrabBoxMpcEnv()
    env.obs_buffer.wait_buffer_ready()
    time.sleep(1)

    for idx in range(dataset.num_frames):
        action = actions[idx]["action"]
        action = np.expand_dims(action, axis=0)

        env.exec_actions(actions=action,
                         control_arm=control_arm,
                         control_claw=control_claw,
                         )

def publish_joint_positions(action_chunk,
                            joint_pub,
                            source_frequency_hz: float,
                            target_frequency_hz: Optional[float] = None):
    """
    ä»åŠ¨ä½œå—ä¸­æå–å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®å¹¶åˆå¹¶å‘å¸ƒåˆ°ROSè¯é¢˜ï¼Œ
    å¯é€‰åœ°å°†åŠ¨ä½œæ’å€¼åˆ°æ›´é«˜çš„æ§åˆ¶é¢‘ç‡åå†å‘å¸ƒã€‚
    
    Args:
        action_chunk: shapeä¸º(N, action_dim)çš„åŠ¨ä½œå—
                     æ”¯æŒæ ¼å¼:
                     - 16ç»´: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
                     - 18ç»´: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
        joint_pub: å…³èŠ‚ä½ç½®å‘å¸ƒå™¨
        source_frequency_hz: åŸå§‹åŠ¨ä½œå—çš„é¢‘ç‡ï¼ˆHzï¼‰
        target_frequency_hz: å¦‚æœæä¾›ï¼Œåˆ™å°†åŠ¨ä½œå—æ’å€¼åˆ°è¯¥é¢‘ç‡åå†å‘å¸ƒ
    """
    try:
        action_chunk = np.asarray(action_chunk)
        if action_chunk.ndim == 1:
            action_chunk = action_chunk.reshape(1, -1)

        if target_frequency_hz is not None and target_frequency_hz > source_frequency_hz:
            action_chunk = resample_action_chunk(
                action_chunk,
                source_dt=1.0 / source_frequency_hz,
                target_dt=1.0 / target_frequency_hz
            )

        action_dim = action_chunk.shape[1]
        
        # æ”¯æŒ16ç»´å’Œ18ç»´åŠ¨ä½œæ ¼å¼
        if action_dim == 16:
            # 16ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
            rospy.logdebug(f"Using depalletize 16-dim action format")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            # åˆå¹¶å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œæœ€åæ˜¯å¤¹çˆªä½ç½®
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions], axis=1)  # shape: (action_chunk_size, 16)
            
        elif action_dim == 18:
            # 18ç»´æ ¼å¼: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
            # å‘é€å®Œæ•´çš„18ç»´æ•°æ®
            rospy.logdebug(f"Using depalletize 18-dim action format (including cmd_pose dimensions)")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            cmd_pose = action_chunk[:, 16:18]  # shape: (action_chunk_size, 2)
            # åˆå¹¶æ‰€æœ‰ç»„ä»¶ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œç„¶åæ˜¯å¤¹çˆªï¼Œæœ€åæ˜¯cmd_pose
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions, cmd_pose], axis=1)  # shape: (action_chunk_size, 18)
            
        else:
            rospy.logwarn(f"Action chunk dimension {action_dim} not supported (expected 16 or 18 for depalletize task)")
            return

        # å‘å¸ƒåˆå¹¶åçš„å…³èŠ‚ä½ç½®ï¼ˆå®Œæ•´çš„action_dimç»´åº¦ï¼‰
        joint_msg = Float64MultiArray()
        joint_msg.data = combined_joints.flatten().tolist()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        joint_pub.publish(joint_msg)
        
        rospy.logdebug(f"Published combined joint positions: {combined_joints.shape} (action_dim={action_dim})")
        
    except Exception as e:
        rospy.logerr(f"Error publishing joint positions: {str(e)}")

def load_and_replay_init_trajectory(bag_path: str, env, control_arm: bool = True, control_claw: bool = True):
    """
    ä»rosbagæ–‡ä»¶ä¸­åŠ è½½åˆå§‹è½¨è¿¹å¹¶å›æ”¾
    
    Args:
        bag_path: rosbagæ–‡ä»¶è·¯å¾„
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
    """
    if not os.path.exists(bag_path):
        rospy.logerr(f"Bag file not found: {bag_path}")
        return False
    
    rospy.loginfo(f"Loading initial trajectory from bag: {bag_path}")
    
    # æœŸæœ›çš„å…³èŠ‚åç§°é¡ºåºï¼ˆä¸publish_target_arm_clawä¸­çš„é¡ºåºä¸€è‡´ï¼‰
    expected_joint_names = [
        "zarm_l1_joint", "zarm_l2_joint", "zarm_l3_joint", "zarm_l4_joint", 
        "zarm_l5_joint", "zarm_l6_joint", "zarm_l7_joint",
        "zarm_r1_joint", "zarm_r2_joint", "zarm_r3_joint", "zarm_r4_joint", 
        "zarm_r5_joint", "zarm_r6_joint", "zarm_r7_joint",
    ]
    
    # è¯»å–bagæ–‡ä»¶ä¸­çš„JointStateæ¶ˆæ¯
    joint_states = []
    try:
        with rosbag.Bag(bag_path, 'r') as bag:
            topic_name = '/mm_kuavo_arm_traj'
            
            # æ£€æŸ¥è¯é¢˜æ˜¯å¦å­˜åœ¨
            bag_info = bag.get_type_and_topic_info()
            if topic_name not in bag_info[1]:
                rospy.logwarn(f"Topic {topic_name} not found in bag file. Available topics: {list(bag_info[1].keys())}")
                return False
            
            # è¯»å–æ‰€æœ‰JointStateæ¶ˆæ¯
            # æ³¨æ„ï¼šç”±äºå·²ç»é€šè¿‡topic_nameè¿‡æ»¤ï¼Œæ‰€æœ‰æ¶ˆæ¯éƒ½åº”è¯¥æ˜¯JointStateç±»å‹
            # ä½†isinstanceæ£€æŸ¥å¯èƒ½ä¸å·¥ä½œï¼ˆrosbagå¯èƒ½è¿”å›åŒ…è£…ç±»å‹ï¼‰ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨æ¶ˆæ¯
            message_count = 0
            for topic, msg, t in bag.read_messages(topics=[topic_name]):
                message_count += 1
                # ç›´æ¥ä½¿ç”¨æ¶ˆæ¯ï¼Œä¸è¿›è¡Œç±»å‹æ£€æŸ¥ï¼ˆå› ä¸ºå·²ç»é€šè¿‡topicè¿‡æ»¤ï¼‰
                joint_states.append({
                    'timestamp': t.to_sec(),
                    'msg': msg
                })
            
            rospy.loginfo(f"Read {message_count} messages from topic {topic_name}")
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            joint_states.sort(key=lambda x: x['timestamp'])
            
            if len(joint_states) == 0:
                rospy.logwarn(f"No JointState messages found in topic {topic_name}")
                return False
            
            rospy.loginfo(f"Loaded {len(joint_states)} joint states from bag file")
            
    except Exception as e:
        rospy.logerr(f"Error loading bag file: {e}")
        return False
    
    # è·å–å½“å‰å¤¹çˆªçŠ¶æ€ï¼ˆç”¨äºå¡«å……16ç»´actionï¼‰
    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
    try:
        obs_data, _, _, robot_obs, _ = env.get_obs()
        if 'claw_state' in robot_obs and len(robot_obs['claw_state']) > 0:
            # è·å–æœ€æ–°çš„å¤¹çˆªçŠ¶æ€
            claw_data = robot_obs['claw_state']
            if claw_data.ndim == 2:
                # å¦‚æœæ˜¯2Dæ•°ç»„ï¼Œå–æœ€åä¸€è¡Œ
                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
            elif claw_data.ndim == 1:
                # å¦‚æœæ˜¯1Dæ•°ç»„ï¼Œç›´æ¥ä½¿ç”¨
                current_claw_state = np.array(claw_data, dtype=np.float32)
            
            # ç¡®ä¿æ˜¯2ç»´
            if current_claw_state.shape[0] != 2:
                rospy.logwarn(f"Claw state has unexpected shape: {current_claw_state.shape}, using default")
                current_claw_state = np.array([0.0, 0.0])
            else:
                rospy.loginfo(f"Current claw state: {current_claw_state}")
    except Exception as e:
        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
        current_claw_state = np.array([0.0, 0.0])
    
    # å›æ”¾è½¨è¿¹ï¼ˆæŒ‰ç…§rosbagä¸­çš„æ—¶é—´æˆ³é—´éš”ï¼‰
    rospy.loginfo("Starting trajectory replay...")
    replay_start_time = time.time()
    bag_start_timestamp = joint_states[0]['timestamp']  # bagä¸­çš„ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³
    
    for i, joint_data in enumerate(joint_states):
        msg = joint_data['msg']
        bag_timestamp = joint_data['timestamp']
        
        # æå–å…³èŠ‚ä½ç½®
        # JointStateçš„positionæ˜¯è§’åº¦ï¼ˆåº¦ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºå¼§åº¦
        if len(msg.position) < 14:
            rospy.logwarn(f"JointState message {i} has insufficient positions: {len(msg.position)} < 14")
            continue
        
        # ç›´æ¥ä½¿ç”¨positionæ•°ç»„çš„å‰14ä¸ªå…ƒç´ ï¼ˆè·³è¿‡åç§°æ£€æŸ¥ï¼‰
        # bagæ–‡ä»¶ä¸­çš„å…³èŠ‚é¡ºåºæ˜¯: arm_joint_1 ~ arm_joint_14
        # å¯¹åº”: å·¦æ‰‹7ä¸ªå…³èŠ‚ + å³æ‰‹7ä¸ªå…³èŠ‚
        # ç›´æ¥ä½¿ç”¨å‰14ä¸ªä½ç½®ï¼Œå‡è®¾é¡ºåºæ­£ç¡®
        arm_action = np.deg2rad(np.array(msg.position[:14]))
        
        # ç»„åˆæˆ16ç»´action: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
        action = np.concatenate([arm_action, current_claw_state])
        
        # è®¡ç®—åº”è¯¥ç­‰å¾…çš„æ—¶é—´ï¼ˆæŒ‰ç…§bagä¸­çš„æ—¶é—´æˆ³é—´éš”ï¼‰
        if i == 0:
            # ç¬¬ä¸€ä¸ªåŠ¨ä½œç«‹å³æ‰§è¡Œ
            expected_elapsed = 0.0
        else:
            # è®¡ç®—ä»bagå¼€å§‹åˆ°å½“å‰æ¶ˆæ¯åº”è¯¥ç»è¿‡çš„æ—¶é—´
            bag_elapsed = bag_timestamp - bag_start_timestamp
            # è®¡ç®—å®é™…ç»è¿‡çš„æ—¶é—´
            actual_elapsed = time.time() - replay_start_time
            # éœ€è¦ç­‰å¾…çš„æ—¶é—´
            expected_elapsed = bag_elapsed - actual_elapsed
        
        # å¦‚æœæ—¶é—´è¿˜æ²¡åˆ°ï¼Œç­‰å¾…
        if expected_elapsed > 0:
            time.sleep(expected_elapsed)
        
        # æ‰§è¡ŒåŠ¨ä½œï¼ˆä¸ä½¿ç”¨env.exec_actionsï¼Œå› ä¸ºå®ƒä¼šæŒ‰ç…§100Hzé¢‘ç‡æ§åˆ¶ï¼Œæˆ‘ä»¬ç›´æ¥å‘å¸ƒï¼‰
        # ç›´æ¥ä½¿ç”¨envçš„target_publisherå‘å¸ƒï¼Œä¸ç»è¿‡env.exec_actionsçš„é¢‘ç‡æ§åˆ¶
        env.target_publisher.publish_target_arm_claw(
            arm_action=arm_action,
            claw_action=current_claw_state,
            control_arm=control_arm,
            control_claw=control_claw
        )
        
        # æ‰“å°è¿›åº¦
        if (i + 1) % 10 == 0 or i == len(joint_states) - 1:
            elapsed = time.time() - replay_start_time
            bag_total_time = joint_states[-1]['timestamp'] - bag_start_timestamp
            rospy.loginfo(f"Replayed {i + 1}/{len(joint_states)} steps (elapsed: {elapsed:.2f}s, bag time: {bag_total_time:.2f}s)")
    
    total_time = time.time() - replay_start_time
    bag_total_time = joint_states[-1]['timestamp'] - bag_start_timestamp
    rospy.loginfo(f"Trajectory replay completed! Real time: {total_time:.2f}s, Bag time: {bag_total_time:.2f}s, {len(joint_states)} steps")
    
    return True

def reset_inference_state(policy, env):
    """
    é‡ç½®æ¨ç†çŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ¬¡æ¨ç†åšå‡†å¤‡
    
    Args:
        policy: GrootPolicyæ¨¡å‹å®ä¾‹
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
    """
    rospy.loginfo("ğŸ”„ Resetting inference state...")
    
    # é‡ç½®policyçŠ¶æ€
    policy.reset()
    rospy.loginfo("   âœ… Policy reset")
    
    # ç­‰å¾…bufferé‡æ–°readyï¼ˆbufferä¼šè‡ªåŠ¨ä¿æŒæœ€æ–°æ•°æ®ï¼Œä½†ç¡®ä¿æ•°æ®å……è¶³ï¼‰
    rospy.loginfo("   â³ Waiting for buffer to be ready...")
    env.obs_buffer.wait_buffer_ready()
    rospy.loginfo("   âœ… Buffer ready")
    
    rospy.loginfo("âœ… Inference state reset complete")


def load_model_and_env(ckpt_path, model_type, action_chunk_size=50, lerobot_dataset_path=None, enable_gui=False, rotate_head_camera=False, state_zero=False, task_description=None):
    """
    åŠ è½½æ¨¡å‹å’Œç¯å¢ƒï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
    
    Returns:
        tuple: (policy, preprocessor, postprocessor, env, dataset_stats, task_description, device)
    """
    # ---------- 1. load GrootPolicy from checkpoint ---------------
    device = "cuda:0"
    print(" =================== Loading GrootPolicy =================== ")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = action_chunk_size
    
    # Load dataset statistics for normalization and task information
    print(f"\nğŸ“‚ Loading dataset for statistics and task information...")
    dataset_stats = None
    available_tasks = None
    if lerobot_dataset_path:
        try:
            dataset_for_stats = LeRobotDataset(repo_id=0, root=lerobot_dataset_path)
            dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
            print(f"âœ… Dataset statistics loaded: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
            if dataset_stats is None:
                print("âš ï¸ Warning: Dataset has no statistics. Action denormalization may not work correctly.")
            elif 'action' not in dataset_stats:
                print("âš ï¸ Warning: Dataset statistics do not contain 'action' key. Action denormalization may not work correctly.")
            
            # åŠ è½½å¯ç”¨çš„ä»»åŠ¡åˆ—è¡¨
            if hasattr(dataset_for_stats.meta, 'tasks') and dataset_for_stats.meta.tasks is not None:
                available_tasks = list(dataset_for_stats.meta.tasks.index)
                print(f"âœ… Available tasks in dataset: {available_tasks}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load dataset statistics: {e}")
            print("   This may cause normalization issues during inference")
    else:
        print("âš ï¸ Warning: No dataset path provided. Using default dataset for statistics.")
        try:
            dataset_for_stats = LeRobotDataset(repo_id=0, root='/home/lab/lerobot_groot/lerobot_data/new_demo/1118_sim_depalletize')
            dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
            print(f"âœ… Dataset statistics loaded from default path: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
            if dataset_stats is None:
                print("âš ï¸ Warning: Default dataset has no statistics. Action denormalization may not work correctly.")
            elif 'action' not in dataset_stats:
                print("âš ï¸ Warning: Default dataset statistics do not contain 'action' key. Action denormalization may not work correctly.")
            
            # åŠ è½½å¯ç”¨çš„ä»»åŠ¡åˆ—è¡¨
            if hasattr(dataset_for_stats.meta, 'tasks') and dataset_for_stats.meta.tasks is not None:
                available_tasks = list(dataset_for_stats.meta.tasks.index)
                print(f"âœ… Available tasks in default dataset: {available_tasks}")
        except Exception as e:
            print(f"âš ï¸ Warning: Could not load default dataset statistics: {e}")
            print("   This may cause normalization issues during inference")
    
    # ç¡®å®šè¦ä½¿ç”¨çš„ä»»åŠ¡æè¿°
    if task_description is None:
        if available_tasks and len(available_tasks) > 0:
            # ä½¿ç”¨æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªä»»åŠ¡ä½œä¸ºé»˜è®¤å€¼
            task_description = available_tasks[0]
            print(f"ğŸ“ Using first task from dataset as default: '{task_description}'")
        else:
            # ä½¿ç”¨é€šç”¨é»˜è®¤å€¼
            task_description = "Depalletize the box"
            print(f"ğŸ“ No task found in dataset, using default: '{task_description}'")
    else:
        print(f"ğŸ“ Using provided task description: '{task_description}'")
    
    # å¦‚æœæä¾›äº†ä»»åŠ¡æè¿°ä½†ä¸åœ¨å¯ç”¨ä»»åŠ¡åˆ—è¡¨ä¸­ï¼Œç»™å‡ºè­¦å‘Š
    if available_tasks and task_description not in available_tasks:
        print(f"âš ï¸ Warning: Task '{task_description}' not found in dataset tasks: {available_tasks}")
        print(f"   Using provided task description anyway...")
    
    # Create preprocessor and postprocessor
    print(f"\nğŸ”§ Creating preprocessor and postprocessor...")
    preprocessor, postprocessor = make_groot_pre_post_processors(
        config=policy.config,
        dataset_stats=dataset_stats,
    )
    print("âœ… Preprocessor and postprocessor created")
    
    # Debug: Print model configuration
    print(f"ğŸ” Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"ğŸ” Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    policy.eval()
    policy.to(device)
    policy.reset()
    
    # Initialize real-time environment
    env = GrabBoxMpcEnv()
    print(f"ğŸ¤– Environment initialized for depalletize task")
    print(" ======================  Waiting for buffer ready ====================== ")
    env.obs_buffer.wait_buffer_ready()
    print(" ======================  Buffer ready ====================== ")
    time.sleep(1)
    
    return policy, preprocessor, postprocessor, env, dataset_stats, task_description, device

def set_arm_quick_mode(enable: bool) -> bool:
    """å¼€å…³æ‰‹è‡‚å¿«é€Ÿæ¨¡å¼"""
    rospy.loginfo(f"call set_arm_quick_mode:{enable}")
    try:
        rospy.wait_for_service('/enable_lb_arm_quick_mode', timeout=5.0)
        cli = rospy.ServiceProxy('/enable_lb_arm_quick_mode', SetBool)
        resp = cli(enable)
        if resp.success:
            rospy.loginfo(f"Successfully {'enabled' if enable else 'disabled'} arm quick mode")
            return True
        else:
            rospy.logwarn(f"Failed to {'enable' if enable else 'disable'} arm quick mode")
            return False
    except rospy.ServiceException as e:
        rospy.logerr(f"Service call failed: {e}")
        return False

def run_inference_loop(policy, preprocessor, env, dataset_stats, task_description, device, 
                       control_arm=True, control_claw=True, action_chunk_size=50, 
                       enable_gui=False, rotate_head_camera=False, state_zero=False,
                       is_first_inference=True, skip_chunk_ratio=0.0, model_action_dt=None):
    """
    è¿è¡Œæ¨ç†å¾ªç¯ï¼ˆå¯ä»¥å¤šæ¬¡è°ƒç”¨ï¼Œæ¯æ¬¡è°ƒç”¨å¼€å§‹æ–°çš„æ¨ç†ä¼šè¯ï¼‰
    
    Args:
        policy: å·²åŠ è½½çš„GrootPolicyæ¨¡å‹
        preprocessor: é¢„å¤„ç†å™¨
        env: å·²åˆå§‹åŒ–çš„GrabBoxMpcEnvç¯å¢ƒ
        dataset_stats: æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        task_description: ä»»åŠ¡æè¿°
        device: è®¾å¤‡
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        enable_gui: æ˜¯å¦å¯ç”¨GUI
        rotate_head_camera: æ˜¯å¦æ—‹è½¬å¤´éƒ¨ç›¸æœº
        state_zero: æ˜¯å¦å°†çŠ¶æ€ç½®é›¶
        is_first_inference: æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡æ¨ç†ï¼ˆç¬¬ä¸€æ¬¡ä¼šåŠ è½½bagæ–‡ä»¶ï¼Œåç»­ä½¿ç”¨jsonæ–‡ä»¶é‡ç½®ï¼‰
        skip_chunk_ratio: è·³è¿‡chunkçš„å‰ç™¾åˆ†ä¹‹å¤šå°‘
        model_action_dt: æ¨¡å‹åŠ¨ä½œæ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ§åˆ¶æ¨ç†é¢‘ç‡ã€‚å¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨å±€MODEL_ACTION_DT
    
    Returns:
        bool: Trueè¡¨ç¤ºæ­£å¸¸é€€å‡ºï¼ˆæŒ‰qï¼‰ï¼ŒFalseè¡¨ç¤ºè¢«ä¸­æ–­ï¼ˆCtrl+Cï¼‰
    """
    # ä½¿ç”¨ä¼ å…¥çš„model_action_dtæˆ–å…¨å±€MODEL_ACTION_DT
    if model_action_dt is None:
        model_action_dt = MODEL_ACTION_DT
    model_action_frequency = 1.0 / model_action_dt
    # Print action mode configuration
    print("\n" + "="*80)
    print("ğŸ¯ DEPALLETIZE TASK CONFIGURATION (GrootPolicy)")
    print("="*80)
    print(f"ğŸ¤– Control arm: {control_arm}")
    print(f"ğŸ¦¾ Control claw: {control_claw}")
    print(f"ğŸ“Š ACTION_COMPONENTS: {ACTION_COMPONENTS}")
    print(f"ğŸ“Š Action dimension: Will be determined by model output (expected: {len(ACTION_COMPONENTS) * 7 if 'Left_arm' in ACTION_COMPONENTS and 'Right_arm' in ACTION_COMPONENTS else 'varies'}D based on config)")
    print(f"ğŸ“¦ Action chunk size: {action_chunk_size}")
    # æ ¹æ®ACTION_COMPONENTSåˆ¤æ–­æ˜¯å¦åŒ…å«cmd_pose
    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
    print(f"ğŸ¯ Cmd_pose control: {'Enabled' if has_cmd_pose else 'Disabled'} (based on ACTION_COMPONENTS)")
    if rotate_head_camera:
        print(f"ğŸ”„ Head camera rotation enabled: images from 'image' camera will be rotated 180 degrees")
    if state_zero:
        print(f"âš ï¸  STATE ZERO MODE: All state inputs will be set to zero (for dependency testing)")
    if skip_chunk_ratio > 0.0:
        print(f"â­ï¸  Skip chunk ratio: {skip_chunk_ratio*100:.1f}% (will skip first {skip_chunk_ratio*100:.1f}% of each predicted chunk)")
    print(f"âš¡ Model action DT: {model_action_dt:.3f}s (inference frequency: {model_action_frequency:.1f} Hz)")
    print(f"ğŸ“ Task description: '{task_description}'")
    print("="*80 + "\n")
    
    # é‡ç½®policyçŠ¶æ€
    policy.reset()
    
    step_counter = 0

    # Initialize ROS publishers for action visualization
    joint_pub = rospy.Publisher('/policy/action/eef_pose_marker_all', Float64MultiArray, queue_size=10)
    
    rospy.loginfo(f"Initialized ROS publishers for action visualization with chunk size: {action_chunk_size}")
    
    # è·å–åˆå§‹è§‚æµ‹
    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

    # TODO: æ£€æŸ¥æ—¶é—´åŒæ­¥æƒ…å†µ
    # TODO: æ˜¾ç¤ºå›¾åƒ
    rospy.loginfo(f"Initialized action visualization with chunk size: {action_chunk_size}")
    
    # ---------- 2. æ¨¡å‹æ¨ç†ï¼ˆå®æ—¶æ¨¡å¼ï¼‰ ----------------------
    # Real-time environment evaluation loop
    robot_sdk.control.set_external_control_arm_mode()
    time.sleep(1)
    
    # æ ¹æ®æœºå™¨äººç‰ˆæœ¬åˆ‡æ¢æ‰‹è‡‚æ§åˆ¶æ¨¡å¼
    if ROBOT_VERSION == "4_pro":
        direct_to_wbc(1)
        function_key = "direct_to_wbc"
    elif ROBOT_VERSION == "5_wheel":
        set_arm_quick_mode(True)
        function_key = "set_arm_quick_mode"    
    # ç­‰å¾…ä½¿èƒ½ç”Ÿæ•ˆ
    input(f"å½“å‰æœºå™¨äººæ¨¡å¼ä¸º: {ROBOT_VERSION} | æ§åˆ¶æ¨¡å¼ {function_key} ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== \n")
    time.sleep(1.0)
    resampled_action_queue: deque[np.ndarray] = deque()
    last_executed_action: Optional[np.ndarray] = None
    
    # åŠ è½½å¹¶å›æ”¾åˆå§‹è½¨è¿¹ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ¨ç†æ—¶åŠ è½½bagæ–‡ä»¶ï¼‰
    if is_first_inference:
        init_traj_bag_path = '/home/lab/kuavo-manip/robot_depalletize_init_traj.bag'
        if os.path.exists(init_traj_bag_path):
            rospy.loginfo("Loading and replaying initial trajectory from bag file (first inference only)...")
            load_and_replay_init_trajectory(
                bag_path=init_traj_bag_path,
                env=env,
                control_arm=control_arm,
                control_claw=control_claw
            )
            rospy.loginfo("Initial trajectory replay completed. Starting model inference...")
            time.sleep(1.0)
        else:
            rospy.logwarn(f"Initial trajectory bag file not found: {init_traj_bag_path}")
            rospy.logwarn("Skipping initial trajectory replay. Starting model inference directly...")
        
        input(f"è½¨è¿¹å›æ”¾ ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== è½¨è¿¹å›æ”¾æˆåŠŸ ==== \n")
        time.sleep(1.0)
    else:
        rospy.loginfo("Skipping bag file replay (not first inference). Using JSON reset instead.")
    
    print("\n" + "="*80)
    print("ğŸš€ Starting inference loop...")
    print("ğŸ’¡ Press 'q' + Enter to stop current inference and prepare for next run")
    print("ğŸ’¡ Press Ctrl+C to exit the program completely")
    print("="*80 + "\n")
    
    while True:
        try:
            state = torch.from_numpy(obs_data["state"]).float()
            # print(f" ==== state ==== {state.shape} ==== ")
            
            # æ ¹æ®CAMERA_COMPONENTSåŠ¨æ€å¤„ç†ç›¸æœºå›¾åƒ
            # å¡«å……ç½‘ç»œçš„obs
            observation = {}
            
            # æ ¹æ®CAMERA_COMPONENTSæ˜ç¡®æŒ‡å®šéœ€è¦å¤„ç†çš„ç›¸æœº
            camera_names = get_camera_names(CAMERA_COMPONENTS)
            for camera_name in camera_names:
                # æ£€æŸ¥ç›¸æœºæ•°æ®æ˜¯å¦åœ¨obs_dataä¸­
                if camera_name in obs_data:
                    # è·å–ç›¸æœºå›¾åƒï¼Œobs_dataä¸­çš„å›¾åƒæ ¼å¼æ˜¯ (T, H, W, C)ï¼Œå…¶ä¸­Tæ˜¯æ—¶é—´æ­¥æ•°
                    camera_img_np = obs_data[camera_name]
                    
                    # æ£€æŸ¥å›¾åƒç»´åº¦ï¼Œåº”è¯¥æ˜¯ (T, H, W, C) æ ¼å¼
                    if camera_img_np.ndim != 4:
                        rospy.logwarn(f"âš ï¸  Unexpected camera image shape: {camera_img_np.shape}, expected (T, H, W, C)")
                        continue
                    
                    # å¦‚æœå¯ç”¨å¤´éƒ¨ç›¸æœºæ—‹è½¬ä¸”å½“å‰æ˜¯å¤´éƒ¨ç›¸æœºï¼ˆimageï¼‰ï¼Œåˆ™å¯¹æ¯ä¸€å¸§æ—‹è½¬180åº¦
                    if rotate_head_camera and camera_name == "image":
                        # æ—‹è½¬180åº¦ï¼šä½¿ç”¨np.rot90ï¼Œk=2è¡¨ç¤ºæ—‹è½¬180åº¦ï¼Œaxes=(1,2)è¡¨ç¤ºåœ¨Hå’ŒWç»´åº¦ä¸Šæ—‹è½¬
                        # camera_img_np shape: (T, H, W, C)
                        # å¯¹æ¯ä¸€å¸§è¿›è¡Œæ—‹è½¬ï¼Œaxes=(1,2)è¡¨ç¤ºåœ¨Hå’ŒWç»´åº¦ä¸Šæ—‹è½¬ï¼ˆä¿æŒTå’ŒCç»´åº¦ä¸å˜ï¼‰
                        # æ³¨æ„ï¼šnp.rot90å¯èƒ½äº§ç”Ÿè´Ÿæ­¥é•¿çš„è§†å›¾ï¼Œéœ€è¦copy()æ¥åˆ›å»ºè¿ç»­æ•°ç»„ï¼Œä»¥ä¾¿PyTorchå¯ä»¥å¤„ç†
                        camera_img_np = np.rot90(camera_img_np, k=2, axes=(1, 2)).copy()
                    
                    # è½¬æ¢ä¸º (T, C, H, W) æ ¼å¼å¹¶å½’ä¸€åŒ–
                    # ä½¿ç”¨np.moveaxiså°† (T, H, W, C) è½¬æ¢ä¸º (T, C, H, W)
                    # æ³¨æ„ï¼šnp.moveaxisä¹Ÿå¯èƒ½äº§ç”Ÿè´Ÿæ­¥é•¿ï¼Œä½¿ç”¨copy()ç¡®ä¿æ•°ç»„è¿ç»­
                    camera_images = torch.from_numpy(np.moveaxis(camera_img_np, 3, 1).copy()).float() / 255
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼: observation.images.cam_*
                    obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                    observation[obs_key] = camera_images.to('cuda:0')
                else:
                    # åªåœ¨ç¬¬ä¸€æ¬¡å‡ºç°æ—¶æ‰“å°è­¦å‘Š
                    if step_counter == 0:
                        rospy.logwarn(f"âš ï¸  Camera '{camera_name}' from CAMERA_COMPONENTS not found in obs_data. Available cameras: {[k for k in obs_data.keys() if 'image' in k.lower()]}")

            # observation['observation.environment_state'] = environment_state
            # å¦‚æœå¯ç”¨state_zeroæ¨¡å¼ï¼Œå°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
            if state_zero:
                # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰çŠ¶æ€å€¼è®¾ä¸º0
                observation['observation.state'] = torch.zeros_like(state).to('cuda:0')
            else:
                observation['observation.state'] = state.to('cuda:0')
            
            # æ·»åŠ  task å­—æ®µï¼ˆlanguage instructionï¼‰
            # processor ä¼šä» complementary_data ä¸­çš„ "task" å­—æ®µè¯»å–å¹¶è½¬æ¢ä¸º language
            observation['task'] = task_description

            if not resampled_action_queue:
                # ä½¿ç”¨GrootPolicyçš„predict_action_chunk
                # é¦–å…ˆä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†è¾“å…¥
                processed_observation = preprocessor(observation)
                
                # æ¨¡å‹æ¨ç†
                with torch.inference_mode():
                    pred_actions = policy.predict_action_chunk(processed_observation)
                
                # pred_actions shape: (batch_size, chunk_size, action_dim)
                # æ³¨æ„ï¼špred_actionsæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ŒèŒƒå›´åœ¨[-1, 1]
                # éœ€è¦æ‰‹åŠ¨åå½’ä¸€åŒ–åˆ°çœŸå®å•ä½
                
                # æ‰‹åŠ¨åå½’ä¸€åŒ–æ•´ä¸ªchunk
                if dataset_stats and 'action' in dataset_stats:
                    action_stats = dataset_stats['action']
                    if 'min' in action_stats and 'max' in action_stats:
                        action_min = torch.as_tensor(action_stats['min'], dtype=torch.float32, device=pred_actions.device)
                        action_max = torch.as_tensor(action_stats['max'], dtype=torch.float32, device=pred_actions.device)
                        
                        # ç¡®ä¿ç»´åº¦åŒ¹é…
                        action_dim = pred_actions.shape[-1]
                        if action_min.numel() < action_dim:
                            action_min = torch.nn.functional.pad(action_min.flatten()[:action_dim], (0, max(0, action_dim - action_min.numel())))
                        if action_max.numel() < action_dim:
                            action_max = torch.nn.functional.pad(action_max.flatten()[:action_dim], (0, max(0, action_dim - action_max.numel())))
                        
                        action_min = action_min[:action_dim]
                        action_max = action_max[:action_dim]
                        
                        # åå½’ä¸€åŒ–å…¬å¼ï¼šx = (y + 1) / 2 * (max - min) + min
                        denom = action_max - action_min
                        mask = denom != 0
                        safe_denom = torch.where(mask, denom, torch.ones_like(denom))
                        
                        # åå½’ä¸€åŒ–æ•´ä¸ªchunk
                        pred_actions_unnorm = (pred_actions + 1.0) * 0.5 * safe_denom + action_min
                        pred_actions_unnorm = torch.where(mask, pred_actions_unnorm, action_min)
                        
                        # è½¬æ¢ä¸ºnumpy
                        action_chunk = pred_actions_unnorm[0].cpu().numpy()  # (chunk_size, action_dim)
                    else:
                        # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼ï¼ˆå¯èƒ½å·²ç»æ˜¯åå½’ä¸€åŒ–çš„ï¼‰
                        action_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
                        rospy.logwarn("âš ï¸ Warning: No action min/max stats found. Using raw predictions (may be normalized).")
                else:
                    # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼
                    action_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
                    rospy.logwarn("âš ï¸ Warning: No dataset stats found. Using raw predictions (may be normalized).")

                # æ ¹æ®skip_chunk_ratioè·³è¿‡chunkçš„å‰ç™¾åˆ†ä¹‹å¤šå°‘
                if skip_chunk_ratio > 0.0:
                    chunk_size = action_chunk.shape[0]
                    skip_steps = int(np.round(chunk_size * skip_chunk_ratio))
                    if skip_steps >= chunk_size:
                        rospy.logwarn(f"âš ï¸ Warning: skip_chunk_ratio {skip_chunk_ratio*100:.1f}% results in skipping all {chunk_size} steps. Using last step only.")
                        action_chunk = action_chunk[-1:].copy()  # è‡³å°‘ä¿ç•™æœ€åä¸€æ­¥
                    elif skip_steps > 0:
                        original_size = chunk_size
                        action_chunk = action_chunk[skip_steps:].copy()
                        rospy.loginfo(f"â­ï¸  Skipped first {skip_steps}/{original_size} steps ({skip_chunk_ratio*100:.1f}%) of chunk. Remaining: {action_chunk.shape[0]} steps")

                # æ ¹æ®åŠ¨ä½œç»´åº¦åŠ¨æ€ç¡®å®šclawç»´åº¦
                action_dim = action_chunk.shape[1]
                if action_dim == 16:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                elif action_dim == 18:
                    # 18ç»´æ ¼å¼: å‰14ç»´æ˜¯æ‰‹è‡‚å…³èŠ‚ï¼Œ14-16æ˜¯å¤¹çˆªï¼Œ16-18æ˜¯cmd_poseï¼ˆä¿ç•™å®Œæ•´18ç»´ï¼‰
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                    # æ³¨æ„ï¼šcmd_poseç»´åº¦(16-18)ä¼šåœ¨resampleæ—¶ä¸€èµ·å¤„ç†ï¼Œä¸éœ€è¦å•ç‹¬å¤„ç†
                else:
                    # é»˜è®¤ä½¿ç”¨å‰14ç»´ä½œä¸ºæ‰‹è‡‚ï¼Œ14-16ä½œä¸ºå¤¹çˆª
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, min(16, action_dim))
                    rospy.logwarn(f"Unknown action dimension {action_dim}, using default arm/claw split")
                
                resampled_chunk = resample_chunk_with_claw_hold(
                    action_chunk,
                    previous_action=last_executed_action,
                    control_frequency=env.control_frequency,
                    source_dt=model_action_dt,
                    arm_dims=arm_dims,
                    claw_dims=claw_dims
                )

                # Apply low-pass filtering at chunk boundaries only if enabled
                if ENABLE_CHUNK_TRANSITION_LOWPASS:
                    if last_executed_action is not None:
                        transition_steps = max(
                            1,
                            int(round(env.control_frequency * CHUNK_TRANSITION_DURATION_S))
                        )
                    else:
                        transition_steps = None
                    
                    resampled_chunk = apply_lowpass_transition(
                        resampled_chunk,
                        previous_action=last_executed_action,
                        alpha=LOWPASS_ALPHA,
                        transition_steps=transition_steps,
                        smooth_slice=arm_dims  # åªå¯¹æ‰‹è‡‚å…³èŠ‚è¿›è¡Œä½é€šæ»¤æ³¢
                    )

                publish_joint_positions(
                    resampled_chunk,
                    joint_pub,
                    source_frequency_hz=env.control_frequency,
                    target_frequency_hz=None
                )
                rospy.loginfo(f"Prepared resampled chunk of size {resampled_chunk.shape[0]} for execution")

                resampled_action_queue = deque(np.array(step, copy=True) for step in resampled_chunk)

            current_action = resampled_action_queue.popleft()
            # æ ¹æ®ACTION_COMPONENTSé…ç½®å†³å®šæ˜¯å¦æ§åˆ¶cmd_pose
            # å¦‚æœACTION_COMPONENTSåŒ…å«Cmd_pose_zæˆ–Cmd_pose_pitchï¼Œåˆ™å¯ç”¨cmd_poseæ§åˆ¶
            control_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
            
            env.exec_actions(actions=current_action,
                             control_arm=control_arm,
                             control_claw=control_claw,
                             control_cmd_pose=control_cmd_pose)
            step_counter += 1
            last_executed_action = current_action.copy()

            obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

            # é”®ç›˜ç›‘å¬ï¼ˆæ— è®ºæ˜¯å¦å¯ç”¨GUIéƒ½ç›‘å¬ï¼Œå‚è€ƒeval_depalletize_camera_dagger.pyçš„å®ç°æ–¹å¼ï¼‰
            key = 0
            if enable_gui:
                key = cv2.waitKey(1) & 0xFF
            else:
                # éGUIæ¨¡å¼ä¸‹ä½¿ç”¨éé˜»å¡é”®ç›˜ç›‘å¬
                try:
                    import select
                    if select.select([sys.stdin], [], [], 0)[0]:
                        import termios
                        import tty
                        old_settings = termios.tcgetattr(sys.stdin)
                        try:
                            tty.setraw(sys.stdin.fileno())
                            ch = sys.stdin.read(1)
                            if ch:
                                key = ord(ch)
                        finally:
                            termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
                except (ImportError, OSError, AttributeError):
                    # å¦‚æœselectæˆ–termiosä¸å¯ç”¨ï¼Œè·³è¿‡é”®ç›˜ç›‘å¬
                    pass
            
            if key == ord('q') or key == 27:  # 'q' or ESC to quit current inference
                print("\n[Keyboard] Stopping current inference by user request (q or ESC pressed)")
                return True  # è¿”å›Trueè¡¨ç¤ºæ­£å¸¸é€€å‡ºå½“å‰æ¨ç†
            
        except KeyboardInterrupt:
            print("\n[Interrupted] Exiting by user Ctrl+C.")
            return False  # è¿”å›Falseè¡¨ç¤ºè¢«ä¸­æ–­
    
    return True  # æ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šåˆ°è¾¾è¿™é‡Œ

def final_reset_arm(json_path, env, control_arm=True, control_claw=True):
    """
    ä½¿ç”¨JSONæ–‡ä»¶ä¸­çš„æ‰‹è‡‚è½¨è¿¹é‡ç½®æ‰‹è‡‚ä½ç½®
    
    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«åˆå§‹æ‰‹è‡‚è½¨è¿¹
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
    """
    # å…ˆæ‰“å¼€å¤¹çˆª
    rospy.loginfo("Opening claws before reset...")
    # è·å–å½“å‰çŠ¶æ€
    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
    current_arm_state = obs_data["state"][0][:14]  # å½“å‰æ‰‹è‡‚ä½ç½®
    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
    try:
        if 'claw_state' in robot_obs and len(robot_obs['claw_state']) > 0:
            claw_data = robot_obs['claw_state']
            if claw_data.ndim == 2:
                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
            elif claw_data.ndim == 1:
                current_claw_state = np.array(claw_data, dtype=np.float32)
            if current_claw_state.shape[0] != 2:
                current_claw_state = np.array([0.0, 0.0])
    except Exception as e:
        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
        current_claw_state = np.array([0.0, 0.0])
    
    # æ‰“å¼€å¤¹çˆªï¼ˆè®¾ç½®ä¸º0ï¼‰ï¼Œä¿æŒæ‰‹è‡‚ä½ç½®ä¸å˜
    # æ³¨æ„ï¼šå¤¹çˆªçš„0å€¼è¡¨ç¤ºæ‰“å¼€çŠ¶æ€
    open_claw_value = np.zeros([2])  # [0.0, 0.0] è¡¨ç¤ºæ‰“å¼€å¤¹çˆª
    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
    if has_cmd_pose:
        # 18ç»´æ ¼å¼
        open_claw_action = np.concatenate([current_arm_state, open_claw_value, np.array([0.0, 0.0])])
    else:
        # 16ç»´æ ¼å¼
        open_claw_action = np.concatenate([current_arm_state, open_claw_value])
    env.exec_actions(actions=open_claw_action, control_arm=False, control_claw=control_claw)
    time.sleep(1)
    
    # æ›´æ–°current_claw_stateä¸ºæ‰“å¼€åçš„çŠ¶æ€ï¼ˆ0ï¼‰ï¼Œç”¨äºåç»­çš„æ‰‹è‡‚é‡ç½®è¿‡ç¨‹
    current_claw_state = open_claw_value.copy()

    # åŠ è½½JSONæ–‡ä»¶ä¸­çš„æ‰‹è‡‚è½¨è¿¹
    rospy.loginfo(f"Loading initial arm trajectory from JSON: {json_path}")
    with open(json_path, 'r') as f:
        init_traj = json.load(f)
        arm_actions = init_traj['arm_action']  # List of arm actions
        dt = init_traj.get('dt', 0.1)  # è·å–æ—¶é—´é—´éš”ï¼Œé»˜è®¤0.1ç§’

    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
    init_joints = np.array(arm_actions[-1])  # ç›®æ ‡å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰
    current_joints = obs_data["state"][0][:14]  # å½“å‰å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰

    # ç¡®ä¿init_jointsæ˜¯14ç»´
    if len(init_joints) != 14:
        rospy.logwarn(f"Expected 14 joint positions, got {len(init_joints)}. Using first 14 elements.")
        init_joints = np.array(init_joints[:14])

    total_time = 5.0  # æ€»æ—¶é—´ï¼ˆç§’ï¼‰
    num_points = int(total_time / dt)
    
    rospy.loginfo(f"Resetting arm from current position to initial position over {total_time}s ({num_points} steps)...")
    
    # ä»current_jointsåˆ°init_jointsæ’å€¼
    for i in range(1, num_points + 1):
        # çº¿æ€§æ’å€¼
        alpha = i / num_points
        interp_joints = current_joints + (init_joints - current_joints) * alpha
        
        # æ„å»ºå®Œæ•´çš„åŠ¨ä½œæ•°ç»„ï¼ˆæ ¹æ®ACTION_COMPONENTSæ ¼å¼ï¼‰
        # æ ¹æ®ACTION_COMPONENTSåŠ¨æ€ç¡®å®šåŠ¨ä½œç»´åº¦
        has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
        
        if has_cmd_pose:
            # 18ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_pose]
            # ä¿æŒcmd_poseä¸å˜ï¼ˆä½¿ç”¨å½“å‰å€¼æˆ–0ï¼‰
            cmd_pose = np.array([0.0, 0.0])  # é»˜è®¤cmd_poseå€¼
            action = np.concatenate([interp_joints, current_claw_state, cmd_pose])
        else:
            # 16ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
            action = np.concatenate([interp_joints, current_claw_state])
        
        # ä½¿ç”¨exec_actionsæ‰§è¡ŒåŠ¨ä½œ
        env.exec_actions(actions=action, control_arm=control_arm, control_claw=control_claw)
        time.sleep(dt)
    
    rospy.loginfo("Arm reset completed!")


def eval(ckpt_path, model_type, control_arm=True, control_claw=True, action_chunk_size=50, lerobot_dataset_path=None, enable_gui=False, rotate_head_camera=False, state_zero=False, task_description=None, skip_chunk_ratio=0.0, model_action_dt=None):
    """
    åœ¨è¿™é‡Œå’Œå®æœº/ä»¿çœŸäº¤äº’ï¼Œåšç½‘ç»œæ¨ç†ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    æ”¯æŒå¤šæ¬¡æ¨ç†ï¼šæŒ‰'q'é€€å‡ºå½“å‰æ¨ç†ï¼Œå¯ä»¥å¿«é€Ÿé‡æ–°å¼€å§‹ä¸‹ä¸€æ¬¡æ¨ç†è€Œæ— éœ€é‡æ–°åŠ è½½æ¨¡å‹
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨åªä½¿ç”¨GrootPolicyï¼‰
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        lerobot_dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆç”¨äºåŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯é€‰ï¼‰
        enable_gui: æ˜¯å¦å¯ç”¨GUIçª—å£æ˜¾ç¤ºç›¸æœºå›¾åƒ
        rotate_head_camera: æ˜¯å¦æ—‹è½¬å¤´éƒ¨ç›¸æœºå›¾åƒ180åº¦
        state_zero: æ˜¯å¦å°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
        task_description: ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ï¼ˆlanguage instructionï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ•°æ®é›†åŠ è½½æˆ–ä½¿ç”¨é»˜è®¤å€¼
        skip_chunk_ratio: è·³è¿‡chunkçš„å‰ç™¾åˆ†ä¹‹å¤šå°‘ï¼ˆ0.0-1.0ï¼‰ï¼Œä¾‹å¦‚0.2è¡¨ç¤ºè·³è¿‡å‰20%
        model_action_dt: æ¨¡å‹åŠ¨ä½œæ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ§åˆ¶æ¨ç†é¢‘ç‡ã€‚ä¾‹å¦‚ï¼š0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz
                        å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.1 ç§’ï¼ˆ10 Hzï¼‰
    """
    
    # åŠ è½½æ¨¡å‹å’Œç¯å¢ƒï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    policy, preprocessor, postprocessor, env, dataset_stats, final_task_description, device = load_model_and_env(
        ckpt_path=ckpt_path,
        model_type=model_type,
        action_chunk_size=action_chunk_size,
        lerobot_dataset_path=lerobot_dataset_path,
        enable_gui=enable_gui,
        rotate_head_camera=rotate_head_camera,
        state_zero=state_zero,
        task_description=task_description
    )
    
    # ä¸»å¾ªç¯ï¼šæ”¯æŒå¤šæ¬¡æ¨ç†
    inference_count = 0
    while True:
        try:
            inference_count += 1
            is_first_inference = (inference_count == 1)
            
            print(f"\n{'='*80}")
            print(f"ğŸ”„ Starting inference session #{inference_count}")
            if is_first_inference:
                print(f"ğŸ“¦ First inference: will load bag file for initial trajectory")
            else:
                print(f"ğŸ“¦ Subsequent inference: will use JSON file for arm reset")
            print(f"{'='*80}\n")
            
            # é‡ç½®æ¨ç†çŠ¶æ€
            reset_inference_state(
                policy=policy,
                env=env
            )
            
            # è¿è¡Œæ¨ç†å¾ªç¯
            normal_exit = run_inference_loop(
                policy=policy,
                preprocessor=preprocessor,
                env=env,
                dataset_stats=dataset_stats,
                task_description=final_task_description,
                device=device,
                control_arm=control_arm,
                control_claw=control_claw,
                action_chunk_size=action_chunk_size,
                enable_gui=enable_gui,
                rotate_head_camera=rotate_head_camera,
                state_zero=state_zero,
                is_first_inference=is_first_inference,
                skip_chunk_ratio=skip_chunk_ratio,
                model_action_dt=model_action_dt
            )
            
            if normal_exit:
                # æ­£å¸¸é€€å‡ºï¼ˆæŒ‰qï¼‰ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡æ¨ç†
                print(f"\n{'='*80}")
                print(f"âœ… Inference session #{inference_count} stopped by user (q pressed)")
                cur_dir = os.path.dirname(os.path.abspath(__file__))
                # æ¯æ¬¡é€€å‡ºæ—¶éƒ½ä½¿ç”¨JSONæ–‡ä»¶é‡ç½®æ‰‹è‡‚ä½ç½®
                # ç¬¬ä¸€æ¬¡æ¨ç†å¼€å§‹æ—¶ä½¿ç”¨bagæ–‡ä»¶ï¼Œåç»­æ¨ç†å¼€å§‹æ—¶è·³è¿‡bagæ–‡ä»¶ï¼ˆåœ¨run_inference_loopä¸­å¤„ç†ï¼‰
                rospy.loginfo("Resetting arm position using JSON file...")
                final_reset_arm(
                    json_path=os.path.join(cur_dir, 'utils/initial_arm_traj.json'), 
                    env=env,
                    control_arm=control_arm,
                    control_claw=control_claw
                )
                print(f"ğŸ’¡ Ready for next inference session. Press Enter to start, or Ctrl+C to exit.")
                print(f"{'='*80}\n")
                
                # ç­‰å¾…ç”¨æˆ·è¾“å…¥ä»¥å¼€å§‹ä¸‹ä¸€æ¬¡æ¨ç†
                try:
                    user_input = input("Press Enter to start next inference, or 'q'+Enter to exit: ").strip().lower()
                    if user_input == 'q':
                        print("\nğŸ‘‹ Exiting program. Goodbye!")
                        break
                except (EOFError, KeyboardInterrupt):
                    print("\nğŸ‘‹ Exiting program. Goodbye!")
                    break
            else:
                # è¢«Ctrl+Cä¸­æ–­ï¼Œé€€å‡ºç¨‹åº
                print("\nğŸ‘‹ Exiting program due to Ctrl+C. Goodbye!")
                break
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Exiting program due to Ctrl+C. Goodbye!")
            break
        except Exception as e:
            rospy.logerr(f"Error during inference: {e}")
            import traceback
            traceback.print_exc()
            print("\nâŒ Error occurred. Exiting program.")
            break
    
    # Cleanup GUI windows
    if enable_gui:
        cv2.destroyAllWindows()




if __name__ == '__main__':
    # æœºå™¨äººä½å¤´
    robot_sdk = RobotSDK()
    robot_sdk.control.control_head(0, np.deg2rad(20))
    robot_sdk.control.set_external_control_arm_mode()  # åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼
    print(" ==== æœºå™¨äººå¤´éƒ¨ä¿¯ä»°è°ƒèŠ‚è§’åº¦: 20 æˆåŠŸ ==== ")
    print(" ==== åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== ")
    
    # python å‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description='Depalletize Task Evaluation Script',
        epilog='This script evaluates models for the depalletize task (16-dim actions: 14 arm joints + 2 claw positions).'
    )
    parser.add_argument('--ckpt-path', type=str, default='/home/lab/kuavo-manip/outputs/train/box_only_vel_obs/checkpoints/080000/pretrained_model',
                        help='Path to the checkpoint directory')
    parser.add_argument('--model-type', type=str, default='groot', choices=['groot', 'act', 'dp'],
                        help='Type of model to use (now only groot is supported, act/dp are deprecated)')
    parser.add_argument('--eval', action='store_true', help='Evaluate the model in real-time environment')
    parser.add_argument('--replay', action='store_true', help='Replay the model')
    parser.add_argument('--action_chunk_size', type=int, default=20, help='Number of action steps')
    parser.add_argument('--lerobot_dataset_path', type=str, default=None, help='Path to the LeRobot dataset for loading statistics (optional)')
    parser.add_argument('--enable_gui', action='store_true',
                        help='Enable GUI windows for camera display (default: disabled)')
    parser.add_argument('--rotate-head-camera', action='store_true',
                        help='If set, rotate head camera images (image) by 180 degrees.')
    parser.add_argument('--state-zero', action='store_true',
                        help='If set, set all state inputs to zero (for testing model dependency on state)')
    parser.add_argument('--task-description', type=str, default=None,
                        help='Task description (language instruction) for the model. If not provided, will use the first task from dataset or a default value.')
    parser.add_argument('--skip-chunk-ratio', type=float, default=0.0,
                        help='Skip the first percentage of each predicted chunk (0.0-1.0). For example, 0.2 means skip the first 20%% of the chunk. Default: 0.0 (no skipping)')
    parser.add_argument('--model-action-dt', type=float, default=None,
                        help='Time interval between predicted actions in seconds (controls inference frequency). '
                             'Smaller values = higher frequency. Examples: 0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz. '
                             'Default: 0.1 (10 Hz). Note: Model was trained with 0.1s interval.')
    
    args = parser.parse_args()
    
    # éªŒè¯skip_chunk_ratioèŒƒå›´
    if args.skip_chunk_ratio < 0.0 or args.skip_chunk_ratio >= 1.0:
        parser.error(f"--skip-chunk-ratio must be in range [0.0, 1.0), got {args.skip_chunk_ratio}")
    
    # éªŒè¯model_action_dt
    if args.model_action_dt is not None:
        if args.model_action_dt <= 0.0:
            parser.error(f"--model-action-dt must be positive, got {args.model_action_dt}")
        if args.model_action_dt > 1.0:
            parser.error(f"--model-action-dt seems too large (> 1.0s), got {args.model_action_dt}")
        print(f"âš¡ Using custom MODEL_ACTION_DT: {args.model_action_dt:.3f}s (inference frequency: {1.0/args.model_action_dt:.1f} Hz)")
    else:
        print(f"âš¡ Using default MODEL_ACTION_DT: {DEFAULT_MODEL_ACTION_DT:.3f}s (inference frequency: {1.0/DEFAULT_MODEL_ACTION_DT:.1f} Hz)")
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å’Œç›¸æœºé…ç½®åˆå§‹åŒ–GUIçª—å£
    camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    init_gui_windows(enable_gui=args.enable_gui, camera_config=camera_config)
    
    # æ‰“å°ç›¸æœºé…ç½®ä¿¡æ¯
    camera_names = get_camera_names(CAMERA_COMPONENTS)
    print(f"\nğŸ“· Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
    print(f"   CAMERA_COMPONENTS: {CAMERA_COMPONENTS}")
    print(f"   Camera names: {camera_names}")
    print(f"   Detected {len(camera_config)} cameras in topic_info: {list(camera_config.keys())}")
    
    print("\n" + "="*80)
    print("ğŸ¯ Depalletize Task Evaluation (GrootPolicy)")
    print("="*80)
    print(f"ğŸ“‚ Checkpoint: {args.ckpt_path}")
    print(f"ğŸ¤– Model type: {args.model_type} (using GrootPolicy)")
    if args.model_type != 'groot':
        print(f"âš ï¸  Warning: model-type '{args.model_type}' is deprecated. Using GrootPolicy instead.")
    print(f"ğŸ“Š Action chunk size: {args.action_chunk_size}")
    print(f"ğŸ“¦ Action dimension: Supports 16 or 18 (14 arm joints + 2 claw positions [+ 2 cmd_pose])")
    print(f"ğŸ–¼ï¸  Enable GUI: {args.enable_gui}")
    if args.rotate_head_camera:
        print(f"ğŸ”„ Rotate head camera: Enabled (images from 'image' camera will be rotated 180 degrees)")
    if args.state_zero:
        print(f"âš ï¸  State zero mode: Enabled (all state inputs will be set to zero)")
    if args.lerobot_dataset_path:
        print(f"ğŸ“ Dataset path (for stats): {args.lerobot_dataset_path}")
    if args.task_description:
        print(f"ğŸ“ Task description: '{args.task_description}'")
    if args.skip_chunk_ratio > 0.0:
        print(f"â­ï¸  Skip chunk ratio: {args.skip_chunk_ratio*100:.1f}% (will skip first {args.skip_chunk_ratio*100:.1f}% of each predicted chunk)")
    if args.model_action_dt is not None:
        print(f"âš¡ Model action DT: {args.model_action_dt:.3f}s (inference frequency: {1.0/args.model_action_dt:.1f} Hz)")
    print("="*80 + "\n")

    if args.eval:
        print("ğŸš€ Starting real-time evaluation...")
        eval(args.ckpt_path, model_type=args.model_type, control_arm=True, control_claw=True, 
             action_chunk_size=args.action_chunk_size, 
             lerobot_dataset_path=args.lerobot_dataset_path,
             enable_gui=args.enable_gui,
             rotate_head_camera=args.rotate_head_camera,
             state_zero=args.state_zero,
             task_description=args.task_description,
             skip_chunk_ratio=args.skip_chunk_ratio,
             model_action_dt=args.model_action_dt)
    elif args.replay:
        print("Replaying the model")
        lerobot_dataset_path = '/home/lab/kuavo-manip/lerobot_data/vel_wrend_box_613'
        replay(lerobot_dataset_path, episode=0, control_arm=True, control_claw=True)
    else:
        print("Please specify either --eval or --replay")
        exit(1)

    # --------------------------------------- #

