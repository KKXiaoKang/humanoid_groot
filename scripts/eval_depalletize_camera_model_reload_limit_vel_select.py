import sys, os

from pandas.core.missing import F
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import cv2
import numpy as np
import rosbag
from sensor_msgs.msg import JointState
import json
from std_srvs.srv import Trigger, TriggerRequest, SetBool, SetBoolRequest

def resample_actions_with_speed_limit(actions: np.ndarray, dt: float, v_max, arm_dims: slice = slice(None)):
    '''
        resample actions (joint positions) which satisfy joint velocity limits
        Only applies speed limit to arm dimensions, other dimensions are interpolated normally
        
        Args:
            actions: Array of shape (T, D) where T is number of timesteps, D is action dimension
            dt: Time interval between actions (seconds)
            v_max: Maximum velocity (rad/s). Can be scalar or array of shape (arm_dim,)
            arm_dims: Slice or indices specifying which dimensions are arm joints (to apply speed limit)
        
        Returns:
            Array of shape (M, D) where M >= T, with speed-limited resampling
    '''
    T, D = actions.shape
    actions = np.asarray(actions)
    
    # Convert v_max to array format
    v_max = np.asarray(v_max)
    if v_max.ndim == 0:
        # If scalar, apply to all arm dimensions
        if isinstance(arm_dims, slice):
            arm_dim_size = len(range(*arm_dims.indices(D)))
        else:
            arm_dim_size = len(arm_dims)
        v_max = np.full(arm_dim_size, v_max)
    
    new_actions = [actions[0]]

    for t in range(T-1):
        a0 = actions[t]
        a1 = actions[t+1]

        # Extract arm dimensions
        if isinstance(arm_dims, slice):
            arm_a0 = a0[arm_dims]
            arm_a1 = a1[arm_dims]
        else:
            arm_a0 = a0[arm_dims]
            arm_a1 = a1[arm_dims]

        delta = arm_a1 - arm_a0
        v_required = np.abs(delta) / dt

        # Calculate scale factor based on arm velocity limits
        scale = np.max(v_required / v_max) if len(v_max) > 0 and np.any(v_max > 0) else 1.0
        scale = max(scale, 1.0)

        # number of sub_steps
        num_sub = int(np.ceil(scale))

        # interpolate all dimensions
        for s in range(1, num_sub + 1):
            alpha = s / num_sub
            new_a = a0 * (1 - alpha) + a1 * alpha
            new_actions.append(new_a)

    return np.stack(new_actions, axis=0)

# Initialize GUI windows if requested
def init_gui_windows(enable_gui=False, camera_config=None):
    """
    Initialize GUI windows if enabled
    
    Args:
        enable_gui: Whether to enable GUI windows
        camera_config: Dictionary of camera names from topic_info (e.g., {'image': ..., 'chest_image': ...})
    """
    if not enable_gui:
        print(" ======================  GUI windows disabled ====================== ")
        return
    
    print(" ======================  Initializing GUI windows ====================== ")
    
    # æ ¹æ®ç›¸æœºé…ç½®åŠ¨æ€åˆ›å»ºçª—å£
    if camera_config is None:
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤3ç›¸æœºé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        from configs.config import topic_info
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    
    # ç›¸æœºåç§°åˆ°çª—å£åç§°çš„æ˜ å°„
    camera_window_map = {
        'image': 'head Camera',
        'chest_image': 'chest Camera',
        'left_shoulder_image': 'left_shoulder Camera',
        'right_shoulder_image': 'right_shoulder Camera'
    }
    
    # åˆ›å»ºç›¸æœºçª—å£
    for camera_name in camera_config.keys():
        if camera_name in camera_window_map:
            window_name = camera_window_map[camera_name]
            cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(window_name, 640, 480)
            print(f"   Created window: {window_name}")
    
    print(f" ======================  GUI windows ready ({len(camera_config)} cameras) ====================== ")

# GUIçª—å£å°†åœ¨è§£æå‘½ä»¤è¡Œå‚æ•°ååˆå§‹åŒ–

from collections import deque
from typing import Optional
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "robot_envs")))
from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, get_camera_names, CAMERA_COMPONENTS, ACTION_COMPONENTS
from configs.config import ROBOT_VERSION
# ä½¿ç”¨GrootPolicyæ¨¡å‹
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.policies.factory import make_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# import torchvision
# import matplotlib.pyplot as plt
from pathlib import Path
import torch
import time
import argparse
import rospy
from std_msgs.msg import Float64MultiArray
from kuavo_humanoid_sdk.kuavo_strategy_pytree.common.robot_sdk import RobotSDK
from kuavo_humanoid_sdk.msg.kuavo_msgs.srv import (changeArmCtrlMode, changeArmCtrlModeRequest)

# Default MODEL_ACTION_DT - can be overridden by command line argument
# This represents the time interval between predicted actions during training
# Smaller values = higher inference frequency (e.g., 0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz)
DEFAULT_MODEL_ACTION_DT = 0.1
MODEL_ACTION_DT = DEFAULT_MODEL_ACTION_DT  # Will be updated by command line argument if provided
MODEL_ACTION_FREQUENCY = 1.0 / MODEL_ACTION_DT
TARGET_CONTROL_FREQUENCY = 100.0
TARGET_CONTROL_DT = 1.0 / TARGET_CONTROL_FREQUENCY
CHUNK_TRANSITION_DURATION_S = 0.2  # seconds of low-pass smoothing at chunk boundary
LOWPASS_ALPHA = 0.85  # closer to 1 => smoother (slower) transitions
ENABLE_CHUNK_TRANSITION_LOWPASS = True  # Enable/disable low-pass filtering at chunk boundaries (default: False, only linear interpolation within chunks)
FIRST_MODEL_INFERENCE = True

def resample_action_chunk(action_chunk: np.ndarray,
                          source_dt: float = MODEL_ACTION_DT,
                          target_dt: float = TARGET_CONTROL_DT) -> np.ndarray:
    """
    Resample an action chunk predicted at a lower frequency to a higher control frequency.

    Args:
        action_chunk: Array of shape (N, action_dim) predicted at intervals of source_dt.
        source_dt: Time interval between successive actions in the chunk.
        target_dt: Desired time interval for control commands.

    Returns:
        Array of shape (M, action_dim) where M approximates (N-1)*source_dt/target_dt + 1,
        interpolated with linear interpolation along time.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if action_chunk.shape[0] <= 1 or np.isclose(source_dt, target_dt):
        # Nothing to resample, either single action or already at target frequency
        return action_chunk

    total_duration = source_dt * (action_chunk.shape[0] - 1)
    if total_duration <= 0:
        repeat_factor = max(int(round(source_dt / target_dt)), 1)
        return np.repeat(action_chunk, repeats=repeat_factor, axis=0)

    num_target_steps = int(round(total_duration / target_dt)) + 1
    source_times = np.linspace(0.0, total_duration, num=action_chunk.shape[0])
    target_times = np.linspace(0.0, total_duration, num=num_target_steps)

    interpolated = np.empty((num_target_steps, action_chunk.shape[1]), dtype=action_chunk.dtype)
    for dim in range(action_chunk.shape[1]):
        interpolated[:, dim] = np.interp(target_times, source_times, action_chunk[:, dim])

    return interpolated


def apply_lowpass_transition(actions: np.ndarray,
                             previous_action: Optional[np.ndarray],
                             alpha: float = LOWPASS_ALPHA,
                             transition_steps: Optional[int] = None,
                             smooth_slice: slice | tuple | np.ndarray = slice(None)) -> np.ndarray:
    """
    Smooth the beginning of a resampled chunk with an exponential low-pass filter
    to reduce discontinuities at chunk boundaries.

    Args:
        actions: Resampled action chunk at control frequency, shape (N, action_dim).
        previous_action: Last action that was executed on the robot. If None, no smoothing applied.
        alpha: Low-pass smoothing coefficient (0 < alpha < 1). Larger alpha = smoother/slower response.
        transition_steps: Number of control steps over which to apply smoothing. If None, smooth entire chunk.
        smooth_slice: Indices/slice specifying which action dimensions to smooth (e.g., only arm joints).

    Returns:
        Smoothed action chunk (same shape as input).
    """
    if previous_action is None:
        return actions

    actions = np.asarray(actions)
    if actions.ndim == 1:
        actions = actions.reshape(1, -1)

    smoothed = actions.copy()
    prev = np.asarray(previous_action, dtype=smoothed.dtype)
    if prev.ndim == 1:
        prev = prev.reshape(1, -1)
    prev = prev[0]

    num_steps = smoothed.shape[0]
    if transition_steps is None or transition_steps > num_steps:
        transition_steps = num_steps
    transition_steps = max(1, transition_steps)

    if isinstance(smooth_slice, slice) or isinstance(smooth_slice, tuple):
        smooth_indices = smooth_slice
    else:
        smooth_indices = smooth_slice

    for idx in range(transition_steps):
        prev_slice = prev[smooth_indices]
        smoothed_slice = smoothed[idx][smooth_indices]
        filtered = alpha * prev_slice + (1.0 - alpha) * smoothed_slice
        prev[smooth_indices] = filtered
        smoothed[idx][smooth_indices] = filtered

    return smoothed


def resample_chunk_with_claw_hold(action_chunk: np.ndarray,
                                  previous_action: Optional[np.ndarray],
                                  control_frequency: float,
                                  source_dt: float = MODEL_ACTION_DT,
                                  arm_dims: slice = slice(0, 14),
                                  claw_dims: slice = slice(14, 16)) -> np.ndarray:
    """
    Resample an action chunk so that arm joints are interpolated to the control frequency
    while claw positions are held at the original (low) frequency.
    """
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if previous_action is not None:
        chunk_with_bridge = np.vstack([previous_action, action_chunk])
        resampled = resample_action_chunk(
            chunk_with_bridge,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )[1:]
        source_array = chunk_with_bridge
    else:
        resampled = resample_action_chunk(
            action_chunk,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )
        source_array = action_chunk

    # Zero-order hold for claw dimensions (keep 10Hz updates)
    if source_array.shape[0] > 0 and resampled.shape[0] > 0:
        total_duration = source_dt * max(source_array.shape[0] - 1, 1)
        if total_duration <= 0:
            hold_indices = np.zeros(resampled.shape[0], dtype=int)
        else:
            target_times = np.linspace(0.0, total_duration, num=resampled.shape[0], endpoint=True)
            source_times = np.linspace(0.0, total_duration, num=source_array.shape[0], endpoint=True)
            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
            hold_indices = np.clip(hold_indices, 0, source_array.shape[0] - 1)
        resampled[:, claw_dims] = source_array[hold_indices][:, claw_dims]

    return resampled

def change_arm_ctrl_mode(control_mode):
    rospy.wait_for_service('/humanoid_change_arm_ctrl_mode')
    try:
        change_mode = rospy.ServiceProxy('/humanoid_change_arm_ctrl_mode', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("æ‰‹è‡‚æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†æ‰‹è‡‚æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)

def direct_to_wbc(control_mode):
    """
        åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
        Args:
            control_mode: æ§åˆ¶æ¨¡å¼
                0: ç¦ç”¨wbcæ§åˆ¶è½¨è¿¹æ¨¡å¼
                1: wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
    """
    rospy.wait_for_service('/enable_wbc_arm_trajectory_control', timeout=5)
    try:
        change_mode = rospy.ServiceProxy('/enable_wbc_arm_trajectory_control', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)


def replay(lerobot_dataset_path, episode, control_arm=True, control_claw=True):
    """
    ç›´æ¥replayæ•°æ®é›†é‡Œçš„è½¨è¿¹ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    """
    repo_id = 0

    dataset = LeRobotDataset(repo_id=repo_id, root=lerobot_dataset_path, episodes=[episode])
    actions = dataset.hf_dataset.select_columns("action")
    env = GrabBoxMpcEnv()
    env.obs_buffer.wait_buffer_ready()
    time.sleep(1)

    for idx in range(dataset.num_frames):
        action = actions[idx]["action"]
        action = np.expand_dims(action, axis=0)

        env.exec_actions(actions=action,
                         control_arm=control_arm,
                         control_claw=control_claw,
                         )

def publish_joint_positions(action_chunk,
                            joint_pub,
                            source_frequency_hz: float,
                            target_frequency_hz: Optional[float] = None):
    """
    ä»åŠ¨ä½œå—ä¸­æå–å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®å¹¶åˆå¹¶å‘å¸ƒåˆ°ROSè¯é¢˜ï¼Œ
    å¯é€‰åœ°å°†åŠ¨ä½œæ’å€¼åˆ°æ›´é«˜çš„æ§åˆ¶é¢‘ç‡åå†å‘å¸ƒã€‚
    
    Args:
        action_chunk: shapeä¸º(N, action_dim)çš„åŠ¨ä½œå—
                     æ”¯æŒæ ¼å¼:
                     - 16ç»´: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
                     - 18ç»´: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
        joint_pub: å…³èŠ‚ä½ç½®å‘å¸ƒå™¨
        source_frequency_hz: åŸå§‹åŠ¨ä½œå—çš„é¢‘ç‡ï¼ˆHzï¼‰
        target_frequency_hz: å¦‚æœæä¾›ï¼Œåˆ™å°†åŠ¨ä½œå—æ’å€¼åˆ°è¯¥é¢‘ç‡åå†å‘å¸ƒ
    """
    try:
        action_chunk = np.asarray(action_chunk)
        if action_chunk.ndim == 1:
            action_chunk = action_chunk.reshape(1, -1)

        if target_frequency_hz is not None and target_frequency_hz > source_frequency_hz:
            action_chunk = resample_action_chunk(
                action_chunk,
                source_dt=1.0 / source_frequency_hz,
                target_dt=1.0 / target_frequency_hz
            )

        action_dim = action_chunk.shape[1]
        
        # æ”¯æŒ16ç»´å’Œ18ç»´åŠ¨ä½œæ ¼å¼
        if action_dim == 16:
            # 16ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
            rospy.logdebug(f"Using depalletize 16-dim action format")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            # åˆå¹¶å·¦å³æ‰‹å…³èŠ‚ä½ç½®å’Œå¤¹çˆªä½ç½®ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œæœ€åæ˜¯å¤¹çˆªä½ç½®
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions], axis=1)  # shape: (action_chunk_size, 16)
            
        elif action_dim == 18:
            # 18ç»´æ ¼å¼: [7ä¸ªå·¦æ‰‹è‡‚å…³èŠ‚, 7ä¸ªå³æ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_poseç»´åº¦]
            # å‘é€å®Œæ•´çš„18ç»´æ•°æ®
            rospy.logdebug(f"Using depalletize 18-dim action format (including cmd_pose dimensions)")
            left_joints_all_steps = action_chunk[:, :7]    # shape: (action_chunk_size, 7)
            right_joints_all_steps = action_chunk[:, 7:14] # shape: (action_chunk_size, 7)
            claw_positions = action_chunk[:, 14:16]  # shape: (action_chunk_size, 2)
            cmd_pose = action_chunk[:, 16:18]  # shape: (action_chunk_size, 2)
            # åˆå¹¶æ‰€æœ‰ç»„ä»¶ï¼šå…ˆå·¦æ‰‹ï¼Œåå³æ‰‹ï¼Œç„¶åæ˜¯å¤¹çˆªï¼Œæœ€åæ˜¯cmd_pose
            combined_joints = np.concatenate([left_joints_all_steps, right_joints_all_steps, claw_positions, cmd_pose], axis=1)  # shape: (action_chunk_size, 18)
            
        else:
            rospy.logwarn(f"Action chunk dimension {action_dim} not supported (expected 16 or 18 for depalletize task)")
            return

        # å‘å¸ƒåˆå¹¶åçš„å…³èŠ‚ä½ç½®ï¼ˆå®Œæ•´çš„action_dimç»´åº¦ï¼‰
        joint_msg = Float64MultiArray()
        joint_msg.data = combined_joints.flatten().tolist()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„
        joint_pub.publish(joint_msg)
        
        rospy.logdebug(f"Published combined joint positions: {combined_joints.shape} (action_dim={action_dim})")
        
    except Exception as e:
        rospy.logerr(f"Error publishing joint positions: {str(e)}")

def load_and_replay_init_trajectory(bag_path: str, env, control_arm: bool = True, control_claw: bool = True):
    """
    ä»rosbagæ–‡ä»¶ä¸­åŠ è½½åˆå§‹è½¨è¿¹å¹¶å›æ”¾
    
    Args:
        bag_path: rosbagæ–‡ä»¶è·¯å¾„
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
    """
    if not os.path.exists(bag_path):
        rospy.logerr(f"Bag file not found: {bag_path}")
        return False
    
    rospy.loginfo(f"Loading initial trajectory from bag: {bag_path}")
    
    # æœŸæœ›çš„å…³èŠ‚åç§°é¡ºåºï¼ˆä¸publish_target_arm_clawä¸­çš„é¡ºåºä¸€è‡´ï¼‰
    expected_joint_names = [
        "zarm_l1_joint", "zarm_l2_joint", "zarm_l3_joint", "zarm_l4_joint", 
        "zarm_l5_joint", "zarm_l6_joint", "zarm_l7_joint",
        "zarm_r1_joint", "zarm_r2_joint", "zarm_r3_joint", "zarm_r4_joint", 
        "zarm_r5_joint", "zarm_r6_joint", "zarm_r7_joint",
    ]
    
    # è¯»å–bagæ–‡ä»¶ä¸­çš„JointStateæ¶ˆæ¯
    joint_states = []
    try:
        with rosbag.Bag(bag_path, 'r') as bag:
            topic_name = '/mm_kuavo_arm_traj'
            
            # æ£€æŸ¥è¯é¢˜æ˜¯å¦å­˜åœ¨
            bag_info = bag.get_type_and_topic_info()
            if topic_name not in bag_info[1]:
                rospy.logwarn(f"Topic {topic_name} not found in bag file. Available topics: {list(bag_info[1].keys())}")
                return False
            
            # è¯»å–æ‰€æœ‰JointStateæ¶ˆæ¯
            # æ³¨æ„ï¼šç”±äºå·²ç»é€šè¿‡topic_nameè¿‡æ»¤ï¼Œæ‰€æœ‰æ¶ˆæ¯éƒ½åº”è¯¥æ˜¯JointStateç±»å‹
            # ä½†isinstanceæ£€æŸ¥å¯èƒ½ä¸å·¥ä½œï¼ˆrosbagå¯èƒ½è¿”å›åŒ…è£…ç±»å‹ï¼‰ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨æ¶ˆæ¯
            message_count = 0
            for topic, msg, t in bag.read_messages(topics=[topic_name]):
                message_count += 1
                # ç›´æ¥ä½¿ç”¨æ¶ˆæ¯ï¼Œä¸è¿›è¡Œç±»å‹æ£€æŸ¥ï¼ˆå› ä¸ºå·²ç»é€šè¿‡topicè¿‡æ»¤ï¼‰
                joint_states.append({
                    'timestamp': t.to_sec(),
                    'msg': msg
                })
            
            rospy.loginfo(f"Read {message_count} messages from topic {topic_name}")
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            joint_states.sort(key=lambda x: x['timestamp'])
            
            if len(joint_states) == 0:
                rospy.logwarn(f"No JointState messages found in topic {topic_name}")
                return False
            
            rospy.loginfo(f"Loaded {len(joint_states)} joint states from bag file")
            
    except Exception as e:
        rospy.logerr(f"Error loading bag file: {e}")
        return False
    
    # è·å–å½“å‰å¤¹çˆªçŠ¶æ€ï¼ˆç”¨äºå¡«å……16ç»´actionï¼‰
    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
    try:
        obs_data, _, _, robot_obs, _ = env.get_obs()
        if 'claw_state' in robot_obs and len(robot_obs['claw_state']) > 0:
            # è·å–æœ€æ–°çš„å¤¹çˆªçŠ¶æ€
            claw_data = robot_obs['claw_state']
            if claw_data.ndim == 2:
                # å¦‚æœæ˜¯2Dæ•°ç»„ï¼Œå–æœ€åä¸€è¡Œ
                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
            elif claw_data.ndim == 1:
                # å¦‚æœæ˜¯1Dæ•°ç»„ï¼Œç›´æ¥ä½¿ç”¨
                current_claw_state = np.array(claw_data, dtype=np.float32)
            
            # ç¡®ä¿æ˜¯2ç»´
            if current_claw_state.shape[0] != 2:
                rospy.logwarn(f"Claw state has unexpected shape: {current_claw_state.shape}, using default")
                current_claw_state = np.array([0.0, 0.0])
            else:
                rospy.loginfo(f"Current claw state: {current_claw_state}")
    except Exception as e:
        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
        current_claw_state = np.array([0.0, 0.0])
    
    # å›æ”¾è½¨è¿¹ï¼ˆæŒ‰ç…§rosbagä¸­çš„æ—¶é—´æˆ³é—´éš”ï¼‰
    rospy.loginfo("Starting trajectory replay...")
    replay_start_time = time.time()
    bag_start_timestamp = joint_states[0]['timestamp']  # bagä¸­çš„ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³
    
    for i, joint_data in enumerate(joint_states):
        msg = joint_data['msg']
        bag_timestamp = joint_data['timestamp']
        
        # æå–å…³èŠ‚ä½ç½®
        # JointStateçš„positionæ˜¯è§’åº¦ï¼ˆåº¦ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºå¼§åº¦
        if len(msg.position) < 14:
            rospy.logwarn(f"JointState message {i} has insufficient positions: {len(msg.position)} < 14")
            continue
        
        # ç›´æ¥ä½¿ç”¨positionæ•°ç»„çš„å‰14ä¸ªå…ƒç´ ï¼ˆè·³è¿‡åç§°æ£€æŸ¥ï¼‰
        # bagæ–‡ä»¶ä¸­çš„å…³èŠ‚é¡ºåºæ˜¯: arm_joint_1 ~ arm_joint_14
        # å¯¹åº”: å·¦æ‰‹7ä¸ªå…³èŠ‚ + å³æ‰‹7ä¸ªå…³èŠ‚
        # ç›´æ¥ä½¿ç”¨å‰14ä¸ªä½ç½®ï¼Œå‡è®¾é¡ºåºæ­£ç¡®
        arm_action = np.deg2rad(np.array(msg.position[:14]))
        
        # ç»„åˆæˆ16ç»´action: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
        action = np.concatenate([arm_action, current_claw_state])
        
        # è®¡ç®—åº”è¯¥ç­‰å¾…çš„æ—¶é—´ï¼ˆæŒ‰ç…§bagä¸­çš„æ—¶é—´æˆ³é—´éš”ï¼‰
        if i == 0:
            # ç¬¬ä¸€ä¸ªåŠ¨ä½œç«‹å³æ‰§è¡Œ
            expected_elapsed = 0.0
        else:
            # è®¡ç®—ä»bagå¼€å§‹åˆ°å½“å‰æ¶ˆæ¯åº”è¯¥ç»è¿‡çš„æ—¶é—´
            bag_elapsed = bag_timestamp - bag_start_timestamp
            # è®¡ç®—å®é™…ç»è¿‡çš„æ—¶é—´
            actual_elapsed = time.time() - replay_start_time
            # éœ€è¦ç­‰å¾…çš„æ—¶é—´
            expected_elapsed = bag_elapsed - actual_elapsed
        
        # å¦‚æœæ—¶é—´è¿˜æ²¡åˆ°ï¼Œç­‰å¾…
        if expected_elapsed > 0:
            time.sleep(expected_elapsed)
        
        # æ‰§è¡ŒåŠ¨ä½œï¼ˆä¸ä½¿ç”¨env.exec_actionsï¼Œå› ä¸ºå®ƒä¼šæŒ‰ç…§100Hzé¢‘ç‡æ§åˆ¶ï¼Œæˆ‘ä»¬ç›´æ¥å‘å¸ƒï¼‰
        # ç›´æ¥ä½¿ç”¨envçš„target_publisherå‘å¸ƒï¼Œä¸ç»è¿‡env.exec_actionsçš„é¢‘ç‡æ§åˆ¶
        env.target_publisher.publish_target_arm_claw(
            arm_action=arm_action,
            claw_action=current_claw_state,
            control_arm=control_arm,
            control_claw=control_claw
        )
        
        # æ‰“å°è¿›åº¦
        if (i + 1) % 10 == 0 or i == len(joint_states) - 1:
            elapsed = time.time() - replay_start_time
            bag_total_time = joint_states[-1]['timestamp'] - bag_start_timestamp
            rospy.loginfo(f"Replayed {i + 1}/{len(joint_states)} steps (elapsed: {elapsed:.2f}s, bag time: {bag_total_time:.2f}s)")
    
    total_time = time.time() - replay_start_time
    bag_total_time = joint_states[-1]['timestamp'] - bag_start_timestamp
    rospy.loginfo(f"Trajectory replay completed! Real time: {total_time:.2f}s, Bag time: {bag_total_time:.2f}s, {len(joint_states)} steps")
    
    return True

def reset_inference_state(policy, env):
    """
    é‡ç½®æ¨ç†çŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ¬¡æ¨ç†åšå‡†å¤‡
    
    Args:
        policy: GrootPolicyæ¨¡å‹å®ä¾‹
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
    """
    rospy.loginfo("ğŸ”„ Resetting inference state...")
    
    # é‡ç½®policyçŠ¶æ€
    policy.reset()
    rospy.loginfo("   âœ… Policy reset")
    
    # ç­‰å¾…bufferé‡æ–°readyï¼ˆbufferä¼šè‡ªåŠ¨ä¿æŒæœ€æ–°æ•°æ®ï¼Œä½†ç¡®ä¿æ•°æ®å……è¶³ï¼‰
    rospy.loginfo("   â³ Waiting for buffer to be ready...")
    env.obs_buffer.wait_buffer_ready()
    rospy.loginfo("   âœ… Buffer ready")
    
    rospy.loginfo("âœ… Inference state reset complete")


def load_model_and_env(ckpt_path, model_type, action_chunk_size=50, enable_gui=False, rotate_head_camera=False, state_zero=False, task_description=None):
    """
    åŠ è½½æ¨¡å‹å’Œç¯å¢ƒï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        enable_gui: æ˜¯å¦å¯ç”¨GUI
        rotate_head_camera: æ˜¯å¦æ—‹è½¬å¤´éƒ¨ç›¸æœº
        state_zero: æ˜¯å¦å°†çŠ¶æ€ç½®é›¶
        task_description: ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    Returns:
        tuple: (policy, preprocessor, postprocessor, env, task_description, device)
    """
    # ---------- 1. load GrootPolicy from checkpoint ---------------
    device = "cuda:0"
    print(" =================== Loading GrootPolicy =================== ")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = action_chunk_size
    
    # ç¡®å®šè¦ä½¿ç”¨çš„ä»»åŠ¡æè¿°
    if task_description is None:
        # ä½¿ç”¨é€šç”¨é»˜è®¤å€¼
        task_description = "Depalletize the box"
        print(f"ğŸ“ Using default task description: '{task_description}'")
    else:
        print(f"ğŸ“ Using provided task description: '{task_description}'")
    
    # ä» checkpoint åŠ è½½ preprocessor å’Œ postprocessorï¼ˆå¿…é¡»åŒ…å« dataset_statsï¼‰
    print(f"\nğŸ”§ Loading preprocessor and postprocessor from checkpoint...")
    try:
        # ä» checkpoint åŠ è½½ï¼Œä¸æä¾› dataset_statsï¼Œè®©å®ƒä» checkpoint ä¸­åŠ è½½
        preprocessor, postprocessor = make_pre_post_processors(
            policy_cfg=policy.config,
            pretrained_path=ckpt_path,
        )
        print("âœ… Preprocessor and postprocessor loaded from checkpoint")
        
        # æ£€æŸ¥ postprocessor ä¸­æ˜¯å¦æœ‰ stats
        # ä» postprocessor çš„æ­¥éª¤ä¸­æå– statsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        dataset_stats = None
        for step in postprocessor.steps:
            if hasattr(step, 'stats') and step.stats is not None:
                dataset_stats = step.stats
                print(f"âœ… Found dataset_stats in checkpoint postprocessor")
                break
        
        if dataset_stats is None:
            raise ValueError(
                "âŒ ERROR: No dataset_stats found in checkpoint postprocessor. "
                "The checkpoint must contain dataset_stats for normalization. "
                "Please ensure the checkpoint was saved with proper statistics."
            )
        
        print(f"âœ… Using dataset_stats from checkpoint: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
                
    except ValueError as e:
        # å¦‚æœæ˜¯æˆ‘ä»¬æŠ›å‡ºçš„ ValueErrorï¼ˆstats ç¼ºå¤±ï¼‰ï¼Œç›´æ¥æŠ›å‡º
        raise
    except Exception as e:
        raise RuntimeError(
            f"âŒ ERROR: Failed to load processors from checkpoint: {e}\n"
            f"   Please ensure the checkpoint path is correct and contains preprocessor/postprocessor files."
        ) from e
    
    # Debug: Print model configuration
    print(f"ğŸ” Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"ğŸ” Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    policy.eval()
    policy.to(device)
    policy.reset()
    
    # Initialize real-time environment
    env = GrabBoxMpcEnv()
    print(f"ğŸ¤– Environment initialized for depalletize task")
    print(" ======================  Waiting for buffer ready ====================== ")
    env.obs_buffer.wait_buffer_ready()
    print(" ======================  Buffer ready ====================== ")
    time.sleep(1)
    
    return policy, preprocessor, postprocessor, env, task_description, device

def set_arm_quick_mode(enable: bool) -> bool:
    """å¼€å…³æ‰‹è‡‚å¿«é€Ÿæ¨¡å¼"""
    rospy.loginfo(f"call set_arm_quick_mode:{enable}")
    try:
        rospy.wait_for_service('/enable_lb_arm_quick_mode', timeout=5.0)
        cli = rospy.ServiceProxy('/enable_lb_arm_quick_mode', SetBool)
        resp = cli(enable)
        if resp.success:
            rospy.loginfo(f"Successfully {'enabled' if enable else 'disabled'} arm quick mode")
            return True
        else:
            rospy.logwarn(f"Failed to {'enable' if enable else 'disable'} arm quick mode")
            return False
    except rospy.ServiceException as e:
        rospy.logerr(f"Service call failed: {e}")
        return False

def run_inference_loop(policy, preprocessor, postprocessor, env, task_description, device, 
                       control_arm=True, control_claw=True, action_chunk_size=50, 
                       enable_gui=False, rotate_head_camera=False, state_zero=False,
                       is_first_inference=True, chunk_start=None, chunk_end=None, model_action_dt=None,
                       sync_mode=False, max_joint_velocity=None):
    """
    è¿è¡Œæ¨ç†å¾ªç¯ï¼ˆå¯ä»¥å¤šæ¬¡è°ƒç”¨ï¼Œæ¯æ¬¡è°ƒç”¨å¼€å§‹æ–°çš„æ¨ç†ä¼šè¯ï¼‰
    
    Args:
        policy: å·²åŠ è½½çš„GrootPolicyæ¨¡å‹
        preprocessor: é¢„å¤„ç†å™¨
        postprocessor: åå¤„ç†å™¨ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
        env: å·²åˆå§‹åŒ–çš„GrabBoxMpcEnvç¯å¢ƒ
        task_description: ä»»åŠ¡æè¿°
        device: è®¾å¤‡
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        enable_gui: æ˜¯å¦å¯ç”¨GUI
        rotate_head_camera: æ˜¯å¦æ—‹è½¬å¤´éƒ¨ç›¸æœº
        state_zero: æ˜¯å¦å°†çŠ¶æ€ç½®é›¶
        is_first_inference: æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡æ¨ç†ï¼ˆç¬¬ä¸€æ¬¡ä¼šåŠ è½½bagæ–‡ä»¶ï¼Œåç»­ä½¿ç”¨jsonæ–‡ä»¶é‡ç½®ï¼‰
        chunk_start: è¦æ‰§è¡Œçš„chunkèµ·å§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼ŒåŒ…å«ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œä»ç¬¬ä¸€ä¸ªactionå¼€å§‹
        chunk_end: è¦æ‰§è¡Œçš„chunkç»“æŸç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼ŒåŒ…å«ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œæ‰§è¡Œåˆ°æœ€åä¸€ä¸ªaction
        model_action_dt: æ¨¡å‹åŠ¨ä½œæ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ§åˆ¶æ¨ç†é¢‘ç‡ã€‚å¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨å±€MODEL_ACTION_DT
        sync_mode: æ˜¯å¦ä½¿ç”¨åŒæ­¥æ¨ç†æ¨¡å¼ã€‚å¦‚æœTrueï¼Œæ¨ç†ä¸€ä¸ªchunk -> æ‰§è¡Œå®Œæ•´ä¸ªchunk -> get_obs -> å†æ¨ç†ä¸‹ä¸€ä¸ªchunk
        max_joint_velocity: æœ€å¤§å…³èŠ‚é€Ÿåº¦é™åˆ¶ï¼ˆrad/sï¼‰ã€‚å¦‚æœæä¾›ï¼Œå°†å¯¹armå…³èŠ‚åº”ç”¨é€Ÿåº¦é™åˆ¶
    
    Returns:
        bool: Trueè¡¨ç¤ºæ­£å¸¸é€€å‡ºï¼ˆæŒ‰qï¼‰ï¼ŒFalseè¡¨ç¤ºè¢«ä¸­æ–­ï¼ˆCtrl+Cï¼‰
    """
    global FIRST_MODEL_INFERENCE
    # ä½¿ç”¨ä¼ å…¥çš„model_action_dtæˆ–å…¨å±€MODEL_ACTION_DT
    if model_action_dt is None:
        model_action_dt = MODEL_ACTION_DT
    model_action_frequency = 1.0 / model_action_dt
    # Print action mode configuration
    print("\n" + "="*80)
    print("ğŸ¯ DEPALLETIZE TASK CONFIGURATION (GrootPolicy)")
    print("="*80)
    print(f"ğŸ¤– Control arm: {control_arm}")
    print(f"ğŸ¦¾ Control claw: {control_claw}")
    print(f"ğŸ“Š ACTION_COMPONENTS: {ACTION_COMPONENTS}")
    print(f"ğŸ“Š Action dimension: Will be determined by model output (expected: {len(ACTION_COMPONENTS) * 7 if 'Left_arm' in ACTION_COMPONENTS and 'Right_arm' in ACTION_COMPONENTS else 'varies'}D based on config)")
    print(f"ğŸ“¦ Action chunk size: {action_chunk_size}")
    # æ ¹æ®ACTION_COMPONENTSåˆ¤æ–­æ˜¯å¦åŒ…å«cmd_pose
    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
    print(f"ğŸ¯ Cmd_pose control: {'Enabled' if has_cmd_pose else 'Disabled'} (based on ACTION_COMPONENTS)")
    if rotate_head_camera:
        print(f"ğŸ”„ Head camera rotation enabled: images from 'image' camera will be rotated 180 degrees")
    if state_zero:
        print(f"âš ï¸  STATE ZERO MODE: All state inputs will be set to zero (for dependency testing)")
    if chunk_start is not None or chunk_end is not None:
        start_idx = chunk_start if chunk_start is not None else 0
        end_idx = chunk_end if chunk_end is not None else action_chunk_size - 1
        print(f"â­ï¸  Chunk selection: will execute actions from index {start_idx} to {end_idx} (inclusive)")
    if sync_mode:
        print(f"ğŸ”„ Sync mode: Enabled (inference -> execute chunk -> get_obs -> repeat)")
    else:
        print(f"âš¡ Model action DT: {model_action_dt:.3f}s (inference frequency: {model_action_frequency:.1f} Hz)")
    if max_joint_velocity is not None:
        print(f"ğŸš¦ Max joint velocity limit: {max_joint_velocity:.2f} rad/s")
    print(f"ğŸ“ Task description: '{task_description}'")
    print("="*80 + "\n")
    
    # é‡ç½®policyçŠ¶æ€
    policy.reset()
    
    step_counter = 0

    # Initialize ROS publishers for action visualization
    joint_pub = rospy.Publisher('/policy/action/eef_pose_marker_all', Float64MultiArray, queue_size=10)
    
    rospy.loginfo(f"Initialized ROS publishers for action visualization with chunk size: {action_chunk_size}")
    
    # è·å–åˆå§‹è§‚æµ‹
    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

    # TODO: æ£€æŸ¥æ—¶é—´åŒæ­¥æƒ…å†µ
    # TODO: æ˜¾ç¤ºå›¾åƒ
    rospy.loginfo(f"Initialized action visualization with chunk size: {action_chunk_size}")
    
    # ---------- 2. æ¨¡å‹æ¨ç†ï¼ˆå®æ—¶æ¨¡å¼ï¼‰ ----------------------
    # Real-time environment evaluation loop
    robot_sdk.control.set_external_control_arm_mode()
    time.sleep(1)
    
    # æ ¹æ®æœºå™¨äººç‰ˆæœ¬åˆ‡æ¢æ‰‹è‡‚æ§åˆ¶æ¨¡å¼
    if ROBOT_VERSION == "4_pro":
        direct_to_wbc(1)
        function_key = "direct_to_wbc"
    elif ROBOT_VERSION == "5_wheel":
        set_arm_quick_mode(True)
        function_key = "set_arm_quick_mode"    
    # ç­‰å¾…ä½¿èƒ½ç”Ÿæ•ˆ
    input(f"å½“å‰æœºå™¨äººæ¨¡å¼ä¸º: {ROBOT_VERSION} | æ§åˆ¶æ¨¡å¼ {function_key} ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== \n")
    time.sleep(1.0)
    resampled_action_queue: deque[np.ndarray] = deque()
    last_executed_action: Optional[np.ndarray] = None
    
    # åŠ è½½å¹¶å›æ”¾åˆå§‹è½¨è¿¹ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ¨ç†æ—¶åŠ è½½bagæ–‡ä»¶ï¼‰
    if is_first_inference:
        init_traj_bag_path = '/home/lab/kuavo-manip/robot_depalletize_init_traj.bag'
        # if os.path.exists(init_traj_bag_path):
        #     rospy.loginfo("Loading and replaying initial trajectory from bag file (first inference only)...")
        #     # FIXME:ç¬¬ä¸€å¸§çš„ä½ç½®4proå’Œ5wheelä¸ä¸€æ ·ï¼Œéœ€è¦å¤„ç†
        if ROBOT_VERSION == "4_pro":
            load_and_replay_init_trajectory(
                bag_path=init_traj_bag_path,
                env=env,
                control_arm=control_arm,
                control_claw=control_claw
            )
            rospy.logwarn(f"Initial trajectory bag file not found: {init_traj_bag_path}")
            rospy.loginfo("4_pro robot Initial trajectory replay completed. Starting model inference...")
            time.sleep(1.0)
        elif ROBOT_VERSION == "5_wheel":
            cur_dir = os.path.dirname(os.path.abspath(__file__))
            final_reset_arm(
                json_path=os.path.join(cur_dir, 'utils/start_arm_traj.json'), 
                env=env,
                control_arm=control_arm,
                control_claw=control_claw
            )
        
        input(f"è½¨è¿¹å›æ”¾ ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== è½¨è¿¹å›æ”¾æˆåŠŸ ==== \n")
        time.sleep(1.0)
        
        # é‡è¦ï¼šåœ¨bagå›æ”¾å®Œæˆåï¼Œé‡æ–°è·å–æœ€æ–°çš„è§‚æµ‹æ•°æ®
        # è¿™æ ·æ‰èƒ½è·å–åˆ°bagå›æ”¾åçš„çœŸå®æ‰‹è‡‚ä½ç½®
        rospy.loginfo("ğŸ”„ Updating observation data after bag replay...")
        obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
        rospy.loginfo("âœ… Observation data updated with post-bag-replay robot state")
    else:
        rospy.loginfo("Skipping bag file replay (not first inference). Using JSON reset instead.")
    
    print("\n" + "="*80)
    print("ğŸš€ Starting inference loop...")
    print("ğŸ’¡ Press 'q' + Enter to stop current inference and prepare for next run")
    print("ğŸ’¡ Press Ctrl+C to exit the program completely")
    print("="*80 + "\n")
    
    # åŒæ­¥æ¨¡å¼ï¼šæ‰§è¡Œå®Œæ•´ä¸ªchunkåå†æ¨ç†ä¸‹ä¸€ä¸ª
    if sync_mode:
        while True:
            try:
                # å‡†å¤‡è§‚æµ‹
                state = torch.from_numpy(obs_data["state"]).float()
                observation = {}
                
                # æ ¹æ®CAMERA_COMPONENTSåŠ¨æ€å¤„ç†ç›¸æœºå›¾åƒ
                camera_names = get_camera_names(CAMERA_COMPONENTS)
                for camera_name in camera_names:
                    if camera_name in obs_data:
                        camera_img_np = obs_data[camera_name]
                        if camera_img_np.ndim != 4:
                            rospy.logwarn(f"âš ï¸  Unexpected camera image shape: {camera_img_np.shape}, expected (T, H, W, C)")
                            continue
                        if rotate_head_camera and camera_name == "image":
                            camera_img_np = np.rot90(camera_img_np, k=2, axes=(1, 2)).copy()
                        camera_images = torch.from_numpy(np.moveaxis(camera_img_np, 3, 1).copy()).float() / 255
                        obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                        observation[obs_key] = camera_images.to('cuda:0')
                    elif step_counter == 0:
                        rospy.logwarn(f"âš ï¸  Camera '{camera_name}' from CAMERA_COMPONENTS not found in obs_data.")
                
                if state_zero:
                    observation['observation.state'] = torch.zeros_like(state).to('cuda:0')
                else:
                    observation['observation.state'] = state.to('cuda:0')
                observation['task'] = task_description
                
                # æ¨ç†
                processed_observation = preprocessor(observation)
                with torch.inference_mode():
                    pred_actions = policy.predict_action_chunk(processed_observation)
                
                # ä½¿ç”¨ postprocessor è¿›è¡Œåå½’ä¸€åŒ–
                # pred_actions shape: (batch_size, chunk_size, action_dim)
                # postprocessor æœŸæœ›è¾“å…¥æ˜¯ (B, action_dim)ï¼Œæ‰€ä»¥éœ€è¦å¤„ç†æ•´ä¸ª chunk
                _, chunk_size, _ = pred_actions.shape
                processed_actions = []
                for i in range(chunk_size):
                    # æå–å•ä¸ª action: (B, action_dim)
                    single_action = pred_actions[:, i, :]
                    # ä½¿ç”¨ postprocessor è¿›è¡Œåå½’ä¸€åŒ–
                    processed_action = postprocessor(single_action)
                    processed_actions.append(processed_action)
                
                # å †å å› (B, chunk_size, action_dim)ï¼Œç„¶åè½¬æ¢ä¸º numpy
                pred_actions_unnorm = torch.stack(processed_actions, dim=1)  # (B, chunk_size, action_dim)
                action_chunk = pred_actions_unnorm[0].cpu().numpy()  # (chunk_size, action_dim)
                
                # æ ¹æ®chunk_startå’Œchunk_endé€‰æ‹©è¦æ‰§è¡Œçš„actionèŒƒå›´
                chunk_size = action_chunk.shape[0]
                start_idx = chunk_start if chunk_start is not None else 0
                end_idx = chunk_end if chunk_end is not None else chunk_size - 1
                
                # éªŒè¯ç´¢å¼•èŒƒå›´
                if start_idx < 0:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_start {start_idx} is negative, using 0 instead")
                    start_idx = 0
                if end_idx >= chunk_size:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_end {end_idx} is >= chunk_size {chunk_size}, using {chunk_size - 1} instead")
                    end_idx = chunk_size - 1
                if start_idx > end_idx:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_start {start_idx} > chunk_end {end_idx}, using last action only")
                    action_chunk = action_chunk[-1:].copy()
                else:
                    # ä½¿ç”¨åˆ‡ç‰‡é€‰æ‹©èŒƒå›´ï¼ˆPythonåˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€ï¼Œæ‰€ä»¥end_idx+1ï¼‰
                    action_chunk = action_chunk[start_idx:end_idx+1].copy()
                    rospy.loginfo(f"â­ï¸  Selected actions from index {start_idx} to {end_idx} (inclusive): {action_chunk.shape[0]} actions")

                # ç¡®å®šarmå’Œclawç»´åº¦ï¼ˆéœ€è¦åœ¨FIRST_MODEL_INFERENCEæ£€æŸ¥ä¹‹å‰ç¡®å®šï¼Œä»¥ä¾¿æå–æ‰‹è‡‚çŠ¶æ€ï¼‰
                action_dim = action_chunk.shape[1]
                if action_dim == 16:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                elif action_dim == 18:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                else:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, min(16, action_dim))
                
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ¨¡å‹æ¨ç†ï¼Œéœ€è¦åœ¨å½“å‰çŠ¶æ€å’Œç¬¬ä¸€ä¸ªactionä¹‹é—´è¿›è¡Œæ’å€¼
                # å°†æ’å€¼åŠ¨ä½œåºåˆ—å’Œchunkåˆå¹¶ï¼Œç»Ÿä¸€è¿›è¡Œresampleå’Œé€Ÿåº¦é™åˆ¶å¤„ç†
                transition_chunk = None
                if FIRST_MODEL_INFERENCE:
                    rospy.loginfo("ğŸ”„ First model inference: generating smooth transition from current robot state to first action")
                    
                    # è·å–å½“å‰æœºå™¨äººçš„æ‰‹è‡‚çŠ¶æ€
                    current_arm_state = obs_data["state"][0][arm_dims]  # å½“å‰æ‰‹è‡‚å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰
                    
                    # è·å–å½“å‰å¤¹çˆªçŠ¶æ€
                    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
                    try:
                        if 'claw_state' in robot_obs and len(robot_obs['claw_state']) > 0:
                            claw_data = robot_obs['claw_state']
                            if claw_data.ndim == 2:
                                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
                            elif claw_data.ndim == 1:
                                current_claw_state = np.array(claw_data, dtype=np.float32)
                            if current_claw_state.shape[0] != 2:
                                current_claw_state = np.array([0.0, 0.0])
                    except Exception as e:
                        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
                        current_claw_state = np.array([0.0, 0.0])
                    
                    # è·å–ç¬¬ä¸€ä¸ªchunkçš„ç¬¬ä¸€ä¸ªactionï¼ˆå·²ç»æ ¹æ®chunk_start/chunk_endé€‰æ‹©ä¹‹åï¼‰
                    if action_chunk.shape[0] > 0:
                        first_action = action_chunk[0].copy()
                        target_arm_state = first_action[arm_dims]  # ç›®æ ‡æ‰‹è‡‚å…³èŠ‚ä½ç½®
                        target_claw_state = first_action[claw_dims]  # ç›®æ ‡å¤¹çˆªä½ç½®
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦cmd_pose
                        has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
                        if has_cmd_pose and action_dim >= 18:
                            target_cmd_pose = first_action[16:18]
                            current_cmd_pose = np.array([0.0, 0.0])  # é»˜è®¤cmd_pose
                        else:
                            target_cmd_pose = None
                            current_cmd_pose = None
                        
                        # è®¡ç®—æ’å€¼å‚æ•°
                        transition_duration = 0.2  # è¿‡æ¸¡æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œç¬¬ä¸€æ¬¡æ¨ç†æ—¶å¿«é€Ÿè¿‡æ¸¡åˆ°ç¬¬ä¸€ä¸ªaction
                        num_interp_steps = int(round(transition_duration / env.control_dt))
                        num_interp_steps = max(1, num_interp_steps)  # è‡³å°‘1æ­¥
                        
                        rospy.loginfo(f"   Current arm state: {current_arm_state}... (showing first 3 joints)")
                        rospy.loginfo(f"   Target arm state: {target_arm_state}... (showing first 3 joints)")
                        rospy.loginfo(f"   Generating {num_interp_steps} interpolation steps over {transition_duration:.2f}s")
                        
                        # ç”Ÿæˆæ’å€¼åŠ¨ä½œåºåˆ—ï¼ˆä½œä¸ºè¿‡æ¸¡chunkï¼Œä¸ç«‹å³æ‰§è¡Œï¼‰
                        interp_actions = []
                        for i in range(num_interp_steps):
                            alpha = (i + 1) / num_interp_steps  # ä»1/num_stepsåˆ°1.0
                            
                            # çº¿æ€§æ’å€¼æ‰‹è‡‚å…³èŠ‚
                            interp_arm = current_arm_state + (target_arm_state - current_arm_state) * alpha
                            
                            # çº¿æ€§æ’å€¼å¤¹çˆª
                            interp_claw = current_claw_state + (target_claw_state - current_claw_state) * alpha
                            
                            # æ„å»ºå®Œæ•´çš„action
                            if has_cmd_pose and target_cmd_pose is not None:
                                # 18ç»´æ ¼å¼ï¼šæ’å€¼cmd_pose
                                interp_cmd_pose = current_cmd_pose + (target_cmd_pose - current_cmd_pose) * alpha
                                interp_action = np.concatenate([interp_arm, interp_claw, interp_cmd_pose])
                            else:
                                # 16ç»´æ ¼å¼
                                interp_action = np.concatenate([interp_arm, interp_claw])
                            
                            interp_actions.append(interp_action)
                        
                        # å°†æ’å€¼åŠ¨ä½œåºåˆ—è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆä½œä¸ºè¿‡æ¸¡chunkï¼‰
                        transition_chunk = np.array(interp_actions)  # shape: (num_interp_steps, action_dim)
                        rospy.loginfo(f"   Generated transition chunk of size {transition_chunk.shape[0]}")
                        
                        # å°†è¿‡æ¸¡chunkå’ŒåŸå§‹chunkåˆå¹¶
                        # æ³¨æ„ï¼štransition_chunkçš„æœ€åä¸€ä¸ªactionåº”è¯¥ç­‰äºfirst_actionï¼ˆæˆ–éå¸¸æ¥è¿‘ï¼‰
                        # ä½†ä¸ºäº†ç¡®ä¿è¿ç»­æ€§ï¼Œæˆ‘ä»¬å°†transition_chunkå’Œaction_chunkåˆå¹¶
                        action_chunk = np.vstack([transition_chunk, action_chunk])
                        rospy.loginfo(f"   Combined transition + chunk: {transition_chunk.shape[0]} + {action_chunk.shape[0] - transition_chunk.shape[0]} = {action_chunk.shape[0]} steps")
                    else:
                        rospy.logwarn("âš ï¸  Warning: action_chunk is empty after chunk selection, cannot generate transition")
                    
                    FIRST_MODEL_INFERENCE = False
                
                # å¦‚æœéœ€è¦è¿æ¥ä¸Šä¸€ä¸ªchunkï¼Œæ·»åŠ æ¡¥æ¥
                if last_executed_action is not None:
                    # åœ¨chunkå‰æ·»åŠ ä¸Šä¸€ä¸ªactionï¼Œç¡®ä¿chunké—´å¹³æ»‘è¿æ¥
                    action_chunk_with_bridge = np.vstack([last_executed_action, action_chunk])
                else:
                    action_chunk_with_bridge = action_chunk
                
                # åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆå¦‚æœæä¾›ï¼‰
                if max_joint_velocity is not None:
                    # ä¿å­˜åŸå§‹chunkçš„å¤¹çˆªå€¼ï¼ˆåœ¨é€Ÿåº¦é™åˆ¶å‰ï¼‰
                    original_chunk_for_claw = action_chunk.copy()
                    
                    # ä½¿ç”¨æ§åˆ¶é¢‘ç‡çš„dt
                    control_dt = env.control_dt
                    
                    # å¦‚æœæœ‰transition_chunkï¼Œéœ€è¦åˆ†å¼€å¤„ç†ï¼štransitionéƒ¨åˆ†ä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆå¿«é€Ÿè¿‡æ¸¡ï¼‰ï¼Œchunkéƒ¨åˆ†åº”ç”¨é€Ÿåº¦é™åˆ¶
                    if transition_chunk is not None:
                        transition_size = transition_chunk.shape[0]
                        transition_part = action_chunk[:transition_size]  # transitionéƒ¨åˆ†ï¼ˆä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                        chunk_part = action_chunk[transition_size:]  # chunkéƒ¨åˆ†ï¼ˆåº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                        
                        # å¯¹chunkéƒ¨åˆ†è¿›è¡Œresampleåˆ°control_dté¢‘ç‡
                        if chunk_part.shape[0] > 0:
                            resampled_chunk_part = resample_action_chunk(
                                chunk_part,
                                source_dt=model_action_dt if model_action_dt is not None else DEFAULT_MODEL_ACTION_DT,
                                target_dt=control_dt
                            )
                            
                            # å¯¹chunkéƒ¨åˆ†åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆä¸åŒ…æ‹¬transitionéƒ¨åˆ†ï¼‰
                            # éœ€è¦è¿æ¥transitionçš„æœ€åä¸€ä¸ªactionå’Œchunkéƒ¨åˆ†
                            if transition_size > 0:
                                chunk_with_transition_end = np.vstack([transition_part[-1:], resampled_chunk_part])
                                resampled_chunk_part = resample_actions_with_speed_limit(
                                    chunk_with_transition_end,
                                    dt=control_dt,
                                    v_max=max_joint_velocity,
                                    arm_dims=arm_dims
                                )[1:]  # ç§»é™¤transitionçš„æœ€åä¸€ä¸ªaction
                            
                            # åˆå¹¶transitionï¼ˆä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰å’Œresampled chunkï¼ˆå·²åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                            action_chunk = np.vstack([transition_part, resampled_chunk_part])
                        else:
                            # å¦‚æœchunkéƒ¨åˆ†ä¸ºç©ºï¼Œåªä¿ç•™transitionéƒ¨åˆ†
                            action_chunk = transition_part
                    else:
                        # æ²¡æœ‰transition_chunkï¼Œæ­£å¸¸å¤„ç†æ•´ä¸ªchunk
                        # å¦‚æœéœ€è¦è¿æ¥ä¸Šä¸€ä¸ªchunkï¼Œæ·»åŠ æ¡¥æ¥
                        if last_executed_action is not None:
                            action_chunk_with_bridge = np.vstack([last_executed_action, action_chunk])
                        else:
                            action_chunk_with_bridge = action_chunk
                        
                        # åªå¯¹æ‰‹è‡‚å…³èŠ‚åº”ç”¨é€Ÿåº¦é™åˆ¶
                        action_chunk_with_bridge = resample_actions_with_speed_limit(
                            action_chunk_with_bridge,
                            dt=control_dt,
                            v_max=max_joint_velocity,
                            arm_dims=arm_dims
                        )
                        # ç§»é™¤æ¡¥æ¥çš„actionï¼ˆå¦‚æœæ·»åŠ äº†ï¼‰
                        if last_executed_action is not None:
                            action_chunk = action_chunk_with_bridge[1:]
                        else:
                            action_chunk = action_chunk_with_bridge
                    
                    # å¯¹å¤¹çˆªåº”ç”¨zero-order holdï¼ˆä»åŸå§‹chunkä¸­æå–ï¼‰
                    if action_chunk.shape[0] > 0 and original_chunk_for_claw.shape[0] > 0:
                        # å°†å¤¹çˆªå€¼æ’å€¼åˆ°resampled chunkçš„æ—¶é—´ç‚¹
                        if original_chunk_for_claw.shape[0] > 1:
                            # å¯¹äºåˆå¹¶åçš„chunkï¼ˆåŒ…å«transitionï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                            # transitionéƒ¨åˆ†ä½¿ç”¨control_dtï¼Œchunkéƒ¨åˆ†ä½¿ç”¨model_action_dt
                            if transition_chunk is not None:
                                # transitionéƒ¨åˆ†ï¼šä½¿ç”¨control_dt
                                transition_duration = transition_chunk.shape[0] * control_dt
                                # chunkéƒ¨åˆ†ï¼šä½¿ç”¨model_action_dt
                                source_dt_used = model_action_dt if model_action_dt is not None else DEFAULT_MODEL_ACTION_DT
                                chunk_duration = (original_chunk_for_claw.shape[0] - transition_chunk.shape[0]) * source_dt_used
                                
                                # æ„å»ºæºæ—¶é—´è½´ï¼ˆtransitionéƒ¨åˆ† + chunkéƒ¨åˆ†ï¼‰
                                transition_times = np.linspace(0.0, transition_duration, num=transition_chunk.shape[0], endpoint=False)
                                chunk_start_time = transition_duration
                                chunk_times = np.linspace(chunk_start_time, chunk_start_time + chunk_duration, 
                                                        num=original_chunk_for_claw.shape[0] - transition_chunk.shape[0])
                                source_times = np.concatenate([transition_times, chunk_times])
                            else:
                                source_dt_used = model_action_dt if model_action_dt is not None else DEFAULT_MODEL_ACTION_DT
                                source_times = np.linspace(0.0, source_dt_used * (original_chunk_for_claw.shape[0] - 1), num=original_chunk_for_claw.shape[0])
                            
                            target_times = np.linspace(0.0, control_dt * (action_chunk.shape[0] - 1), num=action_chunk.shape[0])
                            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
                            hold_indices = np.clip(hold_indices, 0, original_chunk_for_claw.shape[0] - 1)
                            action_chunk[:, claw_dims] = original_chunk_for_claw[hold_indices][:, claw_dims]
                        else:
                            action_chunk[:, claw_dims] = original_chunk_for_claw[0, claw_dims]
                else:
                    # å¦‚æœæ²¡æœ‰é€Ÿåº¦é™åˆ¶ï¼Œä½¿ç”¨resample_chunk_with_claw_holdæ¥ä¿æŒå¤¹çˆªçš„zero-order hold
                    # ä½†å¦‚æœæœ‰transition_chunkï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    if transition_chunk is not None:
                        # å¯¹äºåŒ…å«transitionçš„æƒ…å†µï¼Œéœ€è¦åˆ†åˆ«å¤„ç†transitionå’Œchunkéƒ¨åˆ†
                        # transitionéƒ¨åˆ†å·²ç»æ˜¯åœ¨control_dté¢‘ç‡ä¸‹ï¼Œä¸éœ€è¦resample
                        # chunkéƒ¨åˆ†éœ€è¦resample
                        transition_size = transition_chunk.shape[0]
                        chunk_part = action_chunk[transition_size:]
                        if chunk_part.shape[0] > 0:
                            resampled_chunk_part = resample_chunk_with_claw_hold(
                                chunk_part,
                                previous_action=action_chunk[transition_size - 1] if transition_size > 0 else last_executed_action,
                                control_frequency=env.control_frequency,
                                source_dt=model_action_dt if model_action_dt is not None else DEFAULT_MODEL_ACTION_DT,
                                arm_dims=arm_dims,
                                claw_dims=claw_dims
                            )
                            action_chunk = np.vstack([action_chunk[:transition_size], resampled_chunk_part])
                    else:
                        action_chunk = resample_chunk_with_claw_hold(
                            action_chunk,
                            previous_action=last_executed_action,
                            control_frequency=env.control_frequency,
                            source_dt=model_action_dt if model_action_dt is not None else DEFAULT_MODEL_ACTION_DT,
                            arm_dims=arm_dims,
                            claw_dims=claw_dims
                        )
                
                # æ‰§è¡Œæ•´ä¸ªchunk
                rospy.loginfo(f"Executing chunk of size {action_chunk.shape[0]} in sync mode")
                control_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
                
                for action_step in action_chunk:
                    env.exec_actions(actions=action_step,
                                     control_arm=control_arm,
                                     control_claw=control_claw,
                                     control_cmd_pose=control_cmd_pose)
                    step_counter += 1
                    last_executed_action = action_step.copy()
                    
                    # é”®ç›˜ç›‘å¬
                    key = 0
                    if enable_gui:
                        key = cv2.waitKey(1) & 0xFF
                    else:
                        try:
                            import select
                            if select.select([sys.stdin], [], [], 0)[0]:
                                import termios
                                import tty
                                old_settings = termios.tcgetattr(sys.stdin)
                                try:
                                    tty.setraw(sys.stdin.fileno())
                                    ch = sys.stdin.read(1)
                                    if ch:
                                        key = ord(ch)
                                finally:
                                    termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
                        except (ImportError, OSError, AttributeError):
                            pass
                    
                    if key == ord('q') or key == 27:
                        print("\n[Keyboard] Stopping current inference by user request")
                        FIRST_MODEL_INFERENCE = True
                        return True
                
                # æ‰§è¡Œå®Œchunkåï¼Œè·å–æ–°çš„è§‚æµ‹
                obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
                
            except KeyboardInterrupt:
                print("\n[Interrupted] Exiting by user Ctrl+C.")
                FIRST_MODEL_INFERENCE = True
                return False
    
    # å¼‚æ­¥æ¨¡å¼ï¼šåŸæœ‰çš„å®ç°
    while True:
        try:
            state = torch.from_numpy(obs_data["state"]).float()
            # print(f" ==== state ==== {state.shape} ==== ")
            
            # æ ¹æ®CAMERA_COMPONENTSåŠ¨æ€å¤„ç†ç›¸æœºå›¾åƒ
            # å¡«å……ç½‘ç»œçš„obs
            observation = {}
            
            # æ ¹æ®CAMERA_COMPONENTSæ˜ç¡®æŒ‡å®šéœ€è¦å¤„ç†çš„ç›¸æœº
            camera_names = get_camera_names(CAMERA_COMPONENTS)
            for camera_name in camera_names:
                # æ£€æŸ¥ç›¸æœºæ•°æ®æ˜¯å¦åœ¨obs_dataä¸­
                if camera_name in obs_data:
                    # è·å–ç›¸æœºå›¾åƒï¼Œobs_dataä¸­çš„å›¾åƒæ ¼å¼æ˜¯ (T, H, W, C)ï¼Œå…¶ä¸­Tæ˜¯æ—¶é—´æ­¥æ•°
                    camera_img_np = obs_data[camera_name]
                    
                    # æ£€æŸ¥å›¾åƒç»´åº¦ï¼Œåº”è¯¥æ˜¯ (T, H, W, C) æ ¼å¼
                    if camera_img_np.ndim != 4:
                        rospy.logwarn(f"âš ï¸  Unexpected camera image shape: {camera_img_np.shape}, expected (T, H, W, C)")
                        continue
                    
                    # å¦‚æœå¯ç”¨å¤´éƒ¨ç›¸æœºæ—‹è½¬ä¸”å½“å‰æ˜¯å¤´éƒ¨ç›¸æœºï¼ˆimageï¼‰ï¼Œåˆ™å¯¹æ¯ä¸€å¸§æ—‹è½¬180åº¦
                    if rotate_head_camera and camera_name == "image":
                        # æ—‹è½¬180åº¦ï¼šä½¿ç”¨np.rot90ï¼Œk=2è¡¨ç¤ºæ—‹è½¬180åº¦ï¼Œaxes=(1,2)è¡¨ç¤ºåœ¨Hå’ŒWç»´åº¦ä¸Šæ—‹è½¬
                        # camera_img_np shape: (T, H, W, C)
                        # å¯¹æ¯ä¸€å¸§è¿›è¡Œæ—‹è½¬ï¼Œaxes=(1,2)è¡¨ç¤ºåœ¨Hå’ŒWç»´åº¦ä¸Šæ—‹è½¬ï¼ˆä¿æŒTå’ŒCç»´åº¦ä¸å˜ï¼‰
                        # æ³¨æ„ï¼šnp.rot90å¯èƒ½äº§ç”Ÿè´Ÿæ­¥é•¿çš„è§†å›¾ï¼Œéœ€è¦copy()æ¥åˆ›å»ºè¿ç»­æ•°ç»„ï¼Œä»¥ä¾¿PyTorchå¯ä»¥å¤„ç†
                        camera_img_np = np.rot90(camera_img_np, k=2, axes=(1, 2)).copy()
                    
                    # è½¬æ¢ä¸º (T, C, H, W) æ ¼å¼å¹¶å½’ä¸€åŒ–
                    # ä½¿ç”¨np.moveaxiså°† (T, H, W, C) è½¬æ¢ä¸º (T, C, H, W)
                    # æ³¨æ„ï¼šnp.moveaxisä¹Ÿå¯èƒ½äº§ç”Ÿè´Ÿæ­¥é•¿ï¼Œä½¿ç”¨copy()ç¡®ä¿æ•°ç»„è¿ç»­
                    camera_images = torch.from_numpy(np.moveaxis(camera_img_np, 3, 1).copy()).float() / 255
                    # ä½¿ç”¨æ–°çš„keyæ ¼å¼: observation.images.cam_*
                    obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                    observation[obs_key] = camera_images.to('cuda:0')
                else:
                    # åªåœ¨ç¬¬ä¸€æ¬¡å‡ºç°æ—¶æ‰“å°è­¦å‘Š
                    if step_counter == 0:
                        rospy.logwarn(f"âš ï¸  Camera '{camera_name}' from CAMERA_COMPONENTS not found in obs_data. Available cameras: {[k for k in obs_data.keys() if 'image' in k.lower()]}")

            # observation['observation.environment_state'] = environment_state
            # å¦‚æœå¯ç”¨state_zeroæ¨¡å¼ï¼Œå°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
            if state_zero:
                # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰çŠ¶æ€å€¼è®¾ä¸º0
                observation['observation.state'] = torch.zeros_like(state).to('cuda:0')
            else:
                observation['observation.state'] = state.to('cuda:0')
            
            # æ·»åŠ  task å­—æ®µï¼ˆlanguage instructionï¼‰
            # processor ä¼šä» complementary_data ä¸­çš„ "task" å­—æ®µè¯»å–å¹¶è½¬æ¢ä¸º language
            observation['task'] = task_description

            if not resampled_action_queue:
                # ä½¿ç”¨GrootPolicyçš„predict_action_chunk
                # é¦–å…ˆä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†è¾“å…¥
                processed_observation = preprocessor(observation)
                
                # æ¨¡å‹æ¨ç†
                with torch.inference_mode():
                    pred_actions = policy.predict_action_chunk(processed_observation)
                
                # pred_actions shape: (batch_size, chunk_size, action_dim)
                # æ³¨æ„ï¼špred_actionsæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ŒèŒƒå›´åœ¨[-1, 1]
                # éœ€è¦æ‰‹åŠ¨åå½’ä¸€åŒ–åˆ°çœŸå®å•ä½
                
                # ä½¿ç”¨ postprocessor è¿›è¡Œåå½’ä¸€åŒ–
                # pred_actions shape: (batch_size, chunk_size, action_dim)
                # postprocessor æœŸæœ›è¾“å…¥æ˜¯ (B, action_dim)ï¼Œæ‰€ä»¥éœ€è¦å¤„ç†æ•´ä¸ª chunk
                _, chunk_size, _ = pred_actions.shape
                processed_actions = []
                for i in range(chunk_size):
                    # æå–å•ä¸ª action: (B, action_dim)
                    single_action = pred_actions[:, i, :]
                    # ä½¿ç”¨ postprocessor è¿›è¡Œåå½’ä¸€åŒ–
                    processed_action = postprocessor(single_action)
                    processed_actions.append(processed_action)
                
                # å †å å› (B, chunk_size, action_dim)ï¼Œç„¶åè½¬æ¢ä¸º numpy
                pred_actions_unnorm = torch.stack(processed_actions, dim=1)  # (B, chunk_size, action_dim)
                action_chunk = pred_actions_unnorm[0].cpu().numpy()  # (chunk_size, action_dim)

                # æ ¹æ®chunk_startå’Œchunk_endé€‰æ‹©è¦æ‰§è¡Œçš„actionèŒƒå›´
                chunk_size = action_chunk.shape[0]
                start_idx = chunk_start if chunk_start is not None else 0
                end_idx = chunk_end if chunk_end is not None else chunk_size - 1
                
                # éªŒè¯ç´¢å¼•èŒƒå›´
                if start_idx < 0:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_start {start_idx} is negative, using 0 instead")
                    start_idx = 0
                if end_idx >= chunk_size:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_end {end_idx} is >= chunk_size {chunk_size}, using {chunk_size - 1} instead")
                    end_idx = chunk_size - 1
                if start_idx > end_idx:
                    rospy.logwarn(f"âš ï¸ Warning: chunk_start {start_idx} > chunk_end {end_idx}, using last action only")
                    action_chunk = action_chunk[-1:].copy()
                else:
                    # ä½¿ç”¨åˆ‡ç‰‡é€‰æ‹©èŒƒå›´ï¼ˆPythonåˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€ï¼Œæ‰€ä»¥end_idx+1ï¼‰
                    action_chunk = action_chunk[start_idx:end_idx+1].copy()
                    rospy.loginfo(f"â­ï¸  Selected actions from index {start_idx} to {end_idx} (inclusive): {action_chunk.shape[0]} actions")

                # æ ¹æ®åŠ¨ä½œç»´åº¦åŠ¨æ€ç¡®å®šclawç»´åº¦
                action_dim = action_chunk.shape[1]
                if action_dim == 16:
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                elif action_dim == 18:
                    # 18ç»´æ ¼å¼: å‰14ç»´æ˜¯æ‰‹è‡‚å…³èŠ‚ï¼Œ14-16æ˜¯å¤¹çˆªï¼Œ16-18æ˜¯cmd_poseï¼ˆä¿ç•™å®Œæ•´18ç»´ï¼‰
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, 16)
                    # æ³¨æ„ï¼šcmd_poseç»´åº¦(16-18)ä¼šåœ¨resampleæ—¶ä¸€èµ·å¤„ç†ï¼Œä¸éœ€è¦å•ç‹¬å¤„ç†
                else:
                    # é»˜è®¤ä½¿ç”¨å‰14ç»´ä½œä¸ºæ‰‹è‡‚ï¼Œ14-16ä½œä¸ºå¤¹çˆª
                    arm_dims = slice(0, 14)
                    claw_dims = slice(14, min(16, action_dim))
                    rospy.logwarn(f"Unknown action dimension {action_dim}, using default arm/claw split")
                
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ¨¡å‹æ¨ç†ï¼Œéœ€è¦åœ¨å½“å‰çŠ¶æ€å’Œç¬¬ä¸€ä¸ªactionä¹‹é—´è¿›è¡Œæ’å€¼
                # å°†æ’å€¼åŠ¨ä½œåºåˆ—å’Œchunkåˆå¹¶ï¼Œç»Ÿä¸€è¿›è¡Œresampleå’Œé€Ÿåº¦é™åˆ¶å¤„ç†
                transition_chunk = None
                if FIRST_MODEL_INFERENCE and action_chunk.shape[0] > 0:
                    rospy.loginfo("ğŸ”„ First model inference (async mode): generating smooth transition from current robot state to first action")
                    
                    # åœ¨å¼‚æ­¥æ¨¡å¼ä¸‹ï¼Œéœ€è¦å…ˆè·å–æœ€æ–°çš„obs_dataå’Œrobot_obsï¼ˆç¡®ä¿è·å–åˆ°bagå›æ”¾åçš„çœŸå®çŠ¶æ€ï¼‰
                    obs_data, camera_obs, camera_obs_ts, robot_obs_for_transition, robot_obs_ts = env.get_obs()
                    
                    # è·å–å½“å‰æœºå™¨äººçš„æ‰‹è‡‚çŠ¶æ€
                    current_arm_state = obs_data["state"][0][arm_dims]  # å½“å‰æ‰‹è‡‚å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰
                    
                    # è·å–å½“å‰å¤¹çˆªçŠ¶æ€
                    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
                    try:
                        if 'claw_state' in robot_obs_for_transition and len(robot_obs_for_transition['claw_state']) > 0:
                            claw_data = robot_obs_for_transition['claw_state']
                            if claw_data.ndim == 2:
                                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
                            elif claw_data.ndim == 1:
                                current_claw_state = np.array(claw_data, dtype=np.float32)
                            if current_claw_state.shape[0] != 2:
                                current_claw_state = np.array([0.0, 0.0])
                    except Exception as e:
                        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
                        current_claw_state = np.array([0.0, 0.0])
                    
                    # è·å–ç¬¬ä¸€ä¸ªchunkçš„ç¬¬ä¸€ä¸ªactionï¼ˆå·²ç»æ ¹æ®chunk_start/chunk_endé€‰æ‹©ä¹‹åï¼‰
                    first_action = action_chunk[0].copy()
                    target_arm_state = first_action[arm_dims]  # ç›®æ ‡æ‰‹è‡‚å…³èŠ‚ä½ç½®
                    target_claw_state = first_action[claw_dims]  # ç›®æ ‡å¤¹çˆªä½ç½®
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦cmd_pose
                    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
                    if has_cmd_pose and action_dim >= 18:
                        target_cmd_pose = first_action[16:18]
                        current_cmd_pose = np.array([0.0, 0.0])  # é»˜è®¤cmd_pose
                    else:
                        target_cmd_pose = None
                        current_cmd_pose = None
                    
                    # è®¡ç®—æ’å€¼å‚æ•°
                    transition_duration = 0.2  # è¿‡æ¸¡æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œç¬¬ä¸€æ¬¡æ¨ç†æ—¶å¿«é€Ÿè¿‡æ¸¡åˆ°ç¬¬ä¸€ä¸ªaction
                    num_interp_steps = int(round(transition_duration / env.control_dt))
                    num_interp_steps = max(1, num_interp_steps)  # è‡³å°‘1æ­¥
                    
                    rospy.loginfo(f"   Current arm state: {current_arm_state[:3]}... (showing first 3 joints)")
                    rospy.loginfo(f"   Target arm state: {target_arm_state[:3]}... (showing first 3 joints)")
                    rospy.loginfo(f"   Generating {num_interp_steps} interpolation steps over {transition_duration:.2f}s")
                    
                    # ç”Ÿæˆæ’å€¼åŠ¨ä½œåºåˆ—ï¼ˆä½œä¸ºè¿‡æ¸¡chunkï¼Œä¸ç«‹å³æ‰§è¡Œï¼‰
                    interp_actions = []
                    for i in range(num_interp_steps):
                        alpha = (i + 1) / num_interp_steps  # ä»1/num_stepsåˆ°1.0
                        
                        # çº¿æ€§æ’å€¼æ‰‹è‡‚å…³èŠ‚
                        interp_arm = current_arm_state + (target_arm_state - current_arm_state) * alpha
                        
                        # çº¿æ€§æ’å€¼å¤¹çˆª
                        interp_claw = current_claw_state + (target_claw_state - current_claw_state) * alpha
                        
                        # æ„å»ºå®Œæ•´çš„action
                        if has_cmd_pose and target_cmd_pose is not None:
                            # 18ç»´æ ¼å¼ï¼šæ’å€¼cmd_pose
                            interp_cmd_pose = current_cmd_pose + (target_cmd_pose - current_cmd_pose) * alpha
                            interp_action = np.concatenate([interp_arm, interp_claw, interp_cmd_pose])
                        else:
                            # 16ç»´æ ¼å¼
                            interp_action = np.concatenate([interp_arm, interp_claw])
                        
                        interp_actions.append(interp_action)
                    
                    # å°†æ’å€¼åŠ¨ä½œåºåˆ—è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆä½œä¸ºè¿‡æ¸¡chunkï¼‰
                    transition_chunk = np.array(interp_actions)  # shape: (num_interp_steps, action_dim)
                    rospy.loginfo(f"   Generated transition chunk of size {transition_chunk.shape[0]}")
                    
                    # å°†è¿‡æ¸¡chunkå’ŒåŸå§‹chunkåˆå¹¶
                    action_chunk = np.vstack([transition_chunk, action_chunk])
                    rospy.loginfo(f"   Combined transition + chunk: {transition_chunk.shape[0]} + {action_chunk.shape[0] - transition_chunk.shape[0]} = {action_chunk.shape[0]} steps")
                    
                    FIRST_MODEL_INFERENCE = False
                
                # å¦‚æœéœ€è¦è¿æ¥ä¸Šä¸€ä¸ªchunkï¼Œæ·»åŠ æ¡¥æ¥
                if last_executed_action is not None:
                    action_chunk_with_bridge = np.vstack([last_executed_action, action_chunk])
                else:
                    action_chunk_with_bridge = action_chunk
                
                # åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆå¦‚æœæä¾›ï¼‰
                if max_joint_velocity is not None:
                    # ä¿å­˜åŸå§‹chunkçš„å¤¹çˆªå€¼ï¼ˆåœ¨é€Ÿåº¦é™åˆ¶å‰ï¼‰
                    original_chunk_for_claw = action_chunk.copy()
                    
                    # å¦‚æœæœ‰transition_chunkï¼Œéœ€è¦åˆ†å¼€å¤„ç†ï¼štransitionéƒ¨åˆ†ä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆå¿«é€Ÿè¿‡æ¸¡ï¼‰ï¼Œchunkéƒ¨åˆ†åº”ç”¨é€Ÿåº¦é™åˆ¶
                    if transition_chunk is not None:
                        transition_size = transition_chunk.shape[0]
                        transition_part = action_chunk[:transition_size]  # transitionéƒ¨åˆ†ï¼ˆä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                        chunk_part = action_chunk[transition_size:]  # chunkéƒ¨åˆ†ï¼ˆåº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                        
                        # å¯¹chunkéƒ¨åˆ†è¿›è¡Œresampleåˆ°control_dté¢‘ç‡
                        if chunk_part.shape[0] > 0:
                            resampled_chunk_part = resample_action_chunk(
                                chunk_part,
                                source_dt=model_action_dt,
                                target_dt=env.control_dt
                            )
                            
                            # å¯¹chunkéƒ¨åˆ†åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼ˆä¸åŒ…æ‹¬transitionéƒ¨åˆ†ï¼‰
                            # éœ€è¦è¿æ¥transitionçš„æœ€åä¸€ä¸ªactionå’Œchunkéƒ¨åˆ†
                            if transition_size > 0:
                                chunk_with_transition_end = np.vstack([transition_part[-1:], resampled_chunk_part])
                                resampled_chunk_part = resample_actions_with_speed_limit(
                                    chunk_with_transition_end,
                                    dt=env.control_dt,
                                    v_max=max_joint_velocity,
                                    arm_dims=arm_dims
                                )[1:]  # ç§»é™¤transitionçš„æœ€åä¸€ä¸ªaction
                            
                            # åˆå¹¶transitionï¼ˆä¸åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰å’Œresampled chunkï¼ˆå·²åº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
                            resampled_chunk = np.vstack([transition_part, resampled_chunk_part])
                        else:
                            # å¦‚æœchunkéƒ¨åˆ†ä¸ºç©ºï¼Œåªä¿ç•™transitionéƒ¨åˆ†
                            resampled_chunk = transition_part
                    else:
                        # æ²¡æœ‰transition_chunkï¼Œæ­£å¸¸å¤„ç†æ•´ä¸ªchunk
                        # å…ˆresampleåˆ°æ§åˆ¶é¢‘ç‡
                        if last_executed_action is not None:
                            resampled_bridge = resample_action_chunk(
                                action_chunk_with_bridge,
                                source_dt=model_action_dt,
                                target_dt=env.control_dt
                            )
                            resampled_chunk = resampled_bridge[1:]  # ç§»é™¤æ¡¥æ¥çš„action
                        else:
                            resampled_chunk = resample_action_chunk(
                                action_chunk,
                                source_dt=model_action_dt,
                                target_dt=env.control_dt
                            )
                        
                        # åº”ç”¨é€Ÿåº¦é™åˆ¶åˆ°armå…³èŠ‚
                        if last_executed_action is not None:
                            # å°†ä¸Šä¸€ä¸ªactionå’Œresampled chunkè¿æ¥ï¼Œåº”ç”¨é€Ÿåº¦é™åˆ¶
                            chunk_with_prev = np.vstack([last_executed_action, resampled_chunk])
                            resampled_chunk = resample_actions_with_speed_limit(
                                chunk_with_prev,
                                dt=env.control_dt,
                                v_max=max_joint_velocity,
                                arm_dims=arm_dims
                            )[1:]  # ç§»é™¤æ¡¥æ¥çš„action
                        else:
                            resampled_chunk = resample_actions_with_speed_limit(
                                resampled_chunk,
                                dt=env.control_dt,
                                v_max=max_joint_velocity,
                                arm_dims=arm_dims
                            )
                    
                    # å¯¹å¤¹çˆªåº”ç”¨zero-order holdï¼ˆä»åŸå§‹chunkä¸­æå–ï¼‰
                    if resampled_chunk.shape[0] > 0 and original_chunk_for_claw.shape[0] > 0:
                        # å°†å¤¹çˆªå€¼æ’å€¼åˆ°resampled chunkçš„æ—¶é—´ç‚¹
                        if original_chunk_for_claw.shape[0] > 1:
                            # å¯¹äºåˆå¹¶åçš„chunkï¼ˆåŒ…å«transitionï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                            if transition_chunk is not None:
                                # transitionéƒ¨åˆ†ï¼šä½¿ç”¨control_dt
                                transition_duration = transition_chunk.shape[0] * env.control_dt
                                # chunkéƒ¨åˆ†ï¼šä½¿ç”¨model_action_dt
                                chunk_duration = (original_chunk_for_claw.shape[0] - transition_chunk.shape[0]) * model_action_dt
                                
                                # æ„å»ºæºæ—¶é—´è½´ï¼ˆtransitionéƒ¨åˆ† + chunkéƒ¨åˆ†ï¼‰
                                transition_times = np.linspace(0.0, transition_duration, num=transition_chunk.shape[0], endpoint=False)
                                chunk_start_time = transition_duration
                                chunk_times = np.linspace(chunk_start_time, chunk_start_time + chunk_duration, 
                                                        num=original_chunk_for_claw.shape[0] - transition_chunk.shape[0])
                                source_times = np.concatenate([transition_times, chunk_times])
                            else:
                                source_times = np.linspace(0.0, model_action_dt * (original_chunk_for_claw.shape[0] - 1), num=original_chunk_for_claw.shape[0])
                            
                            target_times = np.linspace(0.0, env.control_dt * (resampled_chunk.shape[0] - 1), num=resampled_chunk.shape[0])
                            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
                            hold_indices = np.clip(hold_indices, 0, original_chunk_for_claw.shape[0] - 1)
                            resampled_chunk[:, claw_dims] = original_chunk_for_claw[hold_indices][:, claw_dims]
                        else:
                            resampled_chunk[:, claw_dims] = original_chunk_for_claw[0, claw_dims]
                else:
                    # æ²¡æœ‰é€Ÿåº¦é™åˆ¶ï¼Œä½¿ç”¨åŸæœ‰çš„resampleæ–¹æ³•
                    # ä½†å¦‚æœæœ‰transition_chunkï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    if transition_chunk is not None:
                        # å¯¹äºåŒ…å«transitionçš„æƒ…å†µï¼Œéœ€è¦åˆ†åˆ«å¤„ç†transitionå’Œchunkéƒ¨åˆ†
                        # transitionéƒ¨åˆ†å·²ç»æ˜¯åœ¨control_dté¢‘ç‡ä¸‹ï¼Œä¸éœ€è¦resample
                        # chunkéƒ¨åˆ†éœ€è¦resample
                        transition_size = transition_chunk.shape[0]
                        chunk_part = action_chunk[transition_size:]
                        if chunk_part.shape[0] > 0:
                            resampled_chunk_part = resample_chunk_with_claw_hold(
                                chunk_part,
                                previous_action=action_chunk[transition_size - 1] if transition_size > 0 else last_executed_action,
                                control_frequency=env.control_frequency,
                                source_dt=model_action_dt,
                                arm_dims=arm_dims,
                                claw_dims=claw_dims
                            )
                            resampled_chunk = np.vstack([action_chunk[:transition_size], resampled_chunk_part])
                        else:
                            resampled_chunk = action_chunk[:transition_size]
                    else:
                        resampled_chunk = resample_chunk_with_claw_hold(
                            action_chunk,
                            previous_action=last_executed_action,
                            control_frequency=env.control_frequency,
                            source_dt=model_action_dt,
                            arm_dims=arm_dims,
                            claw_dims=claw_dims
                        )

                # Apply low-pass filtering at chunk boundaries only if enabled
                if ENABLE_CHUNK_TRANSITION_LOWPASS:
                    if last_executed_action is not None:
                        transition_steps = max(
                            1,
                            int(round(env.control_frequency * CHUNK_TRANSITION_DURATION_S))
                        )
                    else:
                        transition_steps = None
                    
                    resampled_chunk = apply_lowpass_transition(
                        resampled_chunk,
                        previous_action=last_executed_action,
                        alpha=LOWPASS_ALPHA,
                        transition_steps=transition_steps,
                        smooth_slice=arm_dims  # åªå¯¹æ‰‹è‡‚å…³èŠ‚è¿›è¡Œä½é€šæ»¤æ³¢
                    )

                publish_joint_positions(
                    resampled_chunk,
                    joint_pub,
                    source_frequency_hz=env.control_frequency,
                    target_frequency_hz=None
                )
                rospy.loginfo(f"Prepared resampled chunk of size {resampled_chunk.shape[0]} for execution")

                resampled_action_queue = deque(np.array(step, copy=True) for step in resampled_chunk)

            current_action = resampled_action_queue.popleft()
            # æ ¹æ®ACTION_COMPONENTSé…ç½®å†³å®šæ˜¯å¦æ§åˆ¶cmd_pose
            # å¦‚æœACTION_COMPONENTSåŒ…å«Cmd_pose_zæˆ–Cmd_pose_pitchï¼Œåˆ™å¯ç”¨cmd_poseæ§åˆ¶
            control_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
            
            env.exec_actions(actions=current_action,
                             control_arm=control_arm,
                             control_claw=control_claw,
                             control_cmd_pose=control_cmd_pose)
            step_counter += 1
            last_executed_action = current_action.copy()

            obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()

            # é”®ç›˜ç›‘å¬ï¼ˆæ— è®ºæ˜¯å¦å¯ç”¨GUIéƒ½ç›‘å¬ï¼Œå‚è€ƒeval_depalletize_camera_dagger.pyçš„å®ç°æ–¹å¼ï¼‰
            key = 0
            if enable_gui:
                key = cv2.waitKey(1) & 0xFF
            else:
                # éGUIæ¨¡å¼ä¸‹ä½¿ç”¨éé˜»å¡é”®ç›˜ç›‘å¬
                try:
                    import select
                    if select.select([sys.stdin], [], [], 0)[0]:
                        import termios
                        import tty
                        old_settings = termios.tcgetattr(sys.stdin)
                        try:
                            tty.setraw(sys.stdin.fileno())
                            ch = sys.stdin.read(1)
                            if ch:
                                key = ord(ch)
                        finally:
                            termios.tcsetattr(sys.stdin, termios.TCSADRAIN, old_settings)
                except (ImportError, OSError, AttributeError):
                    # å¦‚æœselectæˆ–termiosä¸å¯ç”¨ï¼Œè·³è¿‡é”®ç›˜ç›‘å¬
                    pass
            
            if key == ord('q') or key == 27:  # 'q' or ESC to quit current inference
                print("\n[Keyboard] Stopping current inference by user request (q or ESC pressed)")
                FIRST_MODEL_INFERENCE = True
                return True  # è¿”å›Trueè¡¨ç¤ºæ­£å¸¸é€€å‡ºå½“å‰æ¨ç†
            
        except KeyboardInterrupt:
            print("\n[Interrupted] Exiting by user Ctrl+C.")
            FIRST_MODEL_INFERENCE = True
            return False  # è¿”å›Falseè¡¨ç¤ºè¢«ä¸­æ–­
    
    return True  # æ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šåˆ°è¾¾è¿™é‡Œ

def final_reset_arm(json_path, env, control_arm=True, control_claw=True):
    """
    ä½¿ç”¨JSONæ–‡ä»¶ä¸­çš„æ‰‹è‡‚è½¨è¿¹é‡ç½®æ‰‹è‡‚ä½ç½®
    
    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«åˆå§‹æ‰‹è‡‚è½¨è¿¹
        env: GrabBoxMpcEnvç¯å¢ƒå®ä¾‹
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
    """
    # å…ˆæ‰“å¼€å¤¹çˆª
    rospy.loginfo("Opening claws before reset...")
    # è·å–å½“å‰çŠ¶æ€
    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
    current_arm_state = obs_data["state"][0][:14]  # å½“å‰æ‰‹è‡‚ä½ç½®
    current_claw_state = np.array([0.0, 0.0])  # é»˜è®¤å€¼
    try:
        if 'claw_state' in robot_obs and len(robot_obs['claw_state']) > 0:
            claw_data = robot_obs['claw_state']
            if claw_data.ndim == 2:
                current_claw_state = np.array(claw_data[-1], dtype=np.float32)
            elif claw_data.ndim == 1:
                current_claw_state = np.array(claw_data, dtype=np.float32)
            if current_claw_state.shape[0] != 2:
                current_claw_state = np.array([0.0, 0.0])
    except Exception as e:
        rospy.logwarn(f"Could not get current claw state: {e}, using default [0.0, 0.0]")
        current_claw_state = np.array([0.0, 0.0])
    
    # æ‰“å¼€å¤¹çˆªï¼ˆè®¾ç½®ä¸º0ï¼‰ï¼Œä¿æŒæ‰‹è‡‚ä½ç½®ä¸å˜
    # æ³¨æ„ï¼šå¤¹çˆªçš„0å€¼è¡¨ç¤ºæ‰“å¼€çŠ¶æ€
    open_claw_value = np.zeros([2])  # [0.0, 0.0] è¡¨ç¤ºæ‰“å¼€å¤¹çˆª
    has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
    if has_cmd_pose:
        # 18ç»´æ ¼å¼
        open_claw_action = np.concatenate([current_arm_state, open_claw_value, np.array([0.0, 0.0])])
    else:
        # 16ç»´æ ¼å¼
        open_claw_action = np.concatenate([current_arm_state, open_claw_value])
    env.exec_actions(actions=open_claw_action, control_arm=False, control_claw=control_claw)
    time.sleep(1)
    
    # æ›´æ–°current_claw_stateä¸ºæ‰“å¼€åçš„çŠ¶æ€ï¼ˆ0ï¼‰ï¼Œç”¨äºåç»­çš„æ‰‹è‡‚é‡ç½®è¿‡ç¨‹
    current_claw_state = open_claw_value.copy()

    # åŠ è½½JSONæ–‡ä»¶ä¸­çš„æ‰‹è‡‚è½¨è¿¹
    rospy.loginfo(f"Loading initial arm trajectory from JSON: {json_path}")
    with open(json_path, 'r') as f:
        init_traj = json.load(f)
        arm_actions = init_traj['arm_action']  # List of arm actions
        dt = init_traj.get('dt', 0.1)  # è·å–æ—¶é—´é—´éš”ï¼Œé»˜è®¤0.1ç§’

    obs_data, camera_obs, camera_obs_ts, robot_obs, robot_obs_ts = env.get_obs()
    init_joints = np.array(arm_actions[-1])  # ç›®æ ‡å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰
    current_joints = obs_data["state"][0][:14]  # å½“å‰å…³èŠ‚ä½ç½®ï¼ˆ14ç»´ï¼‰

    # ç¡®ä¿init_jointsæ˜¯14ç»´
    if len(init_joints) != 14:
        rospy.logwarn(f"Expected 14 joint positions, got {len(init_joints)}. Using first 14 elements.")
        init_joints = np.array(init_joints[:14])

    total_time = 5.0  # æ€»æ—¶é—´ï¼ˆç§’ï¼‰
    num_points = int(total_time / dt)
    
    rospy.loginfo(f"Resetting arm from current position to initial position over {total_time}s ({num_points} steps)...")
    
    # ä»current_jointsåˆ°init_jointsæ’å€¼
    for i in range(1, num_points + 1):
        # çº¿æ€§æ’å€¼
        alpha = i / num_points
        interp_joints = current_joints + (init_joints - current_joints) * alpha
        
        # æ„å»ºå®Œæ•´çš„åŠ¨ä½œæ•°ç»„ï¼ˆæ ¹æ®ACTION_COMPONENTSæ ¼å¼ï¼‰
        # æ ¹æ®ACTION_COMPONENTSåŠ¨æ€ç¡®å®šåŠ¨ä½œç»´åº¦
        has_cmd_pose = ("Cmd_pose_z" in ACTION_COMPONENTS or "Cmd_pose_pitch" in ACTION_COMPONENTS)
        
        if has_cmd_pose:
            # 18ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®, 2ä¸ªcmd_pose]
            # ä¿æŒcmd_poseä¸å˜ï¼ˆä½¿ç”¨å½“å‰å€¼æˆ–0ï¼‰
            cmd_pose = np.array([0.0, 0.0])  # é»˜è®¤cmd_poseå€¼
            action = np.concatenate([interp_joints, current_claw_state, cmd_pose])
        else:
            # 16ç»´æ ¼å¼: [14ä¸ªæ‰‹è‡‚å…³èŠ‚, 2ä¸ªå¤¹çˆªä½ç½®]
            action = np.concatenate([interp_joints, current_claw_state])
        
        # ä½¿ç”¨exec_actionsæ‰§è¡ŒåŠ¨ä½œ
        env.exec_actions(actions=action, control_arm=control_arm, control_claw=control_claw)
        time.sleep(dt)
    
    rospy.loginfo("Arm reset completed!")


def eval(ckpt_path, model_type, control_arm=True, control_claw=True, action_chunk_size=50, enable_gui=False, rotate_head_camera=False, state_zero=False, task_description=None, chunk_start=None, chunk_end=None, model_action_dt=None, sync_mode=False, max_joint_velocity=None):
    """
    åœ¨è¿™é‡Œå’Œå®æœº/ä»¿çœŸäº¤äº’ï¼Œåšç½‘ç»œæ¨ç†ï¼ˆdepalletizeä»»åŠ¡ï¼‰
    æ”¯æŒå¤šæ¬¡æ¨ç†ï¼šæŒ‰'q'é€€å‡ºå½“å‰æ¨ç†ï¼Œå¯ä»¥å¿«é€Ÿé‡æ–°å¼€å§‹ä¸‹ä¸€æ¬¡æ¨ç†è€Œæ— éœ€é‡æ–°åŠ è½½æ¨¡å‹
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œç°åœ¨åªä½¿ç”¨GrootPolicyï¼‰
        control_arm: æ˜¯å¦æ§åˆ¶æ‰‹è‡‚
        control_claw: æ˜¯å¦æ§åˆ¶å¤¹çˆª
        action_chunk_size: åŠ¨ä½œå—å¤§å°
        enable_gui: æ˜¯å¦å¯ç”¨GUIçª—å£æ˜¾ç¤ºç›¸æœºå›¾åƒ
        rotate_head_camera: æ˜¯å¦æ—‹è½¬å¤´éƒ¨ç›¸æœºå›¾åƒ180åº¦
        state_zero: æ˜¯å¦å°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
        task_description: ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ï¼ˆlanguage instructionï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        chunk_start: è¦æ‰§è¡Œçš„chunkèµ·å§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼ŒåŒ…å«ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œä»ç¬¬ä¸€ä¸ªactionå¼€å§‹
        chunk_end: è¦æ‰§è¡Œçš„chunkç»“æŸç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼ŒåŒ…å«ï¼‰ã€‚å¦‚æœä¸ºNoneï¼Œæ‰§è¡Œåˆ°æœ€åä¸€ä¸ªaction
        model_action_dt: æ¨¡å‹åŠ¨ä½œæ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œæ§åˆ¶æ¨ç†é¢‘ç‡ã€‚ä¾‹å¦‚ï¼š0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz
                        å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.1 ç§’ï¼ˆ10 Hzï¼‰ã€‚åœ¨sync_modeä¸‹ä¸ä½¿ç”¨æ­¤å‚æ•°
        sync_mode: æ˜¯å¦ä½¿ç”¨åŒæ­¥æ¨ç†æ¨¡å¼ã€‚å¦‚æœTrueï¼Œæ¨ç†ä¸€ä¸ªchunk -> æ‰§è¡Œå®Œæ•´ä¸ªchunk -> get_obs -> å†æ¨ç†ä¸‹ä¸€ä¸ªchunk
        max_joint_velocity: æœ€å¤§å…³èŠ‚é€Ÿåº¦é™åˆ¶ï¼ˆrad/sï¼‰ã€‚å¦‚æœæä¾›ï¼Œå°†å¯¹armå…³èŠ‚åº”ç”¨é€Ÿåº¦é™åˆ¶
    """
    
    # åŠ è½½æ¨¡å‹å’Œç¯å¢ƒï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    policy, preprocessor, postprocessor, env, final_task_description, device = load_model_and_env(
        ckpt_path=ckpt_path,
        model_type=model_type,
        action_chunk_size=action_chunk_size,
        enable_gui=enable_gui,
        rotate_head_camera=rotate_head_camera,
        state_zero=state_zero,
        task_description=task_description
    )
    
    # ä¸»å¾ªç¯ï¼šæ”¯æŒå¤šæ¬¡æ¨ç†
    inference_count = 0
    while True:
        try:
            inference_count += 1
            is_first_inference = (inference_count == 1)
            
            print(f"\n{'='*80}")
            print(f"ğŸ”„ Starting inference session #{inference_count}")
            if is_first_inference:
                print(f"ğŸ“¦ First inference: will load bag file for initial trajectory")
            else:
                print(f"ğŸ“¦ Subsequent inference: will use JSON file for arm reset")
            print(f"{'='*80}\n")
            
            # é‡ç½®æ¨ç†çŠ¶æ€
            reset_inference_state(
                policy=policy,
                env=env
            )
            
            # è¿è¡Œæ¨ç†å¾ªç¯
            normal_exit = run_inference_loop(
                policy=policy,
                preprocessor=preprocessor,
                postprocessor=postprocessor,
                env=env,
                task_description=final_task_description,
                device=device,
                control_arm=control_arm,
                control_claw=control_claw,
                action_chunk_size=action_chunk_size,
                enable_gui=enable_gui,
                rotate_head_camera=rotate_head_camera,
                state_zero=state_zero,
                is_first_inference=is_first_inference,
                chunk_start=chunk_start,
                chunk_end=chunk_end,
                model_action_dt=model_action_dt,
                sync_mode=sync_mode,
                max_joint_velocity=max_joint_velocity
            )
            
            if normal_exit:
                # æ­£å¸¸é€€å‡ºï¼ˆæŒ‰qï¼‰ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡æ¨ç†
                print(f"\n{'='*80}")
                print(f"âœ… Inference session #{inference_count} stopped by user (q pressed)")
                cur_dir = os.path.dirname(os.path.abspath(__file__))
                # æ¯æ¬¡é€€å‡ºæ—¶éƒ½ä½¿ç”¨JSONæ–‡ä»¶é‡ç½®æ‰‹è‡‚ä½ç½®
                # ç¬¬ä¸€æ¬¡æ¨ç†å¼€å§‹æ—¶ä½¿ç”¨bagæ–‡ä»¶ï¼Œåç»­æ¨ç†å¼€å§‹æ—¶è·³è¿‡bagæ–‡ä»¶ï¼ˆåœ¨run_inference_loopä¸­å¤„ç†ï¼‰
                rospy.loginfo("Resetting arm position using JSON file...")
                final_reset_arm(
                    json_path=os.path.join(cur_dir, 'utils/initial_arm_traj.json'), 
                    env=env,
                    control_arm=control_arm,
                    control_claw=control_claw
                )
                print(f"ğŸ’¡ Ready for next inference session. Press Enter to start, or Ctrl+C to exit.")
                print(f"{'='*80}\n")
                
                # ç­‰å¾…ç”¨æˆ·è¾“å…¥ä»¥å¼€å§‹ä¸‹ä¸€æ¬¡æ¨ç†
                try:
                    user_input = input("Press Enter to start next inference, or 'q'+Enter to exit: ").strip().lower()
                    if user_input == 'q':
                        print("\nğŸ‘‹ Exiting program. Goodbye!")
                        break
                except (EOFError, KeyboardInterrupt):
                    print("\nğŸ‘‹ Exiting program. Goodbye!")
                    break
            else:
                # è¢«Ctrl+Cä¸­æ–­ï¼Œé€€å‡ºç¨‹åº
                print("\nğŸ‘‹ Exiting program due to Ctrl+C. Goodbye!")
                break
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Exiting program due to Ctrl+C. Goodbye!")
            break
        except Exception as e:
            rospy.logerr(f"Error during inference: {e}")
            import traceback
            traceback.print_exc()
            print("\nâŒ Error occurred. Exiting program.")
            break
    
    # Cleanup GUI windows
    if enable_gui:
        cv2.destroyAllWindows()




if __name__ == '__main__':
    # æœºå™¨äººä½å¤´
    robot_sdk = RobotSDK()
    robot_sdk.control.control_head(0, np.deg2rad(20))
    robot_sdk.control.set_external_control_arm_mode()  # åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼
    print(" ==== æœºå™¨äººå¤´éƒ¨ä¿¯ä»°è°ƒèŠ‚è§’åº¦: 20 æˆåŠŸ ==== ")
    print(" ==== åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== ")
    
    # python å‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description='Depalletize Task Evaluation Script',
        epilog='This script evaluates models for the depalletize task (16-dim actions: 14 arm joints + 2 claw positions).'
    )
    parser.add_argument('--ckpt-path', type=str, default='/home/lab/kuavo-manip/outputs/train/box_only_vel_obs/checkpoints/080000/pretrained_model',
                        help='Path to the checkpoint directory')
    parser.add_argument('--model-type', type=str, default='groot', choices=['groot', 'act', 'dp'],
                        help='Type of model to use (now only groot is supported, act/dp are deprecated)')
    parser.add_argument('--eval', action='store_true', help='Evaluate the model in real-time environment')
    parser.add_argument('--replay', action='store_true', help='Replay the model')
    parser.add_argument('--action_chunk_size', type=int, default=20, help='Number of action steps')
    parser.add_argument('--enable_gui', action='store_true',
                        help='Enable GUI windows for camera display (default: disabled)')
    parser.add_argument('--rotate-head-camera', action='store_true',
                        help='If set, rotate head camera images (image) by 180 degrees.')
    parser.add_argument('--state-zero', action='store_true',
                        help='If set, set all state inputs to zero (for testing model dependency on state)')
    parser.add_argument('--task-description', type=str, default=None,
                        help='Task description (language instruction) for the model. If not provided, will use the first task from dataset or a default value.')
    parser.add_argument('--chunk-start', type=int, default=None,
                        help='Start index (0-based, inclusive) of the chunk to execute. If not provided, starts from the first action.')
    parser.add_argument('--chunk-end', type=int, default=None,
                        help='End index (0-based, inclusive) of the chunk to execute. If not provided, executes to the last action.')
    parser.add_argument('--model-action-dt', type=float, default=None,
                        help='Time interval between predicted actions in seconds (controls inference frequency). '
                             'Smaller values = higher frequency. Examples: 0.1 = 10 Hz, 0.05 = 20 Hz, 0.033 = 30 Hz. '
                             'Default: 0.1 (10 Hz). Note: Model was trained with 0.1s interval. Ignored in sync mode.')
    parser.add_argument('--sync-mode', action='store_true',
                        help='Enable synchronous inference mode: inference -> execute chunk -> get_obs -> repeat. '
                             'In this mode, model_action_dt is ignored.')
    parser.add_argument('--max-joint-velocity', type=float, default=None,
                        help='Maximum joint velocity limit in rad/s. If provided, will apply speed limiting to arm joints. '
                             'Example: 2.0 means max 2.0 rad/s per joint.')
    
    args = parser.parse_args()
    
    # éªŒè¯chunk_startå’Œchunk_end
    if args.chunk_start is not None and args.chunk_start < 0:
        parser.error(f"--chunk-start must be >= 0, got {args.chunk_start}")
    if args.chunk_end is not None and args.chunk_end < 0:
        parser.error(f"--chunk-end must be >= 0, got {args.chunk_end}")
    if args.chunk_start is not None and args.chunk_end is not None and args.chunk_start > args.chunk_end:
        parser.error(f"--chunk-start ({args.chunk_start}) must be <= --chunk-end ({args.chunk_end})")
    
    # éªŒè¯model_action_dt
    if args.model_action_dt is not None:
        if args.model_action_dt <= 0.0:
            parser.error(f"--model-action-dt must be positive, got {args.model_action_dt}")
        if args.model_action_dt > 1.0:
            parser.error(f"--model-action-dt seems too large (> 1.0s), got {args.model_action_dt}")
        print(f"âš¡ Using custom MODEL_ACTION_DT: {args.model_action_dt:.3f}s (inference frequency: {1.0/args.model_action_dt:.1f} Hz)")
    else:
        print(f"âš¡ Using default MODEL_ACTION_DT: {DEFAULT_MODEL_ACTION_DT:.3f}s (inference frequency: {1.0/DEFAULT_MODEL_ACTION_DT:.1f} Hz)")
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å’Œç›¸æœºé…ç½®åˆå§‹åŒ–GUIçª—å£
    camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
    init_gui_windows(enable_gui=args.enable_gui, camera_config=camera_config)
    
    # æ‰“å°ç›¸æœºé…ç½®ä¿¡æ¯
    camera_names = get_camera_names(CAMERA_COMPONENTS)
    print(f"\nğŸ“· Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
    print(f"   CAMERA_COMPONENTS: {CAMERA_COMPONENTS}")
    print(f"   Camera names: {camera_names}")
    print(f"   Detected {len(camera_config)} cameras in topic_info: {list(camera_config.keys())}")
    
    print("\n" + "="*80)
    print("ğŸ¯ Depalletize Task Evaluation (GrootPolicy)")
    print("="*80)
    print(f"ğŸ“‚ Checkpoint: {args.ckpt_path}")
    print(f"ğŸ¤– Model type: {args.model_type} (using GrootPolicy)")
    if args.model_type != 'groot':
        print(f"âš ï¸  Warning: model-type '{args.model_type}' is deprecated. Using GrootPolicy instead.")
    print(f"ğŸ“Š Action chunk size: {args.action_chunk_size}")
    print(f"ğŸ“¦ Action dimension: Supports 16 or 18 (14 arm joints + 2 claw positions [+ 2 cmd_pose])")
    print(f"ğŸ–¼ï¸  Enable GUI: {args.enable_gui}")
    if args.rotate_head_camera:
        print(f"ğŸ”„ Rotate head camera: Enabled (images from 'image' camera will be rotated 180 degrees)")
    if args.state_zero:
        print(f"âš ï¸  State zero mode: Enabled (all state inputs will be set to zero)")
    if args.task_description:
        print(f"ğŸ“ Task description: '{args.task_description}'")
    if args.chunk_start is not None or args.chunk_end is not None:
        start_idx = args.chunk_start if args.chunk_start is not None else 0
        end_idx = args.chunk_end if args.chunk_end is not None else args.action_chunk_size - 1
        print(f"â­ï¸  Chunk selection: will execute actions from index {start_idx} to {end_idx} (inclusive)")
    if args.sync_mode:
        print(f"ğŸ”„ Sync mode: Enabled")
    elif args.model_action_dt is not None:
        print(f"âš¡ Model action DT: {args.model_action_dt:.3f}s (inference frequency: {1.0/args.model_action_dt:.1f} Hz)")
    if args.max_joint_velocity is not None:
        print(f"ğŸš¦ Max joint velocity limit: {args.max_joint_velocity:.2f} rad/s")
    print("="*80 + "\n")

    if args.eval:
        print("ğŸš€ Starting real-time evaluation...")
        eval(args.ckpt_path, model_type=args.model_type, control_arm=True, control_claw=True, 
             action_chunk_size=args.action_chunk_size, 
             enable_gui=args.enable_gui,
             rotate_head_camera=args.rotate_head_camera,
             state_zero=args.state_zero,
             task_description=args.task_description,
             chunk_start=args.chunk_start,
             chunk_end=args.chunk_end,
             model_action_dt=args.model_action_dt,
             sync_mode=args.sync_mode,
             max_joint_velocity=args.max_joint_velocity)
    elif args.replay:
        print("Replaying the model")
        lerobot_dataset_path = '/home/lab/kuavo-manip/lerobot_data/vel_wrend_box_613'
        replay(lerobot_dataset_path, episode=0, control_arm=True, control_claw=True)
    else:
        print("Please specify either --eval or --replay")
        exit(1)

    # --------------------------------------- #

