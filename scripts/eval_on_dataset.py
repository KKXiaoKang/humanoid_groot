#!/usr/bin/env python3
"""
Evaluate GrootPolicy Model on Dataset

This script evaluates a GrootPolicy model on a LeRobot dataset and computes error metrics.
It supports optional MuJoCo visualization.

Usage:
    python scripts/eval_on_dataset.py \
        --ckpt-path <checkpoint_path> \
        --dataset-root <dataset_path> \
        --episode <episode_number> \
        [--image-zero]  # Optional: set all images to zero to test model dependency on images
        [--state-zero]  # Optional: set all state inputs to zero to test model dependency on state
"""

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import numpy as np
from pathlib import Path
import argparse
import time
from collections import OrderedDict
from tqdm import tqdm

# ä½¿ç”¨GrootPolicyæ¨¡å‹
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# å¯¼å…¥é…ç½®æ¨¡å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, action_names
    CONFIG_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: configs.config not available. Using defaults.")
    CONFIG_AVAILABLE = False
    topic_info = {}
    TASK_DATA_MODE = "unknown"
    action_names = []
    def get_camera_observation_key(camera_name, use_image_features=False):
        return f"observation.images.{camera_name}" if use_image_features else f"observation.images.{camera_name}"

# å¯é€‰çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™ç¦ç”¨ï¼‰
try:
    from visualization_tools.visualizers import RerunVisualizer, KeyboardManager
    RERUN_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: RerunVisualizer not available. Visualization will be disabled.")
    RERUN_AVAILABLE = False
    RerunVisualizer = None
    KeyboardManager = None


def eval_on_dataset(ckpt_path,
                    lerobot_dataset_path,
                    episode,
                    visualize_in_mujoco=False,
                    n_actions=16,
                    show_progress=True,
                    image_zero=False,
                    state_zero=False):
    """
    åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        lerobot_dataset_path: æ•°æ®é›†æ ¹ç›®å½•
        episode: episodeç¼–å·
        visualize_in_mujoco: æ˜¯å¦åœ¨MuJoCoä¸­å¯è§†åŒ–æ‰§è¡Œ
        n_actions: action chunkå¤§å°
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        image_zero: æ˜¯å¦å°†å›¾åƒè¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹å›¾åƒçš„ä¾èµ–æ€§ï¼‰
        state_zero: æ˜¯å¦å°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
    """
    # ----------- ä¸€äº›å‚æ•° ----------------
    mse_per_action_dim = OrderedDict() # è®°å½•æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„MSE
    mae_per_action_dim = OrderedDict() # è®°å½•æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„MAE

    # ------------- åˆå§‹åŒ–visualizer (å¯é€‰) -------------
    if RERUN_AVAILABLE:
        vizer = RerunVisualizer()
        kb = KeyboardManager()
        print("âœ… RerunVisualizer initialized")
    else:
        vizer = None
        kb = None
        print("âš ï¸  Running without RerunVisualizer")

    # ------------- åˆå§‹åŒ–æ•°æ®é›†å’Œæ¨¡å‹ -------------
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    print(f"\n{'='*80}")
    print(f"ğŸ”§ Device: {device}")
    
    # âœ… ä½¿ç”¨GrootPolicyåŠ è½½æ¨¡å‹
    print(f"ğŸ“‚ Loading GrootPolicy model from {ckpt_path}...")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = n_actions
    
    print(f"ğŸ“Š Action chunk size: {n_actions}")
    if image_zero:
        print(f"âš ï¸  IMAGE ZERO MODE: All image inputs will be set to zero (for dependency testing)")
    if state_zero:
        print(f"âš ï¸  STATE ZERO MODE: All state inputs will be set to zero (for dependency testing)")
    
    policy.eval().to(device)
    
    # Load dataset statistics for normalization
    print(f"\nğŸ“‚ Loading dataset for statistics...")
    dataset_for_stats = LeRobotDataset(repo_id=0, root=lerobot_dataset_path)
    dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
    print(f"âœ… Dataset statistics loaded: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
    
    # æ£€æŸ¥actionç»Ÿè®¡ä¿¡æ¯æ ¼å¼ï¼Œç”¨äºæ‰‹åŠ¨åå½’ä¸€åŒ–
    if dataset_stats and 'action' in dataset_stats:
        action_stats = dataset_stats['action']
        print(f"ğŸ“Š Action stats keys: {list(action_stats.keys())}")
        if 'min' in action_stats and 'max' in action_stats:
            action_min = torch.as_tensor(action_stats['min'], dtype=torch.float32)
            action_max = torch.as_tensor(action_stats['max'], dtype=torch.float32)
            print(f"ğŸ“Š Action normalization range: min={action_min[:5].tolist()}... (shape: {action_min.shape}), max={action_max[:5].tolist()}... (shape: {action_max.shape})")
        else:
            print("âš ï¸  Warning: Action stats missing 'min' or 'max'. Manual denormalization may not work correctly.")
    
    # Create preprocessor and postprocessor
    print(f"\nğŸ”§ Creating preprocessor and postprocessor...")
    preprocessor, postprocessor = make_groot_pre_post_processors(
        config=policy.config,
        dataset_stats=dataset_stats,
    )
    print("âœ… Preprocessor and postprocessor created")
    
    # Debug: Print model configuration
    print(f"ğŸ” Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"ğŸ” Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    policy.reset()
    print("âœ… Model loaded and ready")
    
    # âœ… ä½¿ç”¨æ ‡å‡†çš„LeRobotDatasetåŠ è½½æ•°æ®
    print(f"\nğŸ“‚ Loading dataset from {lerobot_dataset_path}")
    print(f"ğŸ“¹ Episode: {episode}")
    
    # æ³¨æ„ï¼šLeRobotDatasetçš„episodeså‚æ•°ä¸»è¦ç”¨äºä¸‹è½½æ—¶é€‰æ‹©æ–‡ä»¶
    # ä½†åœ¨åŠ è½½åéœ€è¦æ‰‹åŠ¨è¿‡æ»¤æ•°æ®ï¼Œå› ä¸ºå¤šä¸ªepisodeså¯èƒ½å­˜å‚¨åœ¨åŒä¸€ä¸ªparquetæ–‡ä»¶ä¸­
    dataset = LeRobotDataset(repo_id=0, root=lerobot_dataset_path, episodes=[episode])
    
    # ä½¿ç”¨episodeçš„ç´¢å¼•èŒƒå›´ç›´æ¥åˆ‡ç‰‡ï¼Œæ¯”filterå¿«å¾—å¤š
    # è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºv3.0æ ¼å¼ä¸­å¤šä¸ªepisodeså¯èƒ½å­˜å‚¨åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­
    print(f"ğŸ” Filtering dataset to episode {episode}...")
    if episode >= len(dataset.meta.episodes):
        raise ValueError(f"Episode {episode} out of range. Available episodes: 0-{len(dataset.meta.episodes)-1}")
    
    ep_meta = dataset.meta.episodes[episode]
    ep_start = ep_meta["dataset_from_index"]
    ep_end = ep_meta["dataset_to_index"]
    
    # ä½¿ç”¨åˆ‡ç‰‡è€Œä¸æ˜¯filterï¼Œè¿™æ ·å¿«å¾—å¤š
    dataset.hf_dataset = dataset.hf_dataset.select(range(ep_start, ep_end))
    print(f"âœ… Filtered dataset. Total frames in episode {episode}: {len(dataset.hf_dataset)} (indices {ep_start}-{ep_end-1})")
    
    # æ‰“å°ç›¸æœºé…ç½®ä¿¡æ¯
    if CONFIG_AVAILABLE:
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
        print(f"\nğŸ“· Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
        print(f"   Detected {len(camera_config)} cameras: {list(camera_config.keys())}")
    else:
        # ä»æ•°æ®é›†å…ƒæ•°æ®ä¸­æ£€æµ‹ç›¸æœº
        sample = dataset[0]
        image_keys = [k for k in sample.keys() if 'image' in k.lower()]
        print(f"\nğŸ“· Camera Configuration:")
        print(f"   Detected {len(image_keys)} image keys: {image_keys}")
    
    # åˆ›å»ºdataloader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    print(f"âœ… Dataset loaded. Total frames: {dataset.num_frames}")

    # è·å–actionç»´åº¦
    first_batch = next(iter(dataloader))
    action_dim = first_batch['action'].shape[1]
    obs_dim = first_batch['observation.state'].shape[1]
    print(f"ğŸ“Š Action dimension: {action_dim}")
    print(f"ğŸ“Š Observation dimension: {obs_dim}")
    
    # é‡æ–°åˆ›å»ºdataloaderï¼ˆå› ä¸ºå·²ç»æ¶ˆè€—äº†ç¬¬ä¸€ä¸ªbatchï¼‰
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    # åˆå§‹åŒ–ç¯å¢ƒï¼ˆå¦‚æœéœ€è¦åœ¨mujocoä¸­å¯è§†åŒ–ï¼‰
    if visualize_in_mujoco:
        print(f"\nğŸ¤– Initializing MuJoCo environment...")
        # æ ¹æ®actionç»´åº¦åˆ¤æ–­ä½¿ç”¨å“ªä¸ªç¯å¢ƒ
        # 16ç»´åŠ¨ä½œ = depalletizeä»»åŠ¡ï¼Œä½¿ç”¨kuavo_depalletize_env
        # å…¶ä»–ç»´åº¦ = comæ§åˆ¶ä»»åŠ¡ï¼Œä½¿ç”¨kuavo_com_env
        if action_dim == 16:
            try:
                from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
                mujoco_env = GrabBoxMpcEnv()
                print(f"âœ… MuJoCo environment initialized (depalletize task)")
                print(f"   - Action dimension: 16 (14 arm joints + 2 claw positions)")
            except ImportError:
                print("âš ï¸  Warning: robot_envs.kuavo_depalletize_env not available. MuJoCo visualization disabled.")
                visualize_in_mujoco = False
                mujoco_env = None
        else:
            try:
                from robot_envs.kuavo_com_env import GrabBoxMpcEnv
                # GrootPolicy uses absolute actions by default
                mujoco_env = GrabBoxMpcEnv(use_action_history_reference=False)
                print(f"âœ… MuJoCo environment initialized (com control task)")
                print(f"   - use_action_history_reference: False (absolute actions)")
            except ImportError:
                print("âš ï¸  Warning: robot_envs.kuavo_com_env not available. MuJoCo visualization disabled.")
                visualize_in_mujoco = False
                mujoco_env = None
    
    # ========= å¯è§†åŒ–æ•°æ®é›†é‡Œçš„groundtruth (å¦‚æœå¯ç”¨RerunVisualizer) =========
    if vizer is not None:
        print(f"\nğŸ“Š Visualizing ground truth data...")
        # åŠ è½½æ‰€æœ‰ground truth actionsç”¨äºå¯è§†åŒ–
        all_gt_actions = []
        all_gt_states = []
        
        temp_dataloader = torch.utils.data.DataLoader(
            dataset, num_workers=0, batch_size=1, shuffle=False, drop_last=False
        )
        
        for batch in temp_dataloader:
            all_gt_actions.append(batch['action'][0].cpu().numpy())
            all_gt_states.append(batch['observation.state'][0].cpu().numpy())
        
        all_gt_actions = np.array(all_gt_actions)
        all_gt_states = np.array(all_gt_states)
        
        # å¯è§†åŒ–ground truth actions
        for dim in range(action_dim):
            vizer.visualize_chunk(
                name=f"chunk/action_dim_{dim}/gt",
                chunk_data=all_gt_actions[:, dim],
                step_id=0,
                width=3.0
            )
        
        # å¯è§†åŒ–observations
        for dim in range(obs_dim):
            vizer.visualize_chunk(
                name=f"obs/obs_{dim}",
                chunk_data=all_gt_states[:, dim],
                step_id=0,
                width=3.0
            )
        
        print(f"âœ… Ground truth visualization ready")

    # ========= å¼€å§‹æ¨¡å‹æ¨ç† =========
    print("\n" + "="*80)
    print("ğŸš€ Starting inference...")
    print("="*80 + "\n")
    
    last_data_step = 0
    predictions = []
    ground_truths = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    iterator = tqdm(enumerate(dataloader), total=dataset.num_frames, desc="Processing") if show_progress else enumerate(dataloader)
    
    for data_step, batch in iterator:
        # æš‚åœæ§åˆ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vizer is not None and kb is not None:
            time.sleep(0.05)
            if kb.paused:
                print(f'===== æš‚åœä¸­ï¼ŒæŒ‰ä¸‹ç©ºæ ¼å¼€å§‹ =====')
            while kb.paused:
                time.sleep(0.1)
        
        # âœ… å‡†å¤‡observation - ä½¿ç”¨Grooté¢„å¤„ç†å™¨
        # é¦–å…ˆæ„å»ºåŸå§‹observationå­—å…¸
        observation = {
            'observation.state': batch['observation.state'],
        }
        
        # æ·»åŠ æ‰€æœ‰å›¾åƒè§‚æµ‹ï¼ˆGrooté¢„å¤„ç†å™¨ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
        for key in batch.keys():
            if 'image' in key.lower() and key.startswith('observation'):
                observation[key] = batch[key]
        
        # å¦‚æœå¯ç”¨state_zeroæ¨¡å¼ï¼Œå°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
        if state_zero:
            # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰çŠ¶æ€å€¼è®¾ä¸º0
            observation['observation.state'] = torch.zeros_like(observation['observation.state'])
        
        # å¦‚æœå¯ç”¨image_zeroæ¨¡å¼ï¼Œå°†æ‰€æœ‰å›¾åƒè¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹å›¾åƒçš„ä¾èµ–æ€§ï¼‰
        if image_zero:
            for key in list(observation.keys()):
                if 'image' in key.lower():
                    # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰åƒç´ å€¼è®¾ä¸º0
                    observation[key] = torch.zeros_like(observation[key])
        
        # è·å–ground truth action
        gt_action = batch['action'][0].cpu().numpy()  # (action_dim,)
        
        # ä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†è¾“å…¥
        processed_observation = preprocessor(observation)
        
        # æ¨¡å‹æ¨ç†
        tic = time.time()
        with torch.inference_mode():
            pred_actions = policy.predict_action_chunk(processed_observation)
        
        # pred_actions shape: (batch_size, chunk_size, action_dim)
        # æ³¨æ„ï¼špred_actionsæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ŒèŒƒå›´åœ¨[-1, 1]
        # éœ€è¦æ‰‹åŠ¨åå½’ä¸€åŒ–åˆ°çœŸå®å•ä½ï¼Œä»¥ä¾¿ä¸ground truthè¿›è¡Œæ¯”è¾ƒ
        
        # å‡†å¤‡åå½’ä¸€åŒ–å‚æ•°
        if dataset_stats and 'action' in dataset_stats:
            action_stats = dataset_stats['action']
            if 'min' in action_stats and 'max' in action_stats:
                action_min = torch.as_tensor(action_stats['min'], dtype=torch.float32, device=pred_actions.device)
                action_max = torch.as_tensor(action_stats['max'], dtype=torch.float32, device=pred_actions.device)
                
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if action_min.numel() < action_dim:
                    action_min = torch.nn.functional.pad(action_min.flatten()[:action_dim], (0, max(0, action_dim - action_min.numel())))
                if action_max.numel() < action_dim:
                    action_max = torch.nn.functional.pad(action_max.flatten()[:action_dim], (0, max(0, action_dim - action_max.numel())))
                
                action_min = action_min[:action_dim]
                action_max = action_max[:action_dim]
                
                # åå½’ä¸€åŒ–å…¬å¼ï¼šx = (y + 1) / 2 * (max - min) + min
                # å…¶ä¸­yæ˜¯å½’ä¸€åŒ–å€¼[-1, 1]ï¼Œxæ˜¯åŸå§‹å€¼[min, max]
                denom = action_max - action_min
                mask = denom != 0
                safe_denom = torch.where(mask, denom, torch.ones_like(denom))
                
                # åå½’ä¸€åŒ–æ•´ä¸ªchunk
                pred_actions_unnorm = (pred_actions + 1.0) * 0.5 * safe_denom + action_min
                pred_actions_unnorm = torch.where(mask, pred_actions_unnorm, action_min)
                
                # é€‰æ‹©æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºå•æ­¥é¢„æµ‹
                pred_action_single = pred_actions_unnorm[0, -1, :].cpu().numpy()  # (action_dim,)
                # æ•´ä¸ªchunkç”¨äºå¯è§†åŒ–
                pred_chunk = pred_actions_unnorm[0].cpu().numpy()  # (chunk_size, action_dim)
            else:
                # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼ï¼ˆå¯èƒ½å·²ç»æ˜¯åå½’ä¸€åŒ–çš„ï¼‰
                pred_action_single = pred_actions[0, -1, :].cpu().numpy()  # (action_dim,)
                pred_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
                print("âš ï¸  Warning: No action min/max stats found. Using raw predictions (may be normalized).")
        else:
            # å¦‚æœæ²¡æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹å€¼
            pred_action_single = pred_actions[0, -1, :].cpu().numpy()  # (action_dim,)
            pred_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
            print("âš ï¸  Warning: No dataset stats found. Using raw predictions (may be normalized).")
        
        inference_time = time.time() - tic
        
        # ä¿å­˜é¢„æµ‹å’ŒçœŸå®å€¼
        predictions.append(pred_action_single)
        ground_truths.append(gt_action)
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„MSEå’ŒMAE
        for dim in range(action_dim):
            error = pred_action_single[dim] - gt_action[dim]
            mse = error ** 2
            mae = abs(error)
            
            if dim not in mse_per_action_dim:
                mse_per_action_dim[dim] = []
                mae_per_action_dim[dim] = []
            
            mse_per_action_dim[dim].append(mse)
            mae_per_action_dim[dim].append(mae)
        
        # å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vizer is not None:
            # æ˜¾ç¤ºå›¾åƒ - åŠ¨æ€æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¯ç”¨çš„ç›¸æœºå›¾åƒ
            for key in batch.keys():
                if 'image' in key.lower() and key.startswith('observation'):
                    img = batch[key][0]  # (C, H, W)
                    camera_name = key.replace('observation.', '').replace('observation.images.', '')
                    vizer.show_img(
                        name=camera_name,
                        image_data=img.to("cpu"),
                        step_id=data_step
                    )
                    # break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ç›¸æœºå›¾åƒ
            
            # å¯è§†åŒ–é¢„æµ‹çš„chunk
            for dim in range(action_dim):
                # å¯è§†åŒ–MSE
                vizer.visualize_chunk(
                    name=f"mse/action_dim_{dim}",
                    chunk_data=mse_per_action_dim[dim][-1],
                    step_id=data_step,
                    width=3.0,
                )
                
                # å¯è§†åŒ–é¢„æµ‹chunk
                vizer.visualize_chunk(
                    name=f"chunk/action_dim_{dim}/pred_seg_{data_step}",
                    chunk_data=pred_chunk[:, dim],
                    step_id=data_step,
                    width=2
                )
                
                # åˆ é™¤ä¸Šä¸€ä¸ªchunkçš„å¯è§†åŒ–
                if last_data_step != data_step and last_data_step > 0:
                    vizer.del_chunk(
                        name=f"chunk/action_dim_{dim}/pred_seg_{last_data_step}",
                        chunk_data=pred_chunk[:, dim],
                        step_id=last_data_step,
                        width=0.5
                    )
        
        last_data_step = data_step
        
        # ========== åœ¨mujocoé‡Œæ‰§è¡ŒåŠ¨ä½œ (å¦‚æœå¯ç”¨) =========
        if visualize_in_mujoco and mujoco_env is not None:
            action_np = pred_action_single[np.newaxis, :]  # (1, action_dim)
            
            # æ ¹æ®actionç»´åº¦é€‰æ‹©æ‰§è¡Œæ–¹æ³•
            if action_dim == 16:
                # depalletizeä»»åŠ¡ï¼š16ç»´åŠ¨ä½œ (14 arm joints + 2 claw positions)
                mujoco_env.exec_actions(
                    actions=action_np,
                    control_arm=True,
                    control_claw=True
                )
            else:
                # comæ§åˆ¶ä»»åŠ¡ï¼šä½¿ç”¨ç»å¯¹åŠ¨ä½œæ‰§è¡Œï¼ˆGrootPolicyé»˜è®¤ä½¿ç”¨ç»å¯¹åŠ¨ä½œï¼‰
                mujoco_env.exec_absolute_actions(
                    actions=action_np,
                    control_arm=True,
                    control_base=True,
                    control_wrench=False
                )

    # ========= æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ =========
    print("\n" + "="*80)
    print("ğŸ“Š Final Statistics")
    print("="*80)
    
    # Actionåç§°å®šä¹‰ - æ ¹æ®actionç»´åº¦è‡ªåŠ¨é€‰æ‹©
    # ä¼˜å…ˆä½¿ç”¨configä¸­çš„action_namesï¼ˆå¦‚æœå¯ç”¨ä¸”ç»´åº¦åŒ¹é…ï¼‰
    if CONFIG_AVAILABLE and action_names and len(action_names) == action_dim:
        eval_action_names = action_names
    elif action_dim == 16:
        # depalletizeä»»åŠ¡ï¼š16ç»´åŠ¨ä½œ (14 arm joints + 2 claw positions)
        eval_action_names = [f"Arm_joint_{i+1}" for i in range(14)] + ["Left_claw", "Right_claw"]
    elif action_dim == 18:
        # 18ç»´åŠ¨ä½œï¼šLeft_arm(7) + Right_arm(7) + Left_claw(1) + Right_claw(1) + Cmd_pose_z(1) + Cmd_pose_pitch(1)
        # æ ¹æ®config.pyçš„ACTION_COMPONENT_DEFINITIONSæ ¼å¼
        eval_action_names = (
            [f"arm_joint_{i+1}" for i in range(7)] +  # Left_arm: arm_joint_1-7
            [f"arm_joint_{i+8}" for i in range(7)] +  # Right_arm: arm_joint_8-14
            ["left_claw_position", "right_claw_position", "cmd_pose_z", "cmd_pose_pitch"]
        )
    elif action_dim == 24:
        # comæ§åˆ¶ä»»åŠ¡ï¼š24ç»´ = 9 COM + 14 Arm + 1 Gait
        eval_action_names = (
            ["COM_dx", "COM_dy", "COM_dz", "COM_dR11", "COM_dR21", "COM_dR31", "COM_dR12", "COM_dR22", "COM_dR32"] +
            [f"Arm_joint_{i+1}" for i in range(14)] +
            ["Gait_mode"]
        )
    else:
        # å…¶ä»–ç»´åº¦ï¼šä½¿ç”¨é€šç”¨å‘½å
        eval_action_names = [f"Action_dim_{i}" for i in range(action_dim)]
    
    print(f"\n{'Dimension':<20} {'MSE':<15} {'MAE':<15}")
    print("-" * 80)
    
    for dim in range(action_dim):
        mse_mean = np.mean(mse_per_action_dim[dim])
        mae_mean = np.mean(mae_per_action_dim[dim])
        dim_name = eval_action_names[dim] if dim < len(eval_action_names) else f"Dim_{dim}"
        print(f'{dim_name:<20} {mse_mean:<15.8f} {mae_mean:<15.8f}')
    
    overall_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(action_dim)])
    overall_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(action_dim)])
    
    print("-" * 80)
    print(f'{"Overall":<20} {overall_mse:<15.8f} {overall_mae:<15.8f}')
    
    # åˆ†ç»„ç»Ÿè®¡ - æ ¹æ®actionç»´åº¦é€‰æ‹©ç»Ÿè®¡æ–¹å¼
    print("\nğŸ“Š Grouped Statistics:")
    print("-" * 80)
    
    if action_dim == 16:
        # depalletizeä»»åŠ¡ï¼š16ç»´ = 14 arm joints + 2 claw positions
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        claw_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14, 16)])
        claw_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14, 16)])
        print(f'{"Claw (avg)":<20} {claw_mse:<15.8f} {claw_mae:<15.8f}')
    elif action_dim == 18:
        # 18ç»´åŠ¨ä½œï¼šLeft_arm(7) + Right_arm(7) + Left_claw(1) + Right_claw(1) + Cmd_pose_z(1) + Cmd_pose_pitch(1)
        left_arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(7)])
        left_arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(7)])
        print(f'{"Left_arm (avg)":<20} {left_arm_mse:<15.8f} {left_arm_mae:<15.8f}')
        
        right_arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(7, 14)])
        right_arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(7, 14)])
        print(f'{"Right_arm (avg)":<20} {right_arm_mse:<15.8f} {right_arm_mae:<15.8f}')
        
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        claw_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14, 16)])
        claw_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14, 16)])
        print(f'{"Claw (avg)":<20} {claw_mse:<15.8f} {claw_mae:<15.8f}')
        
        cmd_pose_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(16, 18)])
        cmd_pose_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(16, 18)])
        print(f'{"Cmd_pose (avg)":<20} {cmd_pose_mse:<15.8f} {cmd_pose_mae:<15.8f}')
    elif action_dim == 24:
        # comæ§åˆ¶ä»»åŠ¡ï¼š24ç»´ = 9 COM + 14 Arm + 1 Gait
        com_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9)])
        com_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9)])
        print(f'{"COM (avg)":<20} {com_mse:<15.8f} {com_mae:<15.8f}')
        
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9, 23)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9, 23)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        gait_mse = np.mean(mse_per_action_dim[23])
        gait_mae = np.mean(mae_per_action_dim[23])
        print(f'{"Gait":<20} {gait_mse:<15.8f} {gait_mae:<15.8f}')
    else:
        # å…¶ä»–ç»´åº¦ï¼šå°è¯•æ ¹æ®configæ¨æ–­ï¼Œæˆ–ä½¿ç”¨é€šç”¨åˆ†ç»„
        if CONFIG_AVAILABLE and action_names and len(action_names) == action_dim:
            # æ ¹æ®action_namesæ¨æ–­åˆ†ç»„
            # æŸ¥æ‰¾å¸¸è§çš„åˆ†ç»„æ¨¡å¼
            if any("arm" in name.lower() for name in action_names):
                # å°è¯•æ‰¾åˆ°armç›¸å…³çš„ç»´åº¦
                arm_dims = [i for i, name in enumerate(action_names) if "arm" in name.lower()]
                if arm_dims:
                    arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in arm_dims])
                    arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in arm_dims])
                    print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
            
            if any("claw" in name.lower() for name in action_names):
                claw_dims = [i for i, name in enumerate(action_names) if "claw" in name.lower()]
                if claw_dims:
                    claw_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in claw_dims])
                    claw_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in claw_dims])
                    print(f'{"Claw (avg)":<20} {claw_mse:<15.8f} {claw_mae:<15.8f}')
            
            if any("cmd_pose" in name.lower() or "com" in name.lower() for name in action_names):
                cmd_dims = [i for i, name in enumerate(action_names) if "cmd_pose" in name.lower() or "com" in name.lower()]
                if cmd_dims:
                    cmd_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in cmd_dims])
                    cmd_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in cmd_dims])
                    print(f'{"Cmd/COM (avg)":<20} {cmd_mse:<15.8f} {cmd_mae:<15.8f}')
        else:
            # æ— æ³•æ¨æ–­ï¼Œè·³è¿‡åˆ†ç»„ç»Ÿè®¡
            print("âš ï¸  Cannot infer action groups for this action dimension. Skipping grouped statistics.")
    
    print("="*80)

    if vizer is not None:
        print("\n[Offline Eval] Visualization active. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\nâœ… Exiting...")
    else:
        print("\nâœ… Evaluation completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Evaluate GrootPolicy Model on Dataset',
        epilog='Evaluates a trained GrootPolicy model on a LeRobot dataset.'
    )
    parser.add_argument('--ckpt-path', type=str, required=True,
                       help='Path to the model checkpoint directory')
    parser.add_argument('--dataset-root', '--dataset_root', type=str, required=True,
                       dest='dataset_root',
                       help='Path to the LeRobot dataset root directory')
    parser.add_argument('--episode', type=int, default=0,
                       help='Episode number to evaluate (default: 0)')
    parser.add_argument('--action-chunk-size', type=int, default=50,
                       help='Action chunk size (default: 50, should match training config)')
    parser.add_argument('--with-mujoco', action='store_true',
                       help='Visualize and execute in MuJoCo environment')
    parser.add_argument('--no-progress', action='store_true',
                       help='Disable progress bar')
    parser.add_argument('--image-zero', action='store_true',
                       help='Set all image inputs to zero (for testing model dependency on images)')
    parser.add_argument('--state-zero', action='store_true',
                       help='Set all state inputs to zero (for testing model dependency on state)')

    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ğŸ¯ GrootPolicy Dataset Evaluation")
    print("="*80)
    print(f"Checkpoint: {args.ckpt_path}")
    print(f"Dataset: {args.dataset_root}")
    print(f"Episode: {args.episode}")
    print(f"Action Chunk Size: {args.action_chunk_size}")
    print(f"MuJoCo Visualization: {args.with_mujoco}")
    print(f"Image Zero Mode: {args.image_zero}")
    print(f"State Zero Mode: {args.state_zero}")
    print("="*80)
    
    eval_on_dataset(
        ckpt_path=args.ckpt_path,
        lerobot_dataset_path=args.dataset_root,
        episode=args.episode,
        n_actions=args.action_chunk_size,
        visualize_in_mujoco=args.with_mujoco,
        show_progress=not args.no_progress,
        image_zero=args.image_zero,
        state_zero=args.state_zero
    )
