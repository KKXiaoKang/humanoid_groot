#!/usr/bin/env python3
"""
Evaluate GrootPolicy Model on Dataset

This script evaluates a GrootPolicy model on a LeRobot dataset and computes error metrics.
It supports optional MuJoCo visualization.

Usage:
    python scripts/eval_on_dataset.py \
        --ckpt-path <checkpoint_path> \
        --dataset-root <dataset_path> \
        --episode <episode_number> \
        [--image-zero]  # Optional: set all images to zero to test model dependency on images
        [--state-zero]  # Optional: set all state inputs to zero to test model dependency on state
"""

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import numpy as np
from pathlib import Path
import argparse
import time
from collections import OrderedDict
from tqdm import tqdm

# ‰ΩøÁî®GrootPolicyÊ®°Âûã
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# ÂØºÂÖ•ÈÖçÁΩÆÊ®°ÂùóÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
try:
    from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, action_names
    CONFIG_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Warning: configs.config not available. Using defaults.")
    CONFIG_AVAILABLE = False
    topic_info = {}
    TASK_DATA_MODE = "unknown"
    action_names = []
    def get_camera_observation_key(camera_name, use_image_features=False):
        return f"observation.images.{camera_name}" if use_image_features else f"observation.images.{camera_name}"

# ÂèØÈÄâÁöÑÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÁ¶ÅÁî®Ôºâ
try:
    from visualization_tools.visualizers import RerunVisualizer, KeyboardManager
    RERUN_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Warning: RerunVisualizer not available. Visualization will be disabled.")
    RERUN_AVAILABLE = False
    RerunVisualizer = None
    KeyboardManager = None


def eval_on_dataset(ckpt_path,
                    lerobot_dataset_path,
                    episode,
                    visualize_in_mujoco=False,
                    n_actions=16,
                    show_progress=True,
                    image_zero=False,
                    state_zero=False):
    """
    Âú®Êï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞Ê®°Âûã
    
    Args:
        ckpt_path: Ê®°ÂûãcheckpointË∑ØÂæÑ
        lerobot_dataset_path: Êï∞ÊçÆÈõÜÊ†πÁõÆÂΩï
        episode: episodeÁºñÂè∑
        visualize_in_mujoco: ÊòØÂê¶Âú®MuJoCo‰∏≠ÂèØËßÜÂåñÊâßË°å
        n_actions: action chunkÂ§ßÂ∞è
        show_progress: ÊòØÂê¶ÊòæÁ§∫ËøõÂ∫¶Êù°
        image_zero: ÊòØÂê¶Â∞ÜÂõæÂÉèËæìÂÖ•ÁΩÆÈõ∂ÔºàÁî®‰∫éÈ™åËØÅÊ®°ÂûãÂØπÂõæÂÉèÁöÑ‰æùËµñÊÄßÔºâ
        state_zero: ÊòØÂê¶Â∞ÜÁä∂ÊÄÅËæìÂÖ•ÁΩÆÈõ∂ÔºàÁî®‰∫éÈ™åËØÅÊ®°ÂûãÂØπÁä∂ÊÄÅÁöÑ‰æùËµñÊÄßÔºâ
    """
    # ----------- ‰∏Ä‰∫õÂèÇÊï∞ ----------------
    mse_per_action_dim = OrderedDict() # ËÆ∞ÂΩïÊØè‰∏™Âä®‰ΩúÁª¥Â∫¶ÁöÑMSE
    mae_per_action_dim = OrderedDict() # ËÆ∞ÂΩïÊØè‰∏™Âä®‰ΩúÁª¥Â∫¶ÁöÑMAE

    # ------------- ÂàùÂßãÂåñvisualizer (ÂèØÈÄâ) -------------
    if RERUN_AVAILABLE:
        vizer = RerunVisualizer()
        kb = KeyboardManager()
        print("‚úÖ RerunVisualizer initialized")
    else:
        vizer = None
        kb = None
        print("‚ö†Ô∏è  Running without RerunVisualizer")

    # ------------- ÂàùÂßãÂåñÊï∞ÊçÆÈõÜÂíåÊ®°Âûã -------------
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    print(f"\n{'='*80}")
    print(f"üîß Device: {device}")
    
    # ‚úÖ ‰ΩøÁî®GrootPolicyÂä†ËΩΩÊ®°Âûã
    print(f"üìÇ Loading GrootPolicy model from {ckpt_path}...")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = n_actions
    
    print(f"üìä Action chunk size: {n_actions}")
    if image_zero:
        print(f"‚ö†Ô∏è  IMAGE ZERO MODE: All image inputs will be set to zero (for dependency testing)")
    if state_zero:
        print(f"‚ö†Ô∏è  STATE ZERO MODE: All state inputs will be set to zero (for dependency testing)")
    
    policy.eval().to(device)
    
    # Load dataset statistics for normalization
    print(f"\nüìÇ Loading dataset for statistics...")
    dataset_for_stats = LeRobotDataset(repo_id=0, root=lerobot_dataset_path)
    dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
    print(f"‚úÖ Dataset statistics loaded: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
    
    # Create preprocessor and postprocessor
    print(f"\nüîß Creating preprocessor and postprocessor...")
    preprocessor, postprocessor = make_groot_pre_post_processors(
        config=policy.config,
        dataset_stats=dataset_stats,
    )
    print("‚úÖ Preprocessor and postprocessor created")
    
    # Debug: Print model configuration
    print(f"üîç Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"üîç Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    policy.reset()
    print("‚úÖ Model loaded and ready")
    
    # ‚úÖ ‰ΩøÁî®Ê†áÂáÜÁöÑLeRobotDatasetÂä†ËΩΩÊï∞ÊçÆ
    print(f"\nüìÇ Loading dataset from {lerobot_dataset_path}")
    print(f"üìπ Episode: {episode}")
    
    dataset = LeRobotDataset(repo_id=0, root=lerobot_dataset_path, episodes=[episode])
    
    # ÊâìÂç∞Áõ∏Êú∫ÈÖçÁΩÆ‰ø°ÊÅØ
    if CONFIG_AVAILABLE:
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
        print(f"\nüì∑ Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
        print(f"   Detected {len(camera_config)} cameras: {list(camera_config.keys())}")
    else:
        # ‰ªéÊï∞ÊçÆÈõÜÂÖÉÊï∞ÊçÆ‰∏≠Ê£ÄÊµãÁõ∏Êú∫
        sample = dataset[0]
        image_keys = [k for k in sample.keys() if 'image' in k.lower()]
        print(f"\nüì∑ Camera Configuration:")
        print(f"   Detected {len(image_keys)} image keys: {image_keys}")
    
    # ÂàõÂª∫dataloader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    print(f"‚úÖ Dataset loaded. Total frames: {dataset.num_frames}")

    # Ëé∑ÂèñactionÁª¥Â∫¶
    first_batch = next(iter(dataloader))
    action_dim = first_batch['action'].shape[1]
    obs_dim = first_batch['observation.state'].shape[1]
    print(f"üìä Action dimension: {action_dim}")
    print(f"üìä Observation dimension: {obs_dim}")
    
    # ÈáçÊñ∞ÂàõÂª∫dataloaderÔºàÂõ†‰∏∫Â∑≤ÁªèÊ∂àËÄó‰∫ÜÁ¨¨‰∏Ä‰∏™batchÔºâ
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    # ÂàùÂßãÂåñÁéØÂ¢ÉÔºàÂ¶ÇÊûúÈúÄË¶ÅÂú®mujoco‰∏≠ÂèØËßÜÂåñÔºâ
    if visualize_in_mujoco:
        print(f"\nü§ñ Initializing MuJoCo environment...")
        # Ê†πÊçÆactionÁª¥Â∫¶Âà§Êñ≠‰ΩøÁî®Âì™‰∏™ÁéØÂ¢É
        # 16Áª¥Âä®‰Ωú = depalletize‰ªªÂä°Ôºå‰ΩøÁî®kuavo_depalletize_env
        # ÂÖ∂‰ªñÁª¥Â∫¶ = comÊéßÂà∂‰ªªÂä°Ôºå‰ΩøÁî®kuavo_com_env
        if action_dim == 16:
            try:
                from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
                mujoco_env = GrabBoxMpcEnv()
                print(f"‚úÖ MuJoCo environment initialized (depalletize task)")
                print(f"   - Action dimension: 16 (14 arm joints + 2 claw positions)")
            except ImportError:
                print("‚ö†Ô∏è  Warning: robot_envs.kuavo_depalletize_env not available. MuJoCo visualization disabled.")
                visualize_in_mujoco = False
                mujoco_env = None
        else:
            try:
                from robot_envs.kuavo_com_env import GrabBoxMpcEnv
                # GrootPolicy uses absolute actions by default
                mujoco_env = GrabBoxMpcEnv(use_action_history_reference=False)
                print(f"‚úÖ MuJoCo environment initialized (com control task)")
                print(f"   - use_action_history_reference: False (absolute actions)")
            except ImportError:
                print("‚ö†Ô∏è  Warning: robot_envs.kuavo_com_env not available. MuJoCo visualization disabled.")
                visualize_in_mujoco = False
                mujoco_env = None
    
    # ========= ÂèØËßÜÂåñÊï∞ÊçÆÈõÜÈáåÁöÑgroundtruth (Â¶ÇÊûúÂêØÁî®RerunVisualizer) =========
    if vizer is not None:
        print(f"\nüìä Visualizing ground truth data...")
        # Âä†ËΩΩÊâÄÊúâground truth actionsÁî®‰∫éÂèØËßÜÂåñ
        all_gt_actions = []
        all_gt_states = []
        
        temp_dataloader = torch.utils.data.DataLoader(
            dataset, num_workers=0, batch_size=1, shuffle=False, drop_last=False
        )
        
        for batch in temp_dataloader:
            all_gt_actions.append(batch['action'][0].cpu().numpy())
            all_gt_states.append(batch['observation.state'][0].cpu().numpy())
        
        all_gt_actions = np.array(all_gt_actions)
        all_gt_states = np.array(all_gt_states)
        
        # ÂèØËßÜÂåñground truth actions
        for dim in range(action_dim):
            vizer.visualize_chunk(
                name=f"chunk/action_dim_{dim}/gt",
                chunk_data=all_gt_actions[:, dim],
                step_id=0,
                width=3.0
            )
        
        # ÂèØËßÜÂåñobservations
        for dim in range(obs_dim):
            vizer.visualize_chunk(
                name=f"obs/obs_{dim}",
                chunk_data=all_gt_states[:, dim],
                step_id=0,
                width=3.0
            )
        
        print(f"‚úÖ Ground truth visualization ready")

    # ========= ÂºÄÂßãÊ®°ÂûãÊé®ÁêÜ =========
    print("\n" + "="*80)
    print("üöÄ Starting inference...")
    print("="*80 + "\n")
    
    last_data_step = 0
    predictions = []
    ground_truths = []
    
    # ‰ΩøÁî®tqdmÊòæÁ§∫ËøõÂ∫¶ÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
    iterator = tqdm(enumerate(dataloader), total=dataset.num_frames, desc="Processing") if show_progress else enumerate(dataloader)
    
    for data_step, batch in iterator:
        # ÊöÇÂÅúÊéßÂà∂ÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
        if vizer is not None and kb is not None:
            time.sleep(0.05)
            if kb.paused:
                print(f'===== ÊöÇÂÅú‰∏≠ÔºåÊåâ‰∏ãÁ©∫Ê†ºÂºÄÂßã =====')
            while kb.paused:
                time.sleep(0.1)
        
        # ‚úÖ ÂáÜÂ§áobservation - ‰ΩøÁî®GrootÈ¢ÑÂ§ÑÁêÜÂô®
        # È¶ñÂÖàÊûÑÂª∫ÂéüÂßãobservationÂ≠óÂÖ∏
        observation = {
            'observation.state': batch['observation.state'],
        }
        
        # Ê∑ªÂä†ÊâÄÊúâÂõæÂÉèËßÇÊµãÔºàGrootÈ¢ÑÂ§ÑÁêÜÂô®‰ºöËá™Âä®Â§ÑÁêÜÔºâ
        for key in batch.keys():
            if 'image' in key.lower() and key.startswith('observation'):
                observation[key] = batch[key]
        
        # Â¶ÇÊûúÂêØÁî®state_zeroÊ®°ÂºèÔºåÂ∞ÜÁä∂ÊÄÅËæìÂÖ•ÁΩÆÈõ∂ÔºàÁî®‰∫éÈ™åËØÅÊ®°ÂûãÂØπÁä∂ÊÄÅÁöÑ‰æùËµñÊÄßÔºâ
        if state_zero:
            # ‰øùÊåÅÁõ∏ÂêåÁöÑÂΩ¢Áä∂ÂíåËÆæÂ§áÔºå‰ΩÜÂ∞ÜÊâÄÊúâÁä∂ÊÄÅÂÄºËÆæ‰∏∫0
            observation['observation.state'] = torch.zeros_like(observation['observation.state'])
        
        # Â¶ÇÊûúÂêØÁî®image_zeroÊ®°ÂºèÔºåÂ∞ÜÊâÄÊúâÂõæÂÉèËæìÂÖ•ÁΩÆÈõ∂ÔºàÁî®‰∫éÈ™åËØÅÊ®°ÂûãÂØπÂõæÂÉèÁöÑ‰æùËµñÊÄßÔºâ
        if image_zero:
            for key in list(observation.keys()):
                if 'image' in key.lower():
                    # ‰øùÊåÅÁõ∏ÂêåÁöÑÂΩ¢Áä∂ÂíåËÆæÂ§áÔºå‰ΩÜÂ∞ÜÊâÄÊúâÂÉèÁ¥†ÂÄºËÆæ‰∏∫0
                    observation[key] = torch.zeros_like(observation[key])
        
        # Ëé∑Âèñground truth action
        gt_action = batch['action'][0].cpu().numpy()  # (action_dim,)
        
        # ‰ΩøÁî®È¢ÑÂ§ÑÁêÜÂô®Â§ÑÁêÜËæìÂÖ•
        processed_observation = preprocessor(observation)
        
        # Ê®°ÂûãÊé®ÁêÜ
        tic = time.time()
        with torch.inference_mode():
            pred_actions = policy.predict_action_chunk(processed_observation)
        
        # ‰ΩøÁî®ÂêéÂ§ÑÁêÜÂô®Â§ÑÁêÜËæìÂá∫
        # pred_actions shape: (batch_size, chunk_size, action_dim)
        # PolicyAction Â∞±ÊòØ torch.Tensor ÁöÑÁ±ªÂûãÂà´ÂêçÔºåÁõ¥Êé•‰º†ÈÄíÂç≥ÂèØ
        processed_action = postprocessor(pred_actions)
        # ÂêéÂ§ÑÁêÜÂô®‰ºöËøîÂõû (B, action_dim) ÂΩ¢Áä∂ÁöÑÂº†ÈáèÔºàÂ∑≤ÈÄâÊã©ÊúÄÂêé‰∏Ä‰∏™Êó∂Èó¥Ê≠•Âπ∂ÂèçÂΩí‰∏ÄÂåñÔºâ
        pred_action_single = processed_action[0].cpu().numpy()  # (action_dim,)
        # ÂØπ‰∫échunkÂèØËßÜÂåñÔºåÊàë‰ª¨ÈúÄË¶Å‰ΩøÁî®ÂéüÂßãÁöÑpred_actionsÔºàÊú™ÁªèËøáÂêéÂ§ÑÁêÜÂô®Ôºâ
        pred_chunk = pred_actions[0].cpu().numpy()  # (chunk_size, action_dim)
        
        inference_time = time.time() - tic
        
        # ‰øùÂ≠òÈ¢ÑÊµãÂíåÁúüÂÆûÂÄº
        predictions.append(pred_action_single)
        ground_truths.append(gt_action)
        
        # ËÆ°ÁÆóÊØè‰∏™Áª¥Â∫¶ÁöÑMSEÂíåMAE
        for dim in range(action_dim):
            error = pred_action_single[dim] - gt_action[dim]
            mse = error ** 2
            mae = abs(error)
            
            if dim not in mse_per_action_dim:
                mse_per_action_dim[dim] = []
                mae_per_action_dim[dim] = []
            
            mse_per_action_dim[dim].append(mse)
            mae_per_action_dim[dim].append(mae)
        
        # ÂèØËßÜÂåñÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
        if vizer is not None:
            # ÊòæÁ§∫ÂõæÂÉè - Âä®ÊÄÅÊü•ÊâæÁ¨¨‰∏Ä‰∏™ÂèØÁî®ÁöÑÁõ∏Êú∫ÂõæÂÉè
            for key in batch.keys():
                if 'image' in key.lower() and key.startswith('observation'):
                    img = batch[key][0]  # (C, H, W)
                    camera_name = key.replace('observation.', '').replace('observation.images.', '')
                    vizer.show_img(
                        name=camera_name,
                        image_data=img.to("cpu"),
                        step_id=data_step
                    )
                    break  # Âè™ÊòæÁ§∫Á¨¨‰∏Ä‰∏™ÊâæÂà∞ÁöÑÁõ∏Êú∫ÂõæÂÉè
            
            # ÂèØËßÜÂåñÈ¢ÑÊµãÁöÑchunk
            for dim in range(action_dim):
                # ÂèØËßÜÂåñMSE
                vizer.visualize_chunk(
                    name=f"mse/action_dim_{dim}",
                    chunk_data=mse_per_action_dim[dim][-1],
                    step_id=data_step,
                    width=3.0,
                )
                
                # ÂèØËßÜÂåñÈ¢ÑÊµãchunk
                vizer.visualize_chunk(
                    name=f"chunk/action_dim_{dim}/pred_seg_{data_step}",
                    chunk_data=pred_chunk[:, dim],
                    step_id=data_step,
                    width=2
                )
                
                # Âà†Èô§‰∏ä‰∏Ä‰∏™chunkÁöÑÂèØËßÜÂåñ
                if last_data_step != data_step and last_data_step > 0:
                    vizer.del_chunk(
                        name=f"chunk/action_dim_{dim}/pred_seg_{last_data_step}",
                        chunk_data=pred_chunk[:, dim],
                        step_id=last_data_step,
                        width=0.5
                    )
        
        last_data_step = data_step
        
        # ========== Âú®mujocoÈáåÊâßË°åÂä®‰Ωú (Â¶ÇÊûúÂêØÁî®) =========
        if visualize_in_mujoco and mujoco_env is not None:
            action_np = pred_action_single[np.newaxis, :]  # (1, action_dim)
            
            # Ê†πÊçÆactionÁª¥Â∫¶ÈÄâÊã©ÊâßË°åÊñπÊ≥ï
            if action_dim == 16:
                # depalletize‰ªªÂä°Ôºö16Áª¥Âä®‰Ωú (14 arm joints + 2 claw positions)
                mujoco_env.exec_actions(
                    actions=action_np,
                    control_arm=True,
                    control_claw=True
                )
            else:
                # comÊéßÂà∂‰ªªÂä°Ôºö‰ΩøÁî®ÁªùÂØπÂä®‰ΩúÊâßË°åÔºàGrootPolicyÈªòËÆ§‰ΩøÁî®ÁªùÂØπÂä®‰ΩúÔºâ
                mujoco_env.exec_absolute_actions(
                    actions=action_np,
                    control_arm=True,
                    control_base=True,
                    control_wrench=False
                )

    # ========= ÊâìÂç∞ÊúÄÁªàÁªüËÆ°ÁªìÊûú =========
    print("\n" + "="*80)
    print("üìä Final Statistics")
    print("="*80)
    
    # ActionÂêçÁß∞ÂÆö‰πâ - Ê†πÊçÆactionÁª¥Â∫¶Ëá™Âä®ÈÄâÊã©
    if action_dim == 16:
        # depalletize‰ªªÂä°Ôºö16Áª¥Âä®‰Ωú (14 arm joints + 2 claw positions)
        # ‰ΩøÁî®config‰∏≠ÁöÑaction_namesÔºàÂ¶ÇÊûúÂèØÁî®Ôºâ
        if CONFIG_AVAILABLE and action_names and len(action_names) >= 16:
            eval_action_names = action_names
        else:
            eval_action_names = [f"Arm_joint_{i}" for i in range(14)] + ["Claw_left", "Claw_right"]
    else:
        # comÊéßÂà∂‰ªªÂä°Ôºö‰ΩøÁî®ÈªòËÆ§ÁöÑactionÂêçÁß∞
        eval_action_names = (
            ["COM_dx", "COM_dy", "COM_dz", "COM_dR11", "COM_dR21", "COM_dR31", "COM_dR12", "COM_dR22", "COM_dR32"] +
            [f"Arm_joint_{i}" for i in range(14)] +
            ["Gait_mode"]
        )
    
    print(f"\n{'Dimension':<20} {'MSE':<15} {'MAE':<15}")
    print("-" * 80)
    
    for dim in range(action_dim):
        mse_mean = np.mean(mse_per_action_dim[dim])
        mae_mean = np.mean(mae_per_action_dim[dim])
        dim_name = eval_action_names[dim] if dim < len(eval_action_names) else f"Dim_{dim}"
        print(f'{dim_name:<20} {mse_mean:<15.8f} {mae_mean:<15.8f}')
    
    overall_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(action_dim)])
    overall_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(action_dim)])
    
    print("-" * 80)
    print(f'{"Overall":<20} {overall_mse:<15.8f} {overall_mae:<15.8f}')
    
    # ÂàÜÁªÑÁªüËÆ° - Ê†πÊçÆactionÁª¥Â∫¶ÈÄâÊã©ÁªüËÆ°ÊñπÂºè
    print("\nüìä Grouped Statistics:")
    print("-" * 80)
    
    if action_dim == 16:
        # depalletize‰ªªÂä°Ôºö16Áª¥ = 14 arm joints + 2 claw positions
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        claw_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14, 16)])
        claw_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14, 16)])
        print(f'{"Claw (avg)":<20} {claw_mse:<15.8f} {claw_mae:<15.8f}')
    else:
        # comÊéßÂà∂‰ªªÂä°ÔºöÊ†áÂáÜÂàÜÁªÑÁªüËÆ°
        com_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9)])
        com_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9)])
        print(f'{"COM (avg)":<20} {com_mse:<15.8f} {com_mae:<15.8f}')
        
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9, 23)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9, 23)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        if action_dim > 23:
            gait_mse = np.mean(mse_per_action_dim[23])
            gait_mae = np.mean(mae_per_action_dim[23])
            print(f'{"Gait":<20} {gait_mse:<15.8f} {gait_mae:<15.8f}')
    
    print("="*80)

    if vizer is not None:
        print("\n[Offline Eval] Visualization active. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\n‚úÖ Exiting...")
    else:
        print("\n‚úÖ Evaluation completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Evaluate GrootPolicy Model on Dataset',
        epilog='Evaluates a trained GrootPolicy model on a LeRobot dataset.'
    )
    parser.add_argument('--ckpt-path', type=str, required=True,
                       help='Path to the model checkpoint directory')
    parser.add_argument('--dataset-root', '--dataset_root', type=str, required=True,
                       dest='dataset_root',
                       help='Path to the LeRobot dataset root directory')
    parser.add_argument('--episode', type=int, default=0,
                       help='Episode number to evaluate (default: 0)')
    parser.add_argument('--action-chunk-size', type=int, default=50,
                       help='Action chunk size (default: 50, should match training config)')
    parser.add_argument('--with-mujoco', action='store_true',
                       help='Visualize and execute in MuJoCo environment')
    parser.add_argument('--no-progress', action='store_true',
                       help='Disable progress bar')
    parser.add_argument('--image-zero', action='store_true',
                       help='Set all image inputs to zero (for testing model dependency on images)')
    parser.add_argument('--state-zero', action='store_true',
                       help='Set all state inputs to zero (for testing model dependency on state)')

    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("üéØ GrootPolicy Dataset Evaluation")
    print("="*80)
    print(f"Checkpoint: {args.ckpt_path}")
    print(f"Dataset: {args.dataset_root}")
    print(f"Episode: {args.episode}")
    print(f"Action Chunk Size: {args.action_chunk_size}")
    print(f"MuJoCo Visualization: {args.with_mujoco}")
    print(f"Image Zero Mode: {args.image_zero}")
    print(f"State Zero Mode: {args.state_zero}")
    print("="*80)
    
    eval_on_dataset(
        ckpt_path=args.ckpt_path,
        lerobot_dataset_path=args.dataset_root,
        episode=args.episode,
        n_actions=args.action_chunk_size,
        visualize_in_mujoco=args.with_mujoco,
        show_progress=not args.no_progress,
        image_zero=args.image_zero,
        state_zero=args.state_zero
    )
