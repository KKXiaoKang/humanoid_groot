#!/usr/bin/env python3
"""
Evaluate GrootPolicy Model on Dataset with Lowpass Visualization

This script evaluates a GrootPolicy model on a LeRobot dataset and computes error metrics.
It visualizes chunk interpolation and lowpass filtering between chunks.
It supports optional MuJoCo visualization.

Usage:
    python scripts/eval_on_dataset_losspass.py \
        --ckpt-path <checkpoint_path> \
        --dataset-root <dataset_path> \
        --episode <episode_number> \
        [--image-zero]  # Optional: set all images to zero to test model dependency on images
        [--state-zero]  # Optional: set all state inputs to zero to test model dependency on state
        [--cam-head-zero]  # Optional: set cam_head (image) to zero to test model dependency on cam_head
"""

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import numpy as np
from pathlib import Path
import argparse
import time
from collections import OrderedDict
from tqdm import tqdm

# ä½¿ç”¨GrootPolicyæ¨¡å‹
from lerobot.policies.groot.modeling_groot import GrootPolicy
from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors
from lerobot.datasets.lerobot_dataset import LeRobotDataset, MultiLeRobotDataset
from lerobot.datasets.compute_stats import aggregate_stats

# å¯¼å…¥é…ç½®æ¨¡å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from configs.config import topic_info, TASK_DATA_MODE, get_camera_observation_key, get_camera_names, CAMERA_COMPONENTS, action_names, CAMERA_KEY_MAPPING
    CONFIG_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: configs.config not available. Using defaults.")
    CONFIG_AVAILABLE = False
    topic_info = {}
    TASK_DATA_MODE = "unknown"
    CAMERA_COMPONENTS = []
    action_names = []
    CAMERA_KEY_MAPPING = {}
    def get_camera_observation_key(camera_name, use_image_features=False):
        return f"observation.images.{camera_name}" if use_image_features else f"observation.images.{camera_name}"
    def get_camera_names(camera_components=None):
        return []

# æ’å€¼ä¸ä½é€šæ»¤æ³¢å¸¸é‡
MODEL_ACTION_DT = 0.1
MODEL_ACTION_FREQUENCY = 1.0 / MODEL_ACTION_DT
TARGET_CONTROL_FREQUENCY = 100.0
TARGET_CONTROL_DT = 1.0 / TARGET_CONTROL_FREQUENCY
CHUNK_TRANSITION_DURATION_S = 0.2
LOWPASS_ALPHA = 0.85
COLOR_INTERP = [0, 128, 255]
COLOR_LOWPASS = [255, 140, 0]

# å¯é€‰çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™ç¦ç”¨ï¼‰
try:
    from visualization_tools.visualizers import RerunVisualizer, KeyboardManager
    RERUN_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: RerunVisualizer not available. Visualization will be disabled.")
    RERUN_AVAILABLE = False
    RerunVisualizer = None
    KeyboardManager = None

# ROS å’Œæœºå™¨äºº SDKï¼ˆä»…åœ¨éœ€è¦æ—¶å¯¼å…¥ï¼Œç”¨äº MuJoCo å¯è§†åŒ–ï¼‰
try:
    import rospy
    from kuavo_humanoid_sdk.kuavo_strategy_pytree.common.robot_sdk import RobotSDK
    from kuavo_humanoid_sdk.msg.kuavo_msgs.srv import (changeArmCtrlMode, changeArmCtrlModeRequest)
    ROS_AVAILABLE = True
except ImportError:
    ROS_AVAILABLE = False
    print("âš ï¸  Warning: ROS dependencies not available. MuJoCo visualization with robot control will be disabled.")

def direct_to_wbc(control_mode):
    """
    åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
    Args:
        control_mode: æ§åˆ¶æ¨¡å¼
            0: ç¦ç”¨wbcæ§åˆ¶è½¨è¿¹æ¨¡å¼
            1: wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼
    """
    if not ROS_AVAILABLE:
        print("âš ï¸  Warning: ROS not available, cannot call direct_to_wbc")
        return
    
    rospy.wait_for_service('/enable_wbc_arm_trajectory_control', timeout=5)
    try:
        change_mode = rospy.ServiceProxy('/enable_wbc_arm_trajectory_control', changeArmCtrlMode)
        req = changeArmCtrlModeRequest()
        req.control_mode = control_mode
        res = change_mode(req)
        if res.result:
            rospy.loginfo("wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼å·²æ›´æ”¹ä¸º %d", control_mode)
        else:
            rospy.logerr("æ— æ³•å°†wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æ›´æ”¹ä¸º %d", control_mode)
    except rospy.ServiceException as e:
        rospy.logerr("æœåŠ¡è°ƒç”¨å¤±è´¥: %s", e)


def resample_action_chunk(action_chunk: np.ndarray,
                          source_dt: float = MODEL_ACTION_DT,
                          target_dt: float = TARGET_CONTROL_DT) -> np.ndarray:
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if action_chunk.shape[0] <= 1 or np.isclose(source_dt, target_dt):
        return action_chunk

    total_duration = source_dt * (action_chunk.shape[0] - 1)
    if total_duration <= 0:
        repeat_factor = max(int(round(source_dt / target_dt)), 1)
        return np.repeat(action_chunk, repeats=repeat_factor, axis=0)

    num_target_steps = int(round(total_duration / target_dt)) + 1
    source_times = np.linspace(0.0, total_duration, num=action_chunk.shape[0])
    target_times = np.linspace(0.0, total_duration, num=num_target_steps)

    interpolated = np.empty((num_target_steps, action_chunk.shape[1]), dtype=action_chunk.dtype)
    for dim in range(action_chunk.shape[1]):
        interpolated[:, dim] = np.interp(target_times, source_times, action_chunk[:, dim])

    return interpolated


def apply_lowpass_transition(actions: np.ndarray,
                             previous_action: np.ndarray | None,
                             alpha: float = LOWPASS_ALPHA,
                             transition_steps: int | None = None,
                             smooth_slice: slice | tuple | np.ndarray = slice(None)) -> np.ndarray:
    if previous_action is None:
        return actions

    actions = np.asarray(actions)
    if actions.ndim == 1:
        actions = actions.reshape(1, -1)

    smoothed = actions.copy()
    prev = np.asarray(previous_action, dtype=smoothed.dtype)
    if prev.ndim == 1:
        prev = prev.reshape(1, -1)
    prev = prev[0]

    num_steps = smoothed.shape[0]
    if transition_steps is None or transition_steps > num_steps:
        transition_steps = num_steps
    transition_steps = max(1, transition_steps)

    indices = smooth_slice
    for idx in range(transition_steps):
        prev_slice = prev[indices]
        smoothed_slice = smoothed[idx][indices]
        filtered = alpha * prev_slice + (1.0 - alpha) * smoothed_slice
        prev[indices] = filtered
        smoothed[idx][indices] = filtered

    return smoothed


def resample_chunk_with_claw_hold(action_chunk: np.ndarray,
                                  previous_action: np.ndarray | None,
                                  control_frequency: float,
                                  source_dt: float = MODEL_ACTION_DT,
                                  arm_dims: slice = slice(0, 14),
                                  claw_dims: slice = slice(14, 16)) -> np.ndarray:
    action_chunk = np.asarray(action_chunk)
    if action_chunk.ndim == 1:
        action_chunk = action_chunk.reshape(1, -1)

    if previous_action is not None:
        chunk_with_bridge = np.vstack([previous_action, action_chunk])
        resampled = resample_action_chunk(
            chunk_with_bridge,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )[1:]
        source_array = chunk_with_bridge
    else:
        resampled = resample_action_chunk(
            action_chunk,
            source_dt=source_dt,
            target_dt=1.0 / control_frequency
        )
        source_array = action_chunk

    if source_array.shape[0] > 0 and resampled.shape[0] > 0:
        total_duration = source_dt * max(source_array.shape[0] - 1, 1)
        if total_duration <= 0:
            hold_indices = np.zeros(resampled.shape[0], dtype=int)
        else:
            target_times = np.linspace(0.0, total_duration, num=resampled.shape[0], endpoint=True)
            source_times = np.linspace(0.0, total_duration, num=source_array.shape[0], endpoint=True)
            hold_indices = np.searchsorted(source_times, target_times, side="right") - 1
            hold_indices = np.clip(hold_indices, 0, source_array.shape[0] - 1)
        resampled[:, claw_dims] = source_array[hold_indices][:, claw_dims]

    return resampled


def eval_on_dataset(ckpt_path,
                    lerobot_dataset_path,
                    episode,
                    visualize_in_mujoco=False,
                    n_actions=16,
                    show_progress=True,
                    image_zero=False,
                    state_zero=False,
                    cam_head_zero=False,
                    infer_per_frame: int = 1,
                    task_description: str | None = None,
                    training_dataset_paths: list[str] | None = None):
    """
    åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
    
    Args:
        ckpt_path: æ¨¡å‹checkpointè·¯å¾„
        lerobot_dataset_path: æ•°æ®é›†æ ¹ç›®å½•
        episode: episodeç¼–å·
        visualize_in_mujoco: æ˜¯å¦åœ¨MuJoCoä¸­å¯è§†åŒ–æ‰§è¡Œ
        n_actions: action chunkå¤§å°
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        image_zero: æ˜¯å¦å°†å›¾åƒè¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹å›¾åƒçš„ä¾èµ–æ€§ï¼‰
        state_zero: æ˜¯å¦å°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
        infer_per_frame: æ¯éš”å¤šå°‘ä¸ªframeé‡æ–°æ¨ç†ä¸€æ¬¡ï¼ˆ>=1ï¼‰ã€‚
        task_description: ä»»åŠ¡æè¿°å­—ç¬¦ä¸²ï¼ˆlanguage instructionï¼‰ï¼Œå¦‚æœæä¾›åˆ™è¦†ç›–æ•°æ®é›†ä¸­çš„taskï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®é›†åŸæœ¬çš„taskã€‚
        training_dataset_paths: ç”¨äºè®¡ç®—ç»Ÿè®¡ä¿¡æ¯çš„è®­ç»ƒæ•°æ®é›†è·¯å¾„åˆ—è¡¨ã€‚å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨è¿™äº›æ•°æ®é›†è®¡ç®—åˆå¹¶çš„ç»Ÿè®¡ä¿¡æ¯ç”¨äºåå½’ä¸€åŒ–ã€‚
    """
    # ----------- ä¸€äº›å‚æ•° ----------------
    mse_per_action_dim = OrderedDict() # è®°å½•æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„MSE
    mae_per_action_dim = OrderedDict() # è®°å½•æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„MAE
    infer_per_frame = max(1, infer_per_frame)  # è‡³å°‘æ¯å¸§æ¨ç†ä¸€æ¬¡
    
    # è¾…åŠ©å‡½æ•°ï¼šåå½’ä¸€åŒ–é¢„æµ‹åŠ¨ä½œ
    def denormalize_actions(pred_actions, action_dim, dataset_stats):
        """åå½’ä¸€åŒ–é¢„æµ‹åŠ¨ä½œ"""
        if dataset_stats and 'action' in dataset_stats:
            action_stats = dataset_stats['action']
            if 'min' in action_stats and 'max' in action_stats:
                action_min = torch.as_tensor(action_stats['min'], dtype=torch.float32, device=pred_actions.device)
                action_max = torch.as_tensor(action_stats['max'], dtype=torch.float32, device=pred_actions.device)
                
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if action_min.numel() < action_dim:
                    action_min = torch.nn.functional.pad(action_min.flatten()[:action_dim], (0, max(0, action_dim - action_min.numel())))
                if action_max.numel() < action_dim:
                    action_max = torch.nn.functional.pad(action_max.flatten()[:action_dim], (0, max(0, action_dim - action_max.numel())))
                
                action_min = action_min[:action_dim]
                action_max = action_max[:action_dim]
                
                # åå½’ä¸€åŒ–å…¬å¼ï¼šx = (y + 1) / 2 * (max - min) + min
                denom = action_max - action_min
                mask = denom != 0
                safe_denom = torch.where(mask, denom, torch.ones_like(denom))
                
                pred_actions_unnorm = (pred_actions + 1.0) * 0.5 * safe_denom + action_min
                pred_actions_unnorm = torch.where(mask, pred_actions_unnorm, action_min)
                
                pred_action_single = pred_actions_unnorm[0, -1, :].cpu().numpy()
                pred_chunk = pred_actions_unnorm[0].cpu().numpy()
                return pred_action_single, pred_chunk
            else:
                pred_action_single = pred_actions[0, -1, :].cpu().numpy()
                pred_chunk = pred_actions[0].cpu().numpy()
                return pred_action_single, pred_chunk
        else:
            pred_action_single = pred_actions[0, -1, :].cpu().numpy()
            pred_chunk = pred_actions[0].cpu().numpy()
            return pred_action_single, pred_chunk

    # ------------- åˆå§‹åŒ–visualizer (å¯é€‰) -------------
    if RERUN_AVAILABLE:
        vizer = RerunVisualizer()
        kb = KeyboardManager()
        print("âœ… RerunVisualizer initialized")
    else:
        vizer = None
        kb = None
        print("âš ï¸  Running without RerunVisualizer")

    # ------------- åˆå§‹åŒ–æ•°æ®é›†å’Œæ¨¡å‹ -------------
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    print(f"\n{'='*80}")
    print(f"ğŸ”§ Device: {device}")
    
    # âœ… ä½¿ç”¨GrootPolicyåŠ è½½æ¨¡å‹
    print(f"ğŸ“‚ Loading GrootPolicy model from {ckpt_path}...")
    policy = GrootPolicy.from_pretrained(Path(ckpt_path), strict=False)
    policy.config.device = device
    policy.config.n_action_steps = n_actions
    
    print(f"ğŸ“Š Action chunk size: {n_actions}")
    print(f"ğŸ”„ Inference frequency: Every {infer_per_frame} frame(s) (infer_per_frame={infer_per_frame})")
    if task_description is not None:
        print(f"ğŸ“ Task description (overridden): '{task_description}'")
    else:
        print(f"ğŸ“ Task description: Will use task from dataset")
    if image_zero:
        print(f"âš ï¸  IMAGE ZERO MODE: All image inputs will be set to zero (for dependency testing)")
    if state_zero:
        print(f"âš ï¸  STATE ZERO MODE: All state inputs will be set to zero (for dependency testing)")
    if cam_head_zero:
        print(f"âš ï¸  CAM_HEAD ZERO MODE: cam_head (image) will be set to zero (for dependency testing)")
    
    policy.eval().to(device)
    
    # Load dataset statistics for normalization
    print(f"\nğŸ“‚ Loading dataset for statistics...")
    if training_dataset_paths is not None and len(training_dataset_paths) > 0:
        # ä½¿ç”¨å¤šä¸ªè®­ç»ƒæ•°æ®é›†è®¡ç®—åˆå¹¶çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š Loading {len(training_dataset_paths)} training datasets for aggregated statistics:")
        for i, path in enumerate(training_dataset_paths):
            print(f"   {i+1}. {path}")
        
        # ä»å®Œæ•´è·¯å¾„åŠ è½½æ•°æ®é›†
        # å¦‚æœè·¯å¾„æ˜¯å®Œæ•´çš„æ•°æ®é›†æ ¹ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨è·¯å¾„ä½œä¸ºrootï¼Œrepo_idå¯ä»¥æ˜¯0æˆ–è·¯å¾„å
        training_datasets = []
        for path in training_dataset_paths:
            path_obj = Path(path)
            # ä½¿ç”¨è·¯å¾„åä½œä¸ºrepo_idï¼Œå®Œæ•´è·¯å¾„ä½œä¸ºroot
            # LeRobotDatasetä¼šç›´æ¥ä½¿ç”¨rootï¼Œä¸ä¼šä¸repo_idæ‹¼æ¥
            repo_id = path_obj.name  # è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºrepo_idï¼ˆç”¨äºæ ‡è¯†ï¼‰
            root = path_obj          # å®Œæ•´è·¯å¾„ä½œä¸ºroot
            print(f"   Loading dataset: repo_id='{repo_id}', root='{root}'")
            dataset = LeRobotDataset(repo_id=repo_id, root=root)
            training_datasets.append(dataset)
        
        # èšåˆç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š Aggregating statistics from {len(training_datasets)} datasets...")
        stats_list = [ds.meta.stats for ds in training_datasets if ds.meta.stats is not None]
        if len(stats_list) > 0:
            dataset_stats = aggregate_stats(stats_list)
            print(f"âœ… Aggregated statistics loaded: {list(dataset_stats.keys())}")
        else:
            print("âš ï¸  Warning: No statistics found in training datasets")
            dataset_stats = None
    else:
        # ä½¿ç”¨å•ä¸ªæ•°æ®é›†ï¼ˆè¯„ä¼°æ•°æ®é›†æœ¬èº«ï¼‰çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š Using statistics from evaluation dataset: {lerobot_dataset_path}")
        dataset_for_stats = LeRobotDataset(repo_id=0, root=lerobot_dataset_path)
        dataset_stats = dataset_for_stats.meta.stats if hasattr(dataset_for_stats.meta, 'stats') else None
        print(f"âœ… Dataset statistics loaded: {list(dataset_stats.keys()) if dataset_stats else 'None'}")
    
    # Create preprocessor and postprocessor
    print(f"\nğŸ”§ Creating preprocessor and postprocessor...")
    preprocessor, postprocessor = make_groot_pre_post_processors(
        config=policy.config,
        dataset_stats=dataset_stats,
    )
    print("âœ… Preprocessor and postprocessor created")
    
    # Debug: Print model configuration
    print(f"ğŸ” Model configuration input_features keys: {list(policy.config.input_features.keys()) if hasattr(policy.config, 'input_features') else 'N/A'}")
    print(f"ğŸ” Model configuration output_features keys: {list(policy.config.output_features.keys()) if hasattr(policy.config, 'output_features') else 'N/A'}")
    
    policy.reset()
    print("âœ… Model loaded and ready")
    
    previous_resampled_action = None
    last_inferred_chunk: np.ndarray | None = None
    last_resampled_chunk: np.ndarray | None = None
    last_lowpass_chunk: np.ndarray | None = None
    last_inference_step = -1
    infer_per_frame = max(1, infer_per_frame)

    control_step_cursor = 0
    
    # âœ… ä½¿ç”¨æ ‡å‡†çš„LeRobotDatasetåŠ è½½æ•°æ®
    print(f"\nğŸ“‚ Loading dataset from {lerobot_dataset_path}")
    print(f"ğŸ“¹ Episode: {episode}")
    
    # æ³¨æ„ï¼šLeRobotDatasetçš„episodeså‚æ•°ä¸»è¦ç”¨äºä¸‹è½½æ—¶é€‰æ‹©æ–‡ä»¶
    # ä½†åœ¨åŠ è½½åéœ€è¦æ‰‹åŠ¨è¿‡æ»¤æ•°æ®ï¼Œå› ä¸ºå¤šä¸ªepisodeså¯èƒ½å­˜å‚¨åœ¨åŒä¸€ä¸ªparquetæ–‡ä»¶ä¸­
    dataset = LeRobotDataset(repo_id=0, root=lerobot_dataset_path, episodes=[episode])
    
    # ä½¿ç”¨episodeçš„ç´¢å¼•èŒƒå›´ç›´æ¥åˆ‡ç‰‡ï¼Œæ¯”filterå¿«å¾—å¤š
    # è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºv3.0æ ¼å¼ä¸­å¤šä¸ªepisodeså¯èƒ½å­˜å‚¨åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­
    print(f"ğŸ” Filtering dataset to episode {episode}...")
    if episode >= len(dataset.meta.episodes):
        raise ValueError(f"Episode {episode} out of range. Available episodes: 0-{len(dataset.meta.episodes)-1}")
    
    ep_meta = dataset.meta.episodes[episode]
    ep_start = ep_meta["dataset_from_index"]
    ep_end = ep_meta["dataset_to_index"]
    
    # ä½¿ç”¨åˆ‡ç‰‡è€Œä¸æ˜¯filterï¼Œè¿™æ ·å¿«å¾—å¤š
    dataset.hf_dataset = dataset.hf_dataset.select(range(ep_start, ep_end))
    print(f"âœ… Filtered dataset. Total frames in episode {episode}: {len(dataset.hf_dataset)} (indices {ep_start}-{ep_end-1})")
    
    # æ‰“å°ç›¸æœºé…ç½®ä¿¡æ¯
    if CONFIG_AVAILABLE:
        # ä½¿ç”¨CAMERA_COMPONENTSæ¥è·å–ç›¸æœºåç§°
        camera_names = get_camera_names(CAMERA_COMPONENTS)
        camera_config = {name: info for name, info in topic_info.items() if 'image' in name}
        print(f"\nğŸ“· Camera Configuration (TASK_DATA_MODE: {TASK_DATA_MODE}):")
        print(f"   CAMERA_COMPONENTS: {CAMERA_COMPONENTS}")
        print(f"   Camera names: {camera_names}")
        print(f"   Detected {len(camera_config)} cameras in topic_info: {list(camera_config.keys())}")
    else:
        # ä»æ•°æ®é›†å…ƒæ•°æ®ä¸­æ£€æµ‹ç›¸æœº
        sample = dataset[0]
        image_keys = [k for k in sample.keys() if 'image' in k.lower()]
        print(f"\nğŸ“· Camera Configuration:")
        print(f"   Detected {len(image_keys)} image keys: {image_keys}")
    
    # åˆ›å»ºdataloader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    print(f"âœ… Dataset loaded. Total frames: {dataset.num_frames}")

    # è·å–actionç»´åº¦
    first_batch = next(iter(dataloader))
    action_dim = first_batch['action'].shape[1]
    obs_dim = first_batch['observation.state'].shape[1]
    print(f"ğŸ“Š Action dimension: {action_dim}")
    print(f"ğŸ“Š Observation dimension: {obs_dim}")
    
    # é‡æ–°åˆ›å»ºdataloaderï¼ˆå› ä¸ºå·²ç»æ¶ˆè€—äº†ç¬¬ä¸€ä¸ªbatchï¼‰
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=0,
        batch_size=1,
        shuffle=False,
        pin_memory=(device == "cuda:0"),
        drop_last=False,
    )
    
    # åˆå§‹åŒ–ç¯å¢ƒï¼ˆå¦‚æœéœ€è¦åœ¨mujocoä¸­å¯è§†åŒ–ï¼‰
    if visualize_in_mujoco:
        # é¦–å…ˆåˆå§‹åŒ–æœºå™¨äººæ§åˆ¶ï¼ˆå‚è€ƒ eval_depalletize_camera.pyï¼‰
        if ROS_AVAILABLE:
            print(f"\nğŸ¤– Initializing robot control...")
            try:
                # åˆå§‹åŒ– ROS èŠ‚ç‚¹ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
                try:
                    rospy.init_node('eval_on_dataset_robot_control', anonymous=True)
                except rospy.exceptions.ROSException:
                    # ROS èŠ‚ç‚¹å·²ç»åˆå§‹åŒ–ï¼Œç»§ç»­æ‰§è¡Œ
                    pass
                
                # åˆå§‹åŒ–æœºå™¨äºº SDK å¹¶è®¾ç½®å¤´éƒ¨å’Œæ§åˆ¶æ¨¡å¼
                robot_sdk = RobotSDK()
                robot_sdk.control.control_head(0, np.deg2rad(10))
                robot_sdk.control.set_external_control_arm_mode()  # åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼
                print(f"âœ… Robot SDK initialized")
                print(f"   - æœºå™¨äººå¤´éƒ¨ä¿¯ä»°è°ƒèŠ‚è§’åº¦: 10 æˆåŠŸ")
                print(f"   - åˆ‡æ¢æ‰‹è‡‚åˆ°å¤–éƒ¨æ§åˆ¶æ¨¡å¼æˆåŠŸ")
                
                # åˆ‡æ¢åˆ° WBC è½¨è¿¹æ§åˆ¶æ¨¡å¼
                direct_to_wbc(1)
                input(f"direct_to_wbc ç»“æŸ, æŒ‰å›è½¦ç»§ç»­ ==== åˆ‡æ¢æ‰‹è‡‚åˆ°wbcè½¨è¿¹æ§åˆ¶æ¨¡å¼æˆåŠŸ ==== \n")
                time.sleep(1.0)
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to initialize robot control: {e}")
                print(f"   Continuing with MuJoCo environment initialization...")
        else:
            print(f"âš ï¸  Warning: ROS not available, skipping robot control initialization")
        
        print(f"\nğŸ¤– Initializing MuJoCo environment...")
        # æ ¹æ®actionç»´åº¦åˆ¤æ–­ä½¿ç”¨å“ªä¸ªç¯å¢ƒ
        # 16ç»´åŠ¨ä½œ = depalletizeä»»åŠ¡ï¼Œä½¿ç”¨kuavo_depalletize_env
        # å…¶ä»–ç»´åº¦ = comæ§åˆ¶ä»»åŠ¡ï¼Œä½¿ç”¨kuavo_com_env
        if action_dim == 16:
            from robot_envs.kuavo_depalletize_env import GrabBoxMpcEnv
            mujoco_env = GrabBoxMpcEnv()
            print(f"âœ… MuJoCo environment initialized (depalletize task)")
            print(f"   - Action dimension: 16 (14 arm joints + 2 claw positions)")
        else:
            try:
                from robot_envs.kuavo_com_env import GrabBoxMpcEnv
                # GrootPolicy uses absolute actions by default
                mujoco_env = GrabBoxMpcEnv(use_action_history_reference=False)
                print(f"âœ… MuJoCo environment initialized (com control task)")
                print(f"   - use_action_history_reference: False (absolute actions)")
            except ImportError:
                print("âš ï¸  Warning: robot_envs.kuavo_com_env not available. MuJoCo visualization disabled.")
                visualize_in_mujoco = False
                mujoco_env = None
    
    # ========= å¯è§†åŒ–æ•°æ®é›†é‡Œçš„groundtruth (å¦‚æœå¯ç”¨RerunVisualizer) =========
    if vizer is not None:
        print(f"\nğŸ“Š Visualizing ground truth data...")
        # åŠ è½½æ‰€æœ‰ground truth actionsç”¨äºå¯è§†åŒ–
        all_gt_actions = []
        all_gt_states = []
        
        temp_dataloader = torch.utils.data.DataLoader(
            dataset, num_workers=0, batch_size=1, shuffle=False, drop_last=False
        )
        
        for batch in temp_dataloader:
            all_gt_actions.append(batch['action'][0].cpu().numpy())
            all_gt_states.append(batch['observation.state'][0].cpu().numpy())
        
        all_gt_actions = np.array(all_gt_actions)
        all_gt_states = np.array(all_gt_states)
        
        # å¯è§†åŒ–ground truth actions
        for dim in range(action_dim):
            vizer.visualize_chunk(
                name=f"chunk/action_dim_{dim}/gt",
                chunk_data=all_gt_actions[:, dim],
                step_id=0,
                width=3.0
            )
        
        # å¯è§†åŒ–observations
        for dim in range(obs_dim):
            vizer.visualize_chunk(
                name=f"obs/obs_{dim}",
                chunk_data=all_gt_states[:, dim],
                step_id=0,
                width=3.0
            )
        
        print(f"âœ… Ground truth visualization ready")

    # ========= å¼€å§‹æ¨¡å‹æ¨ç† =========
    print("\n" + "="*80)
    print("ğŸš€ Starting inference...")
    print("="*80 + "\n")
    
    last_data_step = 0
    predictions = []
    ground_truths = []
    inference_times = []  # è®°å½•æ¯æ¬¡æ¨ç†çš„æ—¶é—´
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    iterator = tqdm(enumerate(dataloader), total=dataset.num_frames, desc="Processing") if show_progress else enumerate(dataloader)
    
    for data_step, batch in iterator:
        # æš‚åœæ§åˆ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vizer is not None and kb is not None:
            time.sleep(0.05)
            if kb.paused:
                print(f'===== æš‚åœä¸­ï¼ŒæŒ‰ä¸‹ç©ºæ ¼å¼€å§‹ =====')
            while kb.paused:
                time.sleep(0.1)
        
        # âœ… å‡†å¤‡observation - ä½¿ç”¨Grooté¢„å¤„ç†å™¨
        # é¦–å…ˆæ„å»ºåŸå§‹observationå­—å…¸
        observation = {
            'observation.state': batch['observation.state'],
        }
        
        # æ·»åŠ å›¾åƒè§‚æµ‹ï¼ˆæ ¹æ®CAMERA_COMPONENTSé…ç½®ï¼‰
        if CONFIG_AVAILABLE:
            # ä½¿ç”¨CAMERA_COMPONENTSæ¥æ˜ç¡®æŒ‡å®šéœ€è¦å“ªäº›ç›¸æœº
            camera_names = get_camera_names(CAMERA_COMPONENTS)
            for camera_name in camera_names:
                # ä½¿ç”¨get_camera_observation_keyè·å–æ­£ç¡®çš„è§‚æµ‹é”®å
                obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                if obs_key in batch:
                    observation[obs_key] = batch[obs_key]
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ç›¸æœºåç§°ä½œä¸ºé”®ï¼ˆå‘åå…¼å®¹ï¼‰
                    fallback_key = f"observation.images.{camera_name}"
                    if fallback_key in batch:
                        observation[fallback_key] = batch[fallback_key]
                    elif data_step == 0:
                        print(f"âš ï¸  Warning: Camera observation key '{obs_key}' not found in batch. Available keys: {[k for k in batch.keys() if 'image' in k.lower()]}")
        else:
            # å¦‚æœæ²¡æœ‰configï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•ï¼šæ·»åŠ æ‰€æœ‰å›¾åƒè§‚æµ‹
            for key in batch.keys():
                if 'image' in key.lower() and key.startswith('observation'):
                    observation[key] = batch[key]
        
        # å¦‚æœå¯ç”¨state_zeroæ¨¡å¼ï¼Œå°†çŠ¶æ€è¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹çŠ¶æ€çš„ä¾èµ–æ€§ï¼‰
        if state_zero:
            # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰çŠ¶æ€å€¼è®¾ä¸º0
            observation['observation.state'] = torch.zeros_like(observation['observation.state'])
        
        # å¦‚æœå¯ç”¨image_zeroæ¨¡å¼ï¼Œå°†æ‰€æœ‰å›¾åƒè¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹å›¾åƒçš„ä¾èµ–æ€§ï¼‰
        if image_zero:
            for key in observation.keys():
                if 'image' in key:
                    # ä¿æŒç›¸åŒçš„å½¢çŠ¶å’Œè®¾å¤‡ï¼Œä½†å°†æ‰€æœ‰åƒç´ å€¼è®¾ä¸º0
                    observation[key] = torch.zeros_like(observation[key])
        
        # å¦‚æœå¯ç”¨cam_head_zeroæ¨¡å¼ï¼Œå°†cam_headï¼ˆimageï¼‰çš„å›¾åƒè¾“å…¥ç½®é›¶ï¼ˆç”¨äºéªŒè¯æ¨¡å‹å¯¹cam_headçš„ä¾èµ–æ€§ï¼‰
        if cam_head_zero:
            # cam_headå¯¹åº”çš„ç›¸æœºåç§°æ˜¯"image"ï¼Œè§‚æµ‹é”®æ˜¯"observation.images.cam_head"
            cam_head_obs_key = "observation.images.cam_head"
            if cam_head_obs_key in observation:
                observation[cam_head_obs_key] = torch.zeros_like(observation[cam_head_obs_key])
            else:
                # å‘åå…¼å®¹ï¼šå°è¯•ä½¿ç”¨"image"ä½œä¸ºé”®å
                fallback_key = "observation.images.image"
                if fallback_key in observation:
                    observation[fallback_key] = torch.zeros_like(observation[fallback_key])
                elif data_step == 0:
                    print(f"âš ï¸  Warning: cam_head observation key not found. Available keys: {[k for k in observation.keys() if 'image' in k.lower()]}")
        
        # æ·»åŠ  task å­—æ®µï¼ˆlanguage instructionï¼‰
        # å¦‚æœæä¾›äº† task_descriptionï¼Œåˆ™ä½¿ç”¨å®ƒè¦†ç›–æ•°æ®é›†ä¸­çš„ taskï¼›å¦åˆ™ä½¿ç”¨æ•°æ®é›†åŸæœ¬çš„ task
        if task_description is not None:
            observation['task'] = task_description
        elif 'task' in batch:
            # ä» batch ä¸­è·å– taskï¼ˆLeRobotDataset ä¼šåœ¨ __getitem__ ä¸­æ·»åŠ  task å­—æ®µï¼‰
            batch_task = batch['task']
            # å¤„ç† batch_task å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²çš„æƒ…å†µ
            if isinstance(batch_task, (list, tuple)) and len(batch_task) > 0:
                observation['task'] = batch_task[0]
            elif isinstance(batch_task, str):
                observation['task'] = batch_task
            else:
                # å¦‚æœç±»å‹ä¸åŒ¹é…ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                observation['task'] = str(batch_task) if batch_task is not None else ""
        else:
            # å¦‚æœ batch ä¸­æ²¡æœ‰ taskï¼Œå°è¯•ä»æ•°æ®é›†å…ƒæ•°æ®ä¸­è·å–ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡ä½œä¸ºé»˜è®¤å€¼ï¼‰
            if hasattr(dataset, 'meta') and hasattr(dataset.meta, 'tasks') and len(dataset.meta.tasks) > 0:
                observation['task'] = dataset.meta.tasks.index[0]
            else:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                observation['task'] = ""
        
        # è·å–ground truth action
        gt_action = batch['action'][0].cpu().numpy()  # (action_dim,)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œæ¨ç†ï¼ˆæ ¹æ®infer_per_frameå‚æ•°ï¼‰
        should_infer = (data_step % infer_per_frame == 0)
        
        # æ¨¡å‹æ¨ç†
        tic = time.time()
        if should_infer:
            # éœ€è¦æ¨ç†ï¼šæ‰§è¡Œå®Œæ•´çš„æ¨ç†æµç¨‹
            # ä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†è¾“å…¥
            processed_observation = preprocessor(observation)
            
            # ç²¾ç¡®æµ‹é‡ predict_action_chunk çš„æ¨ç†æ—¶é—´
            # ä½¿ç”¨ CUDA åŒæ­¥ç¡®ä¿å‡†ç¡®æµ‹é‡ GPU æ¨ç†æ—¶é—´
            if device.startswith('cuda'):
                torch.cuda.synchronize()
            inference_start = time.perf_counter()
            
            # æ¨¡å‹æ¨ç†
            with torch.inference_mode():
                pred_actions = policy.predict_action_chunk(processed_observation)
            
            # ç¡®ä¿ GPU æ“ä½œå®Œæˆåå†è®°å½•ç»“æŸæ—¶é—´
            if device.startswith('cuda'):
                torch.cuda.synchronize()
            inference_end = time.perf_counter()
            inference_time = inference_end - inference_start
            inference_times.append(inference_time)
            
            # æ‰“å°actionç»´åº¦
            print(f"pred_actions shape: {pred_actions.shape}")
            
            # pred_actions shape: (batch_size, chunk_size, action_dim)
            # æ³¨æ„ï¼špred_actionsæ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ŒèŒƒå›´åœ¨[-1, 1]
            # éœ€è¦æ‰‹åŠ¨åå½’ä¸€åŒ–åˆ°çœŸå®å•ä½
            
            # åå½’ä¸€åŒ–é¢„æµ‹åŠ¨ä½œ
            pred_action_single, pred_chunk = denormalize_actions(pred_actions, action_dim, dataset_stats)
            
            # ä¿å­˜é¢„æµ‹ç»“æœä¾›åç»­å¸§ä½¿ç”¨
            last_inferred_chunk = pred_chunk.copy()
            last_inference_step = data_step
        else:
            # ä¸éœ€è¦æ¨ç†ï¼šå¤ç”¨ä¸Šä¸€æ¬¡çš„é¢„æµ‹ç»“æœ
            if last_inferred_chunk is not None:
                pred_chunk = last_inferred_chunk.copy()
                pred_action_single = pred_chunk[0]  # å–ç¬¬ä¸€ä¸ªaction
            else:
                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€å¸§ä¸”infer_per_frame > 1ï¼Œéœ€è¦å…ˆæ¨ç†ä¸€æ¬¡
                if data_step == 0:
                    print(f"âš ï¸  Warning: First frame but no previous prediction. Performing inference anyway.")
                    # æ‰§è¡Œæ¨ç†
                    processed_observation = preprocessor(observation)
                    
                    # ç²¾ç¡®æµ‹é‡ predict_action_chunk çš„æ¨ç†æ—¶é—´
                    # ä½¿ç”¨ CUDA åŒæ­¥ç¡®ä¿å‡†ç¡®æµ‹é‡ GPU æ¨ç†æ—¶é—´
                    if device.startswith('cuda'):
                        torch.cuda.synchronize()
                    inference_start = time.perf_counter()
                    
                    with torch.inference_mode():
                        pred_actions = policy.predict_action_chunk(processed_observation)
                    
                    # ç¡®ä¿ GPU æ“ä½œå®Œæˆåå†è®°å½•ç»“æŸæ—¶é—´
                    if device.startswith('cuda'):
                        torch.cuda.synchronize()
                    inference_end = time.perf_counter()
                    inference_time = inference_end - inference_start
                    inference_times.append(inference_time)
                    
                    pred_action_single, pred_chunk = denormalize_actions(pred_actions, action_dim, dataset_stats)
                    last_inferred_chunk = pred_chunk.copy()
                    last_inference_step = data_step
                else:
                    # å¦‚æœè¿˜æ²¡æœ‰é¢„æµ‹ç»“æœï¼Œä½¿ç”¨é›¶å‘é‡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                    print(f"âš ï¸  Warning: No previous prediction available at frame {data_step}. Using zeros.")
                    pred_action_single = np.zeros(action_dim)
                    pred_chunk = np.zeros((n_actions, action_dim))
    
        transition_steps = None
        if previous_resampled_action is not None:
            transition_steps = max(1, int(round(TARGET_CONTROL_FREQUENCY * CHUNK_TRANSITION_DURATION_S)))
        if should_infer or last_resampled_chunk is None:
            resampled_chunk = resample_chunk_with_claw_hold(
                pred_chunk,
                previous_action=previous_resampled_action,
                control_frequency=TARGET_CONTROL_FREQUENCY,
                source_dt=MODEL_ACTION_DT
            )
            lowpass_chunk = apply_lowpass_transition(
                resampled_chunk,
                previous_action=previous_resampled_action,
                alpha=LOWPASS_ALPHA,
                transition_steps=transition_steps,
                smooth_slice=slice(0, 14)
            )
            last_resampled_chunk = resampled_chunk
            last_lowpass_chunk = lowpass_chunk
        else:
            resampled_chunk = last_resampled_chunk
            lowpass_chunk = last_lowpass_chunk

        if lowpass_chunk is not None and lowpass_chunk.size > 0:
            previous_resampled_action = lowpass_chunk[-1].copy()
    
        inference_time = time.time() - tic
        
        # ä¿å­˜é¢„æµ‹å’ŒçœŸå®å€¼
        predictions.append(pred_action_single)
        ground_truths.append(gt_action)
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„MSEå’ŒMAE
        for dim in range(action_dim):
            error = pred_action_single[dim] - gt_action[dim]
            mse = error ** 2
            mae = abs(error)
            
            if dim not in mse_per_action_dim:
                mse_per_action_dim[dim] = []
                mae_per_action_dim[dim] = []
            
            mse_per_action_dim[dim].append(mse)
            mae_per_action_dim[dim].append(mae)
        
        # å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if vizer is not None:
            # æ˜¾ç¤ºå›¾åƒ - åŠ¨æ€æŸ¥æ‰¾å¯ç”¨çš„ç›¸æœºå›¾åƒ
            if CONFIG_AVAILABLE:
                camera_names = get_camera_names(CAMERA_COMPONENTS)
                for camera_name in camera_names:
                    obs_key = get_camera_observation_key(camera_name, use_image_features=False)
                    fallback_key = f"observation.images.{camera_name}"
                    
                    # ä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ—§æ ¼å¼
                    key_to_use = obs_key if obs_key in batch else fallback_key
                    if key_to_use in batch:
                        img = batch[key_to_use][0]  # (C, H, W)
                        # ä»obs_keyä¸­æå–ç»„ä»¶åï¼ˆå¦‚ observation.images.cam_head -> cam_headï¼‰
                        # æˆ–è€…ä»camera_nameæ˜ å°„åˆ°ç»„ä»¶å
                        if obs_key in batch:
                            # ä½¿ç”¨æ–°æ ¼å¼ï¼šä» observation.images.cam_head æå– cam_head
                            display_name = obs_key.replace('observation.images.', '')
                        else:
                            # ä½¿ç”¨æ—§æ ¼å¼ï¼šä» camera_name æ˜ å°„åˆ°ç»„ä»¶å
                            display_name = CAMERA_KEY_MAPPING.get(camera_name, camera_name)
                        vizer.show_img(
                            name=f"images.{display_name}",
                            image_data=img.to("cpu"),
                            step_id=data_step
                        )
            else:
                # å¦‚æœæ²¡æœ‰configï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•
                for key in batch.keys():
                    if 'image' in key.lower() and key.startswith('observation'):
                        img = batch[key][0]  # (C, H, W)
                        camera_name = key.replace('observation.', '').replace('observation.images.', '')
                        vizer.show_img(
                            name=camera_name,
                            image_data=img.to("cpu"),
                            step_id=data_step
                        )
            
            # å¯è§†åŒ–é¢„æµ‹çš„chunk
            for dim in range(action_dim):
                # å¯è§†åŒ–MSE
                vizer.visualize_chunk(
                    name=f"mse/action_dim_{dim}",
                    chunk_data=mse_per_action_dim[dim][-1],
                    step_id=data_step,
                    width=3.0,
                )
                
                if should_infer:
                    vizer.visualize_chunk(
                        name=f"chunk/action_dim_{dim}/pred_seg_{data_step}",
                        chunk_data=pred_chunk[:, dim],
                        step_id=data_step,
                        width=2
                    )

                    if last_data_step != data_step and last_data_step > 0:
                        vizer.del_chunk(
                            name=f"chunk/action_dim_{dim}/pred_seg_{last_data_step}",
                            chunk_data=pred_chunk[:, dim],
                            step_id=last_data_step,
                            width=0.5
                        )
            
            if should_infer and resampled_chunk is not None and resampled_chunk.size > 0:
                resampled_steps_axis = np.arange(control_step_cursor, control_step_cursor + resampled_chunk.shape[0], dtype=int)
                start_step = resampled_steps_axis[0]
                end_step = resampled_steps_axis[-1]
                start_point = resampled_chunk[0]
                end_point = resampled_chunk[-1]
                raw_start_point = pred_chunk[0]
                for dim in range(action_dim):
                    if hasattr(vizer, "clear_path"):
                        vizer.clear_path(f"chunk_interp/action_dim_{dim}/start_point")
                        vizer.clear_path(f"chunk_interp/action_dim_{dim}/start_point_raw")
                        vizer.clear_path(f"chunk_interp/action_dim_{dim}/end_point")
                        vizer.clear_path(f"chunk_lowpass/action_dim_{dim}/start_point")
                        vizer.clear_path(f"chunk_lowpass/action_dim_{dim}/start_point_raw")
                        vizer.clear_path(f"chunk_lowpass/action_dim_{dim}/end_point")
                    vizer.visualize_chunk(
                        name=f"chunk_interp/action_dim_{dim}/pred_seg_{data_step}",
                        chunk_data=resampled_chunk[:, dim],
                        step_id=0,
                        x_axis=resampled_steps_axis,
                        width=1.5,
                        color=COLOR_INTERP
                    )
                    vizer.visualize_points(
                        name=f"chunk_interp/action_dim_{dim}/start_point",
                        xs=np.array([start_step]),
                        ys=np.array([start_point[dim]]),
                        colors=np.array([[0, 255, 0]])
                    )
                    vizer.visualize_points(
                        name=f"chunk_interp/action_dim_{dim}/start_point_raw",
                        xs=np.array([start_step]),
                        ys=np.array([raw_start_point[dim]]),
                        colors=np.array([[0, 128, 0]])
                    )
                    vizer.visualize_points(
                        name=f"chunk_interp/action_dim_{dim}/end_point",
                        xs=np.array([end_step]),
                        ys=np.array([end_point[dim]]),
                        colors=np.array([[255, 0, 0]])
                    )
                    vizer.visualize_chunk(
                        name=f"chunk_lowpass/action_dim_{dim}/pred_seg_{data_step}",
                        chunk_data=lowpass_chunk[:, dim],
                        step_id=0,
                        x_axis=resampled_steps_axis,
                        width=1.5,
                        color=COLOR_LOWPASS
                    )
                    vizer.visualize_points(
                        name=f"chunk_lowpass/action_dim_{dim}/start_point",
                        xs=np.array([start_step]),
                        ys=np.array([lowpass_chunk[0, dim]]),
                        colors=np.array([[0, 255, 0]])
                    )
                    vizer.visualize_points(
                        name=f"chunk_lowpass/action_dim_{dim}/start_point_raw",
                        xs=np.array([start_step]),
                        ys=np.array([raw_start_point[dim]]),
                        colors=np.array([[0, 128, 0]])
                    )
                    vizer.visualize_points(
                        name=f"chunk_lowpass/action_dim_{dim}/end_point",
                        xs=np.array([end_step]),
                        ys=np.array([lowpass_chunk[-1, dim]]),
                        colors=np.array([[255, 0, 0]])
                    )
                control_step_cursor += resampled_chunk.shape[0]
        
        last_data_step = data_step
        
        # ========== åœ¨mujocoé‡Œæ‰§è¡ŒåŠ¨ä½œ (å¦‚æœå¯ç”¨) =========
        if visualize_in_mujoco:
            action_np = pred_action_single[np.newaxis, :]  # (1, action_dim)
            
            # æ ¹æ®actionç»´åº¦é€‰æ‹©æ‰§è¡Œæ–¹æ³•
            if action_dim == 16:
                # depalletizeä»»åŠ¡ï¼š16ç»´åŠ¨ä½œ (14 arm joints + 2 claw positions)
                mujoco_env.exec_actions(
                    actions=action_np,
                    control_arm=True,
                    control_claw=True
                )
            else:
                # comæ§åˆ¶ä»»åŠ¡ï¼šä½¿ç”¨ç»å¯¹åŠ¨ä½œæ‰§è¡Œï¼ˆGrootPolicyé»˜è®¤ä½¿ç”¨ç»å¯¹åŠ¨ä½œï¼‰
                if mujoco_env is not None:
                    mujoco_env.exec_absolute_actions(
                        actions=action_np,
                        control_arm=True,
                        control_base=True,
                        control_wrench=False
                    )

    # ========= æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ =========
    print("\n" + "="*80)
    print("ğŸ“Š Final Statistics")
    print("="*80)
    
    # Actionåç§°å®šä¹‰ - æ ¹æ®actionç»´åº¦è‡ªåŠ¨é€‰æ‹©
    # ä¼˜å…ˆä½¿ç”¨configä¸­çš„action_namesï¼ˆå¦‚æœå¯ç”¨ä¸”ç»´åº¦åŒ¹é…ï¼‰
    if CONFIG_AVAILABLE and action_names and len(action_names) == action_dim:
        eval_action_names = action_names
    elif action_dim == 16:
        # depalletizeä»»åŠ¡ï¼š16ç»´åŠ¨ä½œ (14 arm joints + 2 claw positions)
        eval_action_names = [f"Arm_joint_{i+1}" for i in range(14)] + ["Left_claw", "Right_claw"]
    elif action_dim == 18:
        # 18ç»´åŠ¨ä½œï¼šLeft_arm(7) + Right_arm(7) + Left_claw(1) + Right_claw(1) + Cmd_pose_z(1) + Cmd_pose_pitch(1)
        eval_action_names = (
            [f"arm_joint_{i+1}" for i in range(7)] +  # Left_arm: arm_joint_1-7
            [f"arm_joint_{i+8}" for i in range(7)] +  # Right_arm: arm_joint_8-14
            ["left_claw_position", "right_claw_position", "cmd_pose_z", "cmd_pose_pitch"]
        )
    elif action_dim == 24:
        # comæ§åˆ¶ä»»åŠ¡ï¼š24ç»´ = 9 COM + 14 Arm + 1 Gait
        eval_action_names = (
            ["COM_dx", "COM_dy", "COM_dz", "COM_dR11", "COM_dR21", "COM_dR31", "COM_dR12", "COM_dR22", "COM_dR32"] +
            [f"Arm_joint_{i+1}" for i in range(14)] +
            ["Gait_mode"]
        )
    else:
        # å…¶ä»–ç»´åº¦ï¼šä½¿ç”¨é€šç”¨å‘½å
        eval_action_names = [f"Action_dim_{i}" for i in range(action_dim)]
    
    print(f"\n{'Dimension':<20} {'MSE':<15} {'MAE':<15}")
    print("-" * 80)
    
    for dim in range(action_dim):
        mse_mean = np.mean(mse_per_action_dim[dim])
        mae_mean = np.mean(mae_per_action_dim[dim])
        dim_name = eval_action_names[dim] if dim < len(eval_action_names) else f"Dim_{dim}"
        print(f'{dim_name:<20} {mse_mean:<15.8f} {mae_mean:<15.8f}')
    
    overall_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(action_dim)])
    overall_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(action_dim)])
    
    print("-" * 80)
    print(f'{"Overall":<20} {overall_mse:<15.8f} {overall_mae:<15.8f}')
    
    # ========= æ¨ç†æ—¶é—´ç»Ÿè®¡ =========
    if len(inference_times) > 0:
        print("\n" + "="*80)
        print("â±ï¸  Inference Time Statistics")
        print("="*80)
        avg_inference_time = np.mean(inference_times)
        min_inference_time = np.min(inference_times)
        max_inference_time = np.max(inference_times)
        median_inference_time = np.median(inference_times)
        std_inference_time = np.std(inference_times)
        
        print(f"Total inference calls: {len(inference_times)}")
        print(f"Average inference time: {avg_inference_time*1000:.2f} ms")
        print(f"Median inference time: {median_inference_time*1000:.2f} ms")
        print(f"Min inference time: {min_inference_time*1000:.2f} ms")
        print(f"Max inference time: {max_inference_time*1000:.2f} ms")
        print(f"Std inference time: {std_inference_time*1000:.2f} ms")
        
        # è®¡ç®—ç†è®ºæœ€å¤§æ¨ç†é¢‘ç‡ï¼ˆåŸºäºå¹³å‡æ¨ç†æ—¶é—´ï¼‰
        # è¿™è¡¨ç¤ºå¦‚æœè¿ç»­æ¨ç†ï¼Œç†è®ºä¸Šå¯ä»¥è¾¾åˆ°çš„æœ€å¤§é¢‘ç‡
        if avg_inference_time > 0:
            max_frequency = 1.0 / avg_inference_time
            print(f"Max theoretical inference frequency: {max_frequency:.2f} Hz")
            print(f"  (Based on average inference time: {avg_inference_time*1000:.2f} ms)")
        
        print("="*80)
    else:
        print("\nâš ï¸  Warning: No inference time statistics available (no inference was performed)")
    
    # åˆ†ç»„ç»Ÿè®¡ - æ ¹æ®actionç»´åº¦é€‰æ‹©ç»Ÿè®¡æ–¹å¼
    print("\nğŸ“Š Grouped Statistics:")
    print("-" * 80)
    
    if action_dim == 16:
        # depalletizeä»»åŠ¡ï¼š16ç»´ = 14 arm joints + 2 claw positions
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        claw_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(14, 16)])
        claw_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(14, 16)])
        print(f'{"Claw (avg)":<20} {claw_mse:<15.8f} {claw_mae:<15.8f}')
    else:
        # comæ§åˆ¶ä»»åŠ¡ï¼šæ ‡å‡†åˆ†ç»„ç»Ÿè®¡
        com_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9)])
        com_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9)])
        print(f'{"COM (avg)":<20} {com_mse:<15.8f} {com_mae:<15.8f}')
        
        arm_mse = np.mean([np.mean(mse_per_action_dim[dim]) for dim in range(9, 23)])
        arm_mae = np.mean([np.mean(mae_per_action_dim[dim]) for dim in range(9, 23)])
        print(f'{"Arm (avg)":<20} {arm_mse:<15.8f} {arm_mae:<15.8f}')
        
        if action_dim > 23:
            gait_mse = np.mean(mse_per_action_dim[23])
            gait_mae = np.mean(mae_per_action_dim[23])
            print(f'{"Gait":<20} {gait_mse:<15.8f} {gait_mae:<15.8f}')
    
    print("="*80)

    if vizer is not None:
        print("\n[Offline Eval] Visualization active. Press Ctrl+C to exit.")
        try:
            while True:
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\nâœ… Exiting...")
    else:
        print("\nâœ… Evaluation completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Evaluate GrootPolicy Model on Dataset with Lowpass Visualization',
        epilog='Evaluates a trained GrootPolicy model on a LeRobot dataset with chunk interpolation and lowpass filtering visualization.'
    )
    parser.add_argument('--ckpt-path', type=str, required=True,
                       help='Path to the model checkpoint directory')
    parser.add_argument('--dataset-root', '--dataset_root', type=str, required=True,
                       dest='dataset_root',
                       help='Path to the LeRobot dataset root directory')
    parser.add_argument('--episode', type=int, default=0,
                       help='Episode number to evaluate (default: 0)')
    parser.add_argument('--action-chunk-size', type=int, default=50,
                       help='Action chunk size (default: 50, should match training config)')
    parser.add_argument('--with-mujoco', action='store_true',
                       help='Visualize and execute in MuJoCo environment')
    parser.add_argument('--no-progress', action='store_true',
                       help='Disable progress bar')
    parser.add_argument('--image-zero', action='store_true',
                       help='Set all image inputs to zero (for testing model dependency on images)')
    parser.add_argument('--state-zero', action='store_true',
                       help='Set all state inputs to zero (for testing model dependency on state)')
    parser.add_argument('--cam-head-zero', action='store_true',
                       help='Set cam_head (image) input to zero (for testing model dependency on cam_head)')
    parser.add_argument('--infer-per-frame', type=int, default=1,
                       help='Run policy inference every N frames (default: 1 = every frame)')
    parser.add_argument('--task-description', type=str, default=None,
                       help='Task description (language instruction) to override the task from dataset. If not provided, will use the task from dataset.')
    parser.add_argument('--training-dataset-paths', nargs='+', type=str, default=None,
                       help='Paths to training datasets for computing aggregated statistics. If provided, statistics from all these datasets will be aggregated and used for denormalization. Example: --training-dataset-paths /path/to/dataset1 /path/to/dataset2')

    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ğŸ¯ GrootPolicy Dataset Evaluation with Lowpass Visualization")
    print("="*80)
    print(f"Checkpoint: {args.ckpt_path}")
    print(f"Dataset: {args.dataset_root}")
    print(f"Episode: {args.episode}")
    print(f"Action Chunk Size: {args.action_chunk_size}")
    print(f"MuJoCo Visualization: {args.with_mujoco}")
    print(f"Image Zero Mode: {args.image_zero}")
    print(f"State Zero Mode: {args.state_zero}")
    print(f"Cam Head Zero Mode: {args.cam_head_zero}")
    print(f"Infer Every N Frames: {args.infer_per_frame}")
    if args.task_description:
        print(f"Task Description (overridden): '{args.task_description}'")
    else:
        print(f"Task Description: Will use task from dataset")
    if args.training_dataset_paths:
        print(f"Training Dataset Paths (for statistics): {args.training_dataset_paths}")
    else:
        print(f"Training Dataset Paths: Using evaluation dataset statistics")
    print("="*80)
    
    eval_on_dataset(
        ckpt_path=args.ckpt_path,
        lerobot_dataset_path=args.dataset_root,
        episode=args.episode,
        n_actions=args.action_chunk_size,
        visualize_in_mujoco=args.with_mujoco,
        show_progress=not args.no_progress,
        image_zero=args.image_zero,
        state_zero=args.state_zero,
        cam_head_zero=args.cam_head_zero,
        infer_per_frame=args.infer_per_frame,
        task_description=args.task_description,
        training_dataset_paths=args.training_dataset_paths
    )
