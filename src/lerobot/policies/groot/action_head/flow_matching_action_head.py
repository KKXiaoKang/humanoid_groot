# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass, field
from typing import TYPE_CHECKING

import torch
import torch.nn.functional as F  # noqa: N812
from torch import nn
from torch.distributions import Beta

from lerobot.utils.import_utils import _transformers_available

# Conditional import for type checking and lazy loading
if TYPE_CHECKING or _transformers_available:
    from transformers import PretrainedConfig
    from transformers.feature_extraction_utils import BatchFeature
else:
    PretrainedConfig = object
    BatchFeature = None

from lerobot.policies.groot.action_head.action_encoder import (
    SinusoidalPositionalEncoding,
    swish,
)

from .cross_attention_dit import DiT, SelfAttentionTransformer
from typing_extensions import Unpack
from lerobot.policies.rtc.modeling_rtc import RTCProcessor


class ReasoningHead(nn.Module):
    """
    Chain of Causation (CoC) Reasoning Head
    
    å®ç°çœŸæ­£çš„Chain of Causationæ¨ç†é“¾ï¼š
    1. ä»backbone_featuresç”Ÿæˆreasoning traceï¼ˆæ€ç»´é“¾ï¼‰
    2. åŸºäºreasoning traceç”Ÿæˆaction decisionï¼ˆåŠ¨ä½œå†³ç­–ï¼‰
    3. ä½¿ç”¨reasoning conditioningæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆ
    
    è¿™æ˜¯çœŸæ­£çš„å› æœå…³ç³»é“¾ï¼šbackbone â†’ reasoning trace â†’ action decision â†’ action
    
    æ”¯æŒ6ç§action decisionç±»å‹ï¼š
    1. left_search_grasp_pull: æœºå™¨äººç§»åŠ¨å·¦æ‰‹å¯»æ‰¾ç®±å­å·¦ä¾§è¾¹ç¼˜ï¼Œå¤¹çˆªæŠ“å–åå¹¶æ‹‰å¼€ï¼Œå³æ‰‹ä¿æŒä¸åŠ¨
    2. left_hold_right_search_grasp: æœºå™¨äººå·¦æ‰‹æŠ“ä½ç®±å­è¾¹ç¼˜ä¿æŒä¸åŠ¨ï¼Œå³æ‰‹æ‰¾åˆ°ç®±å­çš„è¾¹ç¼˜å¹¶ä¸”æŠ“ä½
    3. right_search_grasp_pull: æœºå™¨äººç§»åŠ¨å³æ‰‹å¯»æ‰¾ç®±å­å³ä¾§è¾¹ç¼˜ï¼Œå¤¹çˆªæŠ“å–åå¹¶æ‹‰å¼€ï¼Œå·¦æ‰‹ä¿æŒä¸åŠ¨
    4. right_hold_left_search_grasp: æœºå™¨äººå³æ‰‹æŠ“ä½ç®±å­è¾¹ç¼˜ä¿æŒä¸åŠ¨ï¼Œå·¦æ‰‹æ‰¾åˆ°ç®±å­çš„è¾¹ç¼˜å¹¶ä¸”æŠ“ä½
    5. both_search_grasp: æœºå™¨äººå·¦å³æ‰‹åŒæ—¶æ‰¾åˆ°ç®±å­çš„å·¦å³è¾¹ç¼˜ï¼Œå¹¶ä¸”æŠ“å–
    6. both_hold_lift: æœºå™¨äººå·¦æ‰‹å³æ‰‹å·²ç»æŠ“ä½ç®±å­è¾¹ç¼˜ï¼ŒåŒæ—¶ä¸ŠæŠ¬æèµ·ç®±å­
    
    å…³é”®è®¾è®¡ï¼š
    - è®­ç»ƒæ—¶ï¼šä½¿ç”¨ground truth reasoning labelsï¼ŒåŸºäºreasoning traceç”Ÿæˆaction decision
    - æ¨ç†æ—¶ï¼šè‡ªå›å½’ç”Ÿæˆreasoning traceï¼Œç„¶ååŸºäºç”Ÿæˆçš„reasoning traceç”Ÿæˆaction decision
    - è¿™ç¡®ä¿äº†reasoning traceå’Œaction decisionä¹‹é—´çš„å› æœå…³ç³»ï¼Œç¬¦åˆChain of Causationçš„è®¾è®¡ç†å¿µ
    """
    def __init__(
        self,
        backbone_embedding_dim: int,
        reasoning_hidden_dim: int,
        reasoning_vocab_size: int,
        reasoning_max_length: int,
        num_layers: int = 2,
    ):
        super().__init__()
        self.reasoning_hidden_dim = reasoning_hidden_dim
        self.reasoning_vocab_size = reasoning_vocab_size
        self.reasoning_max_length = reasoning_max_length
        
        # å°†backboneç‰¹å¾æŠ•å½±åˆ°reasoningç©ºé—´
        self.backbone_proj = nn.Linear(backbone_embedding_dim, reasoning_hidden_dim)
        
        # å°å‹Transformerç”¨äºç”Ÿæˆreasoning tokens
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=reasoning_hidden_dim,
            nhead=8,
            dim_feedforward=reasoning_hidden_dim * 4,
            dropout=0.1,
            batch_first=True,
        )
        self.reasoning_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Token embeddingå’Œä½ç½®ç¼–ç 
        self.token_embedding = nn.Embedding(reasoning_vocab_size, reasoning_hidden_dim)
        self.position_embedding = nn.Embedding(reasoning_max_length, reasoning_hidden_dim)
        
        # è¾“å‡ºå±‚ï¼šç”Ÿæˆreasoning tokensçš„logits
        self.output_proj = nn.Linear(reasoning_hidden_dim, reasoning_vocab_size)
        
        # æ¡ä»¶åŒ–embeddingï¼šå°†reasoning tokensç¼–ç ä¸ºæ¡ä»¶å‘é‡ï¼Œç”¨äºæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆ
        self.conditioning_proj = nn.Linear(reasoning_hidden_dim, reasoning_hidden_dim)
        
        # Action decision prediction: é¢„æµ‹action decisionç±»å‹
        self.action_decision_predictor = nn.Sequential(
            nn.Linear(reasoning_hidden_dim, reasoning_hidden_dim),
            nn.ReLU(),
            nn.Linear(reasoning_hidden_dim, 6),  # 6ç§å†³ç­–ç±»å‹
        )
        
        # Action decision embedding: å°†action decisionç±»å‹ç¼–ç ä¸ºæ¡ä»¶å‘é‡
        # ç”¨äºç›´æ¥æŒ‡å¯¼decoderçš„åŠ¨ä½œç”Ÿæˆæ–¹å‘
        self.action_decision_embedding = nn.Embedding(6, reasoning_hidden_dim)  # 6ç§å†³ç­–ç±»å‹
        
    def forward(
        self, 
        backbone_features: torch.Tensor, 
        reasoning_labels: torch.Tensor | None = None,
        action_decision_labels: torch.Tensor | None = None,
    ):
        """
        Args:
            backbone_features: (B, T, backbone_embedding_dim) - æ¥è‡ªbackboneçš„ç‰¹å¾
            reasoning_labels: (B, L) - å¯é€‰çš„ground truth reasoning token idsï¼Œç”¨äºè®­ç»ƒ
            action_decision_labels: (B,) - å¯é€‰çš„ground truth action decision labelsï¼Œç”¨äºè®­ç»ƒ
        
        Returns:
            reasoning_logits: (B, L, vocab_size) - reasoning tokensçš„logits
            reasoning_conditioning: (B, reasoning_hidden_dim) - ç”¨äºæ¡ä»¶åŒ–åŠ¨ä½œç”Ÿæˆçš„å‘é‡ï¼ˆèåˆäº†action decisionä¿¡æ¯ï¼‰
            action_decision_logits: (B, 6) - action decisionç±»å‹çš„logits (6ç§å†³ç­–ç±»å‹)
        """
        B, T, _ = backbone_features.shape
        
        # 1. æŠ•å½±backboneç‰¹å¾
        backbone_proj = self.backbone_proj(backbone_features)  # (B, T, reasoning_hidden_dim)
        
        # 2. èšåˆbackboneç‰¹å¾ï¼ˆä½¿ç”¨å¹³å‡æ± åŒ–æˆ–CLS tokenï¼‰
        # ä½¿ç”¨å¹³å‡æ± åŒ–å¾—åˆ°å…¨å±€è¡¨ç¤º
        backbone_global = backbone_proj.mean(dim=1)  # (B, reasoning_hidden_dim)
        
        # 3. ç”Ÿæˆreasoning tokens
        reasoning_output = None  # ç”¨äºåç»­ç”Ÿæˆaction decision
        if reasoning_labels is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ground truth labels
            L = reasoning_labels.shape[1]  # reasoning sequence length
            token_embeds = self.token_embedding(reasoning_labels)  # (B, L, reasoning_hidden_dim)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            pos_ids = torch.arange(L, device=reasoning_labels.device).unsqueeze(0).expand(B, -1)
            pos_embeds = self.position_embedding(pos_ids)
            token_embeds = token_embeds + pos_embeds
            
            # å°†backboneå…¨å±€ç‰¹å¾ä½œä¸ºåˆå§‹token
            # æ‹¼æ¥: [backbone_global, token_embeds]
            reasoning_input = torch.cat([backbone_global.unsqueeze(1), token_embeds], dim=1)  # (B, 1+L, reasoning_hidden_dim)
            
            # é€šè¿‡Transformer
            reasoning_output = self.reasoning_transformer(reasoning_input)  # (B, 1+L, reasoning_hidden_dim)
            
            # åªå–tokenéƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬backbone_globalï¼‰
            reasoning_output = reasoning_output[:, 1:]  # (B, L, reasoning_hidden_dim)
            
            # ç”Ÿæˆlogits
            reasoning_logits = self.output_proj(reasoning_output)  # (B, L, vocab_size)
        else:
            # æ¨ç†æ¨¡å¼ï¼šè‡ªå›å½’ç”Ÿæˆreasoning trace
            # è¿™æ˜¯çœŸæ­£çš„Chain of Causationï¼šä»backboneç‰¹å¾ç”Ÿæˆreasoning trace
            reasoning_logits, reasoning_output = self._generate_reasoning_autoregressive(
                backbone_global, max_length=self.reasoning_max_length
            )
        
        # 4. ç”Ÿæˆaction decision logitsï¼ˆåŸºäºreasoning traceï¼Œè€Œä¸æ˜¯ç›´æ¥åŸºäºbackboneï¼‰
        # è¿™æ˜¯Chain of Causationçš„å…³é”®ï¼šaction decisionåº”è¯¥åŸºäºreasoning traceç”Ÿæˆ
        if reasoning_output is not None:
            # ä½¿ç”¨reasoning traceçš„èšåˆç‰¹å¾æ¥é¢„æµ‹action decision
            reasoning_aggregated = reasoning_output.mean(dim=1)  # (B, reasoning_hidden_dim)
            action_decision_logits = self._predict_action_decision(reasoning_aggregated)  # (B, 6)
        else:
            # å¦‚æœæ²¡æœ‰reasoning traceï¼Œå›é€€åˆ°backboneç‰¹å¾ï¼ˆç”¨äºè®­ç»ƒåˆæœŸæˆ–å…¼å®¹æ€§ï¼‰
            action_decision_logits = self._predict_action_decision(backbone_global)  # (B, 6)
        
        # 5. ç”Ÿæˆreasoning conditioningå‘é‡ï¼ˆç”¨äºæ¡ä»¶åŒ–åŠ¨ä½œç”Ÿæˆï¼‰
        # å…³é”®æ”¹è¿›ï¼šå°†action decisionçš„ä¿¡æ¯èå…¥åˆ°conditioningä¸­ï¼Œä½¿å…¶èƒ½å¤ŸçœŸæ­£å¼•å¯¼åŠ¨ä½œç”Ÿæˆ
        if reasoning_output is not None:
            # ä½¿ç”¨reasoning traceçš„èšåˆç‰¹å¾ï¼ˆå¹³å‡æ± åŒ–ï¼‰æ¥ç”ŸæˆåŸºç¡€conditioning
            reasoning_aggregated = reasoning_output.mean(dim=1)  # (B, reasoning_hidden_dim)
            base_conditioning = self.conditioning_proj(reasoning_aggregated)  # (B, reasoning_hidden_dim)
        else:
            # å¦‚æœæ²¡æœ‰reasoning traceï¼Œä½¿ç”¨backboneç‰¹å¾
            base_conditioning = self.conditioning_proj(backbone_global)  # (B, reasoning_hidden_dim)
        
        # å°†action decisionçš„embeddingèå…¥åˆ°conditioningä¸­
        # è¿™æ˜¯å…³é”®ï¼šè®©action decisionçœŸæ­£å¼•å¯¼åŠ¨ä½œç”Ÿæˆ
        # åœ¨è®­ç»ƒæ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ground truth action_decision_labelsï¼ˆteacher forcingï¼‰
        # åœ¨æ¨ç†æ—¶ï¼Œä½¿ç”¨é¢„æµ‹çš„action_decision_logits
        if action_decision_labels is not None:
            # è®­ç»ƒæ—¶ï¼šä½¿ç”¨ground truth action_decision_labelsï¼ˆteacher forcingï¼‰
            # è¿™ç¡®ä¿äº†è®­ç»ƒæ—¶conditioningä½¿ç”¨çš„æ˜¯æ­£ç¡®çš„action decision
            action_decision_idx = action_decision_labels  # (B,)
            action_decision_emb = self.action_decision_embedding(action_decision_idx)  # (B, reasoning_hidden_dim)
        elif action_decision_logits is not None:
            # æ¨ç†æ—¶ï¼šä½¿ç”¨é¢„æµ‹çš„action_decision_logits
            predicted_decision_idx = torch.argmax(action_decision_logits, dim=-1)  # (B,)
            action_decision_emb = self.action_decision_embedding(predicted_decision_idx)  # (B, reasoning_hidden_dim)
        else:
            # å¦‚æœæ²¡æœ‰action decisionä¿¡æ¯ï¼Œåªä½¿ç”¨base conditioning
            action_decision_emb = None
        
        # å°†action decision embeddingä¸base conditioningèåˆ
        # ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œè®©action decisionçš„ä¿¡æ¯ç›´æ¥æ³¨å…¥åˆ°conditioningä¸­
        # è¿™æ ·action decisionå°±èƒ½çœŸæ­£å¼•å¯¼DiTçš„åŠ¨ä½œç”Ÿæˆæ–¹å‘
        if action_decision_emb is not None:
            reasoning_conditioning = base_conditioning + action_decision_emb  # (B, reasoning_hidden_dim)
        else:
            reasoning_conditioning = base_conditioning
        
        return reasoning_logits, reasoning_conditioning, action_decision_logits
    
    def _predict_action_decision(self, features: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹action decisionç±»å‹"""
        return self.action_decision_predictor(features)
    
    def _generate_reasoning_autoregressive(
        self, 
        backbone_global: torch.Tensor, 
        max_length: int,
        temperature: float = 1.0,
    ) -> tuple[torch.Tensor | None, torch.Tensor]:
        """
        è‡ªå›å½’ç”Ÿæˆreasoning trace
        
        Args:
            backbone_global: (B, reasoning_hidden_dim) - backboneçš„å…¨å±€ç‰¹å¾
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
        
        Returns:
            reasoning_logits: (B, L, vocab_size) - æœ€åä¸€ä¸ªtokençš„logitsï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼Œæ¨ç†æ—¶å¯èƒ½ä¸ºNoneï¼‰
            reasoning_output: (B, L, reasoning_hidden_dim) - ç”Ÿæˆçš„reasoning traceçš„éšè—çŠ¶æ€
        """
        B = backbone_global.shape[0]
        device = backbone_global.device
        
        # åˆå§‹åŒ–ï¼šä»backbone_globalå¼€å§‹
        current_input = backbone_global.unsqueeze(1)  # (B, 1, reasoning_hidden_dim)
        generated_tokens = []
        generated_embeds = []
        
        # è‡ªå›å½’ç”Ÿæˆ
        for step in range(max_length):
            # é€šè¿‡Transformerå¤„ç†å½“å‰åºåˆ—
            reasoning_output_step = self.reasoning_transformer(current_input)  # (B, seq_len, reasoning_hidden_dim)
            
            # å–æœ€åä¸€ä¸ªtokençš„è¾“å‡ºï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰
            last_token_output = reasoning_output_step[:, -1:]  # (B, 1, reasoning_hidden_dim)
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = self.output_proj(last_token_output)  # (B, 1, vocab_size)
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtokenï¼ˆä½¿ç”¨greedy decodingæˆ–temperature samplingï¼‰
            if temperature == 0.0:
                # Greedy decoding
                next_token_id = torch.argmax(next_token_logits, dim=-1)  # (B, 1)
            else:
                # Temperature sampling
                probs = F.softmax(next_token_logits / temperature, dim=-1)
                next_token_id = torch.multinomial(probs.squeeze(1), num_samples=1).unsqueeze(1)  # (B, 1)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸtokenï¼ˆè¿™é‡Œå‡è®¾0æ˜¯ç»“æŸtokenï¼Œå®é™…åº”è¯¥æ ¹æ®vocabå®šä¹‰ï¼‰
            # ç®€åŒ–å®ç°ï¼šå¦‚æœç”Ÿæˆçš„tokenæ˜¯0ï¼Œåˆ™åœæ­¢ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„ç»“æŸtokenï¼Œå¦‚EOS tokenï¼‰
            if (next_token_id == 0).all():
                break
            
            generated_tokens.append(next_token_id)
            
            # å°†æ–°ç”Ÿæˆçš„token embeddingæ·»åŠ åˆ°è¾“å…¥ä¸­
            next_token_embed = self.token_embedding(next_token_id.squeeze(1))  # (B, reasoning_hidden_dim)
            pos_embed = self.position_embedding(
                torch.full((B,), step + 1, device=device, dtype=torch.long)
            )  # (B, reasoning_hidden_dim)
            next_token_embed = next_token_embed + pos_embed.unsqueeze(1)  # (B, 1, reasoning_hidden_dim)
            
            # æ›´æ–°è¾“å…¥ï¼šæ‹¼æ¥æ–°ç”Ÿæˆçš„token
            current_input = torch.cat([current_input, next_token_embed], dim=1)  # (B, seq_len+1, reasoning_hidden_dim)
        
        # é‡æ–°é€šè¿‡Transformerå¤„ç†å®Œæ•´åºåˆ—ï¼Œè·å–æ‰€æœ‰tokençš„éšè—çŠ¶æ€
        # è¿™æ ·å¯ä»¥å¾—åˆ°å®Œæ•´çš„reasoning traceè¡¨ç¤ºï¼Œç”¨äºåç»­çš„action decisioné¢„æµ‹
        if len(generated_tokens) > 0:
            # é‡æ–°å¤„ç†å®Œæ•´åºåˆ—ä»¥è·å–æ‰€æœ‰tokençš„éšè—çŠ¶æ€
            reasoning_output = self.reasoning_transformer(current_input)  # (B, 1+L, reasoning_hidden_dim)
            # åªå–ç”Ÿæˆçš„tokenéƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬åˆå§‹çš„backbone_globalï¼‰
            reasoning_output = reasoning_output[:, 1:]  # (B, L, reasoning_hidden_dim)
        else:
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•tokenï¼Œä½¿ç”¨backbone_global
            reasoning_output = backbone_global.unsqueeze(1)  # (B, 1, reasoning_hidden_dim)
        
        # æ¨ç†æ—¶ä¸éœ€è¦è¿”å›logitsï¼ˆå› ä¸ºå·²ç»é‡‡æ ·äº†ï¼‰ï¼Œä½†ä¸ºäº†æ¥å£ä¸€è‡´æ€§ï¼Œè¿”å›None
        reasoning_logits = None
        
        return reasoning_logits, reasoning_output
    
    def get_action_decision_embedding(self, decision_type: str) -> torch.Tensor:
        """
        è·å–action decisionç±»å‹çš„embedding
        
        Args:
            decision_type: 6ç§å†³ç­–ç±»å‹ä¹‹ä¸€ï¼š
                - "left_search_grasp_pull": å·¦æ‰‹æœç´¢æŠ“å–æ‹‰å¼€ï¼Œå³æ‰‹ä¸åŠ¨
                - "left_hold_right_search_grasp": å·¦æ‰‹ä¿æŒï¼Œå³æ‰‹æœç´¢æŠ“å–
                - "right_search_grasp_pull": å³æ‰‹æœç´¢æŠ“å–æ‹‰å¼€ï¼Œå·¦æ‰‹ä¸åŠ¨
                - "right_hold_left_search_grasp": å³æ‰‹ä¿æŒï¼Œå·¦æ‰‹æœç´¢æŠ“å–
                - "both_search_grasp": åŒæ‰‹åŒæ—¶æœç´¢æŠ“å–
                - "both_hold_lift": åŒæ‰‹ä¿æŒå¹¶ä¸ŠæŠ¬
        
        Returns:
            embedding: (reasoning_hidden_dim,) - action decisionçš„embeddingå‘é‡
        """
        decision_map = {
            "left_search_grasp_pull": 0,
            "left_hold_right_search_grasp": 1,
            "right_search_grasp_pull": 2,
            "right_hold_left_search_grasp": 3,
            "both_search_grasp": 4,
            "both_hold_lift": 5,
        }
        if decision_type not in decision_map:
            raise ValueError(
                f"Unknown decision type: {decision_type}. "
                f"Valid types: {list(decision_map.keys())}"
            )
        idx = decision_map[decision_type]
        return self.action_decision_embedding(torch.tensor(idx))


class CategorySpecificLinear(nn.Module):
    def __init__(self, num_categories, input_dim, hidden_dim):
        super().__init__()
        self.num_categories = num_categories
        # For each category, we have separate weights and biases.
        self.W = nn.Parameter(0.02 * torch.randn(num_categories, input_dim, hidden_dim))
        self.b = nn.Parameter(torch.zeros(num_categories, hidden_dim))

    def forward(self, x, cat_ids):
        selected_w = self.W[cat_ids]
        selected_b = self.b[cat_ids]
        return torch.bmm(x, selected_w) + selected_b.unsqueeze(1)


class CategorySpecificMLP(nn.Module):
    def __init__(self, num_categories, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.num_categories = num_categories
        self.layer1 = CategorySpecificLinear(num_categories, input_dim, hidden_dim)
        self.layer2 = CategorySpecificLinear(num_categories, hidden_dim, output_dim)

    def forward(self, x, cat_ids):
        hidden = F.relu(self.layer1(x, cat_ids))
        return self.layer2(hidden, cat_ids)


class SharedBottomArmDecoder(nn.Module):
    """
    å…±äº«åº•å±‚ç‰¹å¾çš„å·¦å³æ‰‹decoderï¼Œæå‡åè°ƒæ€§
    
    æ³¨æ„ï¼šå¦‚æœ use_cross_attention=Falseï¼Œè¿™ä¸ªæ–¹æ¡ˆåœ¨å‚æ•°ä¸Šå‡ ä¹ç­‰ä»·äº
    "åˆæˆä¸€ä¸ªMLPè¾“å‡º14ç»´ç„¶åsplit"ï¼Œä¸»è¦åŒºåˆ«æ˜¯ï¼š
    1. è¾“å‡ºå±‚åˆ†ç¦»ï¼Œå¯ä»¥åˆ†åˆ«æ§åˆ¶å·¦å³æ‰‹çš„æŸå¤±æƒé‡
    2. å¯ä»¥åˆ†åˆ«å­¦ä¹ ä¸åŒçš„è¾“å‡ºæ˜ å°„
    
    çœŸæ­£çš„ä»·å€¼åœ¨äºå¯ç”¨äº¤å‰æ³¨æ„åŠ›ï¼ˆuse_cross_attention=Trueï¼‰ï¼Œ
    è®©å·¦å³æ‰‹ç‰¹å¾èƒ½å¤Ÿç›¸äº’å…³æ³¨ï¼Œè¿™æ˜¯"åˆæˆä¸€ä¸ªMLP"æ— æ³•å®ç°çš„ã€‚
    """
    def __init__(self, num_categories, input_dim, hidden_dim, left_output_dim, right_output_dim, use_cross_attention=False):
        super().__init__()
        self.num_categories = num_categories
        self.use_cross_attention = use_cross_attention
        
        # å…±äº«çš„åº•å±‚ç‰¹å¾æå–å±‚
        # æ³¨æ„ï¼šå¦‚æœåªæ˜¯å…±äº«åº•å±‚ï¼Œç¡®å®å’Œ"åˆæˆä¸€ä¸ªMLPç„¶åsplit"ç±»ä¼¼
        # ä½†è¾“å‡ºå±‚åˆ†ç¦»å…è®¸åˆ†åˆ«æ§åˆ¶æŸå¤±æƒé‡å’Œå­¦ä¹ ä¸åŒçš„æ˜ å°„
        self.shared_layer = CategorySpecificLinear(num_categories, input_dim, hidden_dim)
        
        # å·¦å³æ‰‹å„è‡ªçš„è¾“å‡ºå±‚
        # è¿™æ˜¯å’Œ"åˆæˆä¸€ä¸ªMLP"çš„ä¸»è¦åŒºåˆ«ï¼šè¾“å‡ºå±‚åˆ†ç¦»
        self.left_output_layer = CategorySpecificLinear(num_categories, hidden_dim, left_output_dim)
        self.right_output_layer = CategorySpecificLinear(num_categories, hidden_dim, right_output_dim)
        
        # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šè¿™æ˜¯çœŸæ­£çš„ä»·å€¼æ‰€åœ¨
        # è®©å·¦å³æ‰‹ç‰¹å¾èƒ½å¤Ÿç›¸äº’å…³æ³¨ï¼Œè¿™æ˜¯"åˆæˆä¸€ä¸ªMLP"æ— æ³•å®ç°çš„
        if use_cross_attention:
            # ç®€å•çš„äº¤å‰æ³¨æ„åŠ›ï¼šå·¦å³æ‰‹ç‰¹å¾ç›¸äº’å…³æ³¨
            self.cross_attn_left = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)
            self.cross_attn_right = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)
            self.layer_norm_left = nn.LayerNorm(hidden_dim)
            self.layer_norm_right = nn.LayerNorm(hidden_dim)
            print(f"   âœ… Cross-attention enabled: leftâ†”right arm features can attend to each other")
        else:
            print(f"   âš ï¸  Cross-attention disabled: This is similar to 'single MLP then split'")
            print(f"      Main difference: separate output layers allow different loss weights")
    
    def forward(self, x, cat_ids):
        """
        x: (B, T, input_dim)
        cat_ids: (B,)
        returns: (left_features, right_features) æˆ– (left_output, right_output)
        """
        # å…±äº«åº•å±‚ç‰¹å¾æå–
        shared_features = F.relu(self.shared_layer(x, cat_ids))  # (B, T, hidden_dim)
        
        if self.use_cross_attention:
            # äº¤å‰æ³¨æ„åŠ›ï¼šå·¦å³æ‰‹ç‰¹å¾ç›¸äº’å…³æ³¨
            # è¿™æ˜¯çœŸæ­£çš„ä»·å€¼ï¼šè®©å·¦å³æ‰‹èƒ½å¤Ÿæ„ŸçŸ¥å¯¹æ–¹çš„çŠ¶æ€
            # è¿™æ˜¯"åˆæˆä¸€ä¸ªMLPç„¶åsplit"æ— æ³•å®ç°çš„
            # ä½¿ç”¨å¯¹ç§°çš„äº¤å‰æ³¨æ„åŠ›ï¼Œç¡®ä¿ä¿¡æ¯äº¤æ¢çš„ä¸€è‡´æ€§
            left_features = self.layer_norm_left(shared_features)
            right_features = self.layer_norm_right(shared_features)
            
            # å¯¹ç§°çš„äº¤å‰æ³¨æ„åŠ›ï¼šåŒæ—¶è®¡ç®—ï¼Œé¿å…ä¿¡æ¯ä¸å¯¹ç§°
            # å·¦æ‰‹çš„queryå…³æ³¨å³æ‰‹çš„key/valueï¼ˆä½¿ç”¨åŸå§‹right_featuresï¼‰
            left_attended, _ = self.cross_attn_left(
                left_features, right_features, right_features
            )
            # å³æ‰‹çš„queryå…³æ³¨å·¦æ‰‹çš„key/valueï¼ˆä½¿ç”¨åŸå§‹left_featuresï¼‰
            right_attended, _ = self.cross_attn_right(
                right_features, left_features, left_features
            )
            
            # æ®‹å·®è¿æ¥ï¼šä¿æŒåŸå§‹ç‰¹å¾ï¼Œåªæ·»åŠ æ³¨æ„åŠ›ä¿¡æ¯
            left_features = left_features + left_attended
            right_features = right_features + right_attended
            
            # è¾“å‡ºå±‚
            left_output = self.left_output_layer(left_features, cat_ids)
            right_output = self.right_output_layer(right_features, cat_ids)
        else:
            # ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œç›´æ¥è¾“å‡º
            # æ³¨æ„ï¼šè¿™ç§æƒ…å†µä¸‹ï¼Œç¡®å®å’Œ"åˆæˆä¸€ä¸ªMLPç„¶åsplit"ç±»ä¼¼
            # ä¸»è¦åŒºåˆ«æ˜¯è¾“å‡ºå±‚åˆ†ç¦»ï¼Œå¯ä»¥åˆ†åˆ«æ§åˆ¶æŸå¤±æƒé‡
            left_output = self.left_output_layer(shared_features, cat_ids)
            right_output = self.right_output_layer(shared_features, cat_ids)
        
        return left_output, right_output


class MultiEmbodimentActionEncoder(nn.Module):
    def __init__(self, action_dim, hidden_size, num_embodiments):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_embodiments = num_embodiments

        # W1: R^{w x d}, W2: R^{w x 2w}, W3: R^{w x w}
        self.W1 = CategorySpecificLinear(num_embodiments, action_dim, hidden_size)  # (d -> w)
        self.W2 = CategorySpecificLinear(num_embodiments, 2 * hidden_size, hidden_size)  # (2w -> w)
        self.W3 = CategorySpecificLinear(num_embodiments, hidden_size, hidden_size)  # (w -> w)
        self.pos_encoding = SinusoidalPositionalEncoding(hidden_size)

    def forward(self, actions, timesteps, cat_ids):
        """
        actions:   shape (B, T, action_dim)
        timesteps: shape (B,)  -- a single scalar per batch item
        cat_ids:   shape (B,)
        returns:   shape (B, T, hidden_size)
        """
        b, t, _ = actions.shape

        # 1) Expand each batch's single scalar time 'tau' across all T steps
        #    so that shape => (B, T)
        #    e.g. if timesteps is (B,), replicate across T
        if timesteps.dim() == 1 and timesteps.shape[0] == b:
            # shape (B,) => (B,T)
            timesteps = timesteps.unsqueeze(1).expand(-1, t)
        else:
            raise ValueError("Expected `timesteps` to have shape (B,) so we can replicate across T.")

        # 2) Standard action MLP step for shape => (B, T, w)
        a_emb = self.W1(actions, cat_ids)

        # 3) Get the sinusoidal encoding (B, T, w)
        tau_emb = self.pos_encoding(timesteps).to(dtype=a_emb.dtype)

        # 4) Concat along last dim => (B, T, 2w), then W2 => (B, T, w), swish
        x = torch.cat([a_emb, tau_emb], dim=-1)
        x = swish(self.W2(x, cat_ids))

        # 5) Finally W3 => (B, T, w)
        x = self.W3(x, cat_ids)
        return x


@dataclass
class FlowmatchingActionHeadConfig(PretrainedConfig):
    """NOTE: N1.5 uses XEmbFlowmatchingPolicyHeadConfig as action head"""

    add_pos_embed: bool = field(default=True, metadata={"help": "Whether to add positional embedding"})
    model_dtype: str = field(default="float32", metadata={"help": "Model data type."})
    diffusion_model_cfg: dict = field(default=None, metadata={"help": "Diffusion model configuration."})
    input_embedding_dim: int = field(default=1536, metadata={"help": "Input embedding channel dimension."})
    backbone_embedding_dim: int = field(
        default=1536, metadata={"help": "Backbone embedding channel dimension."}
    )

    hidden_size: int = field(default=1024, metadata={"help": "Input embedding dimension."})
    max_seq_len: int = field(default=1024, metadata={"help": "Maximum Sequence Length"})
    action_dim: int = field(default=None, metadata={"help": "Action dimension."})
    action_horizon: int = field(default=None, metadata={"help": "Action horizon."})
    noise_beta_alpha: float = field(default=1.5, metadata={"help": ""})
    noise_beta_beta: float = field(default=1.0, metadata={"help": ""})
    noise_s: float = field(default=0.999, metadata={"help": "Flow matching noise Beta distribution s."})
    num_timestep_buckets: int = field(
        default=1000, metadata={"help": "Number of timestep discretization buckets."}
    )
    num_inference_timesteps: int = field(
        default=None,
        metadata={"help": "Number of inference steps for noise diffusion."},
    )
    max_num_embodiments: int = field(default=32, metadata={"help": "Number of embodiments."})
    tune_projector: bool = field(default=True, metadata={"help": "Whether to tune the projector."})
    tune_diffusion_model: bool = field(
        default=True, metadata={"help": "Whether to tune the diffusion model."}
    )
    load_pretrained_det_decode_layer_path: str = field(
        default=None, metadata={"help": "Path to pretrained detection model."}
    )
    detection_coeff: float = field(default=1.0, metadata={"help": "Detection coefficient."})

    freeze_decode_layer: bool = field(default=False)
    expand_batch: int = field(default=None)
    use_vlln: bool = field(default=True)

    vl_self_attention_cfg: dict = field(default=None)
    # num_target_vision_tokens: int = field(default=32, metadata={"help": "Number of target vision tokens."})
    num_target_vision_tokens: int = field(default=64, metadata={"help": "Number of target vision tokens."})

    # Multi-head action prediction
    use_multi_action_heads: bool = field(default=True, metadata={"help": "Whether to use multi-head action prediction"})
    action_arm_dim: int = field(default=14, metadata={"help": "Arm joint dimensions (0-13) - absolute actions"})
    action_claw_dim: int = field(default=2, metadata={"help": "Claw position dimensions (14-15) - absolute actions"})
    
    # Split arm into left and right hands
    split_arm_heads: bool = field(default=True, metadata={"help": "Whether to split arm head into left and right arm heads"})
    action_left_arm_dim: int = field(default=7, metadata={"help": "Left arm joint dimensions (0-6) - absolute actions"})
    action_right_arm_dim: int = field(default=7, metadata={"help": "Right arm joint dimensions (7-13) - absolute actions"})
    
    # Coordination mechanisms for split arms
    # æœ€ä¼˜æ–¹æ¡ˆï¼šå…±äº«åº•å±‚ç‰¹å¾ + äº¤å‰æ³¨æ„åŠ› + åè°ƒæ€§æŸå¤±
    # è¿™æ ·å¯ä»¥å¹³è¡¡å·¦å³æ‰‹çš„ç‹¬ç«‹æ€§å’Œåè°ƒæ€§
    use_shared_arm_features: bool = field(default=True, metadata={"help": "Whether to share bottom layer features between left and right arms for better coordination"})
    use_cross_attention_arms: bool = field(default=True, metadata={"help": "Whether to use cross-attention between left and right arm features. Recommended: True for bimanual tasks"})
    arm_coordination_loss_weight: float = field(default=0.2, metadata={"help": "Weight for arm coordination loss (encourages synchronized movements). Recommended: 0.1-0.3"})
    
    # Loss weights for different action heads
    arm_loss_weight: float = field(default=1.0, metadata={"help": "Arm absolute position loss weight"})
    left_arm_loss_weight: float = field(default=1.0, metadata={"help": "Left arm absolute position loss weight"})
    right_arm_loss_weight: float = field(default=1.0, metadata={"help": "Right arm absolute position loss weight"})
    claw_loss_weight: float = field(default=1.0, metadata={"help": "Claw position loss weight"})
    
    # Learnable uncertainty weights (å‚è€ƒ https://arxiv.org/pdf/1705.07115)
    use_learnable_loss_weights: bool = field(default=True, metadata={"help": "Enable learnable loss weights based on uncertainty"})
    
    # Pretrained action dimension (for compatibility with pretrained models)
    pretrained_action_dim: int = field(default=None, metadata={"help": "Action dimension of pretrained model (for compatibility)"})
    
    # Chain of Causation (CoC) reasoning configuration
    use_coc_reasoning: bool = field(default=True, metadata={"help": "Whether to use Chain of Causation reasoning"})
    reasoning_vocab_size: int = field(default=1000, metadata={"help": "Vocabulary size for reasoning tokens"})
    reasoning_max_length: int = field(default=128, metadata={"help": "Maximum length of reasoning trace"})
    reasoning_hidden_dim: int = field(default=512, metadata={"help": "Hidden dimension for reasoning head"})
    reasoning_num_layers: int = field(default=2, metadata={"help": "Number of transformer layers in reasoning head"})
    reasoning_loss_weight: float = field(default=1.0, metadata={"help": "Weight for reasoning loss"})
    tune_reasoning_head: bool = field(default=True, metadata={"help": "Whether to tune the reasoning head"})
    reasoning_conditioning_type: str = field(default="decoder", metadata={"help": "Where to condition reasoning: 'decoder' or 'dit' or 'both'"})
    # Action decision types: 6ç§ç»†ç²’åº¦çš„å†³ç­–ç±»å‹
    action_decision_types: list[str] = field(
        default_factory=lambda: [
            "left_search_grasp_pull",      # 1. å·¦æ‰‹æœç´¢æŠ“å–æ‹‰å¼€ï¼Œå³æ‰‹ä¸åŠ¨
            "left_hold_right_search_grasp", # 2. å·¦æ‰‹ä¿æŒï¼Œå³æ‰‹æœç´¢æŠ“å–
            "right_search_grasp_pull",     # 3. å³æ‰‹æœç´¢æŠ“å–æ‹‰å¼€ï¼Œå·¦æ‰‹ä¸åŠ¨
            "right_hold_left_search_grasp", # 4. å³æ‰‹ä¿æŒï¼Œå·¦æ‰‹æœç´¢æŠ“å–
            "both_search_grasp",           # 5. åŒæ‰‹åŒæ—¶æœç´¢æŠ“å–
            "both_hold_lift",              # 6. åŒæ‰‹ä¿æŒå¹¶ä¸ŠæŠ¬
        ],
        metadata={"help": "List of action decision types (6 types)"}
    )

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        for key, value in kwargs.items():
            setattr(self, key, value)
        
        # Validate multi-head configuration
        if self.use_multi_action_heads:
            if self.split_arm_heads:
                # When splitting arms, validate left + right = total arm dim
                # Note: action_arm_dim should be set to left+right in groot_n1.py
                expected_arm_dim = self.action_left_arm_dim + self.action_right_arm_dim
                if self.action_arm_dim != expected_arm_dim:
                    raise ValueError(
                        f"When split_arm_heads=True, action_arm_dim ({self.action_arm_dim}) must equal "
                        f"action_left_arm_dim ({self.action_left_arm_dim}) + action_right_arm_dim ({self.action_right_arm_dim}) = {expected_arm_dim}"
                    )
                expected_action_dim = self.action_left_arm_dim + self.action_right_arm_dim + self.action_claw_dim
            else:
                expected_action_dim = self.action_arm_dim + self.action_claw_dim
            
            if self.action_dim is not None and self.action_dim != expected_action_dim:
                # If pretrained_action_dim is set, allow mismatch (we'll pad/truncate)
                if self.pretrained_action_dim is None:
                    raise ValueError(
                        f"When using multi-action heads, action_dim ({self.action_dim}) must equal "
                        f"{'left_arm + right_arm + claw' if self.split_arm_heads else 'arm + claw'} = {expected_action_dim}"
                    )
                # If pretrained_action_dim is set, use it for action_encoder
                if self.pretrained_action_dim != expected_action_dim:
                    print(f"âš ï¸  Pretrained model uses {self.pretrained_action_dim}D, but data uses {expected_action_dim}D. "
                          f"Will pad/truncate actions for compatibility.")


class FlowmatchingActionHead(nn.Module):
    config_class = FlowmatchingActionHeadConfig
    supports_gradient_checkpointing = True

    def __init__(
        self,
        config: FlowmatchingActionHeadConfig,
        rtc_processor: RTCProcessor | None = None,
    ):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.input_embedding_dim = config.input_embedding_dim

        self.model = DiT(**config.diffusion_model_cfg)
        self.action_dim = config.action_dim
        self.action_horizon = config.action_horizon
        self.num_inference_timesteps = config.num_inference_timesteps
        
        # Use pretrained_action_dim for action_encoder if specified (for compatibility with pretrained models)
        # Otherwise use action_dim
        encoder_action_dim = config.pretrained_action_dim if config.pretrained_action_dim is not None else config.action_dim
        self.encoder_action_dim = encoder_action_dim
        self.actual_action_dim = config.action_dim  # Actual action dimension from data

        self.state_encoder = CategorySpecificMLP(
            num_categories=config.max_num_embodiments,
            input_dim=config.max_state_dim,
            hidden_dim=self.hidden_size,
            output_dim=self.input_embedding_dim,
        )
        self.action_encoder = MultiEmbodimentActionEncoder(
            action_dim=encoder_action_dim,  # Use pretrained dimension for encoder
            hidden_size=self.input_embedding_dim,
            num_embodiments=config.max_num_embodiments,
        )
        
        # Multi-head action prediction
        if config.use_multi_action_heads:
            if config.split_arm_heads:
                # Split arm into left and right
                if config.use_shared_arm_features:
                    # ä½¿ç”¨å…±äº«åº•å±‚ç‰¹å¾çš„decoderï¼Œæå‡å·¦å³æ‰‹åè°ƒæ€§
                    self.shared_arm_decoder = SharedBottomArmDecoder(
                        num_categories=config.max_num_embodiments,
                        input_dim=self.hidden_size,
                        hidden_dim=self.hidden_size,
                        left_output_dim=config.action_left_arm_dim,
                        right_output_dim=config.action_right_arm_dim,
                        use_cross_attention=config.use_cross_attention_arms,
                    )
                    self.action_left_arm_decoder = None
                    self.action_right_arm_decoder = None
                    if config.use_cross_attention_arms:
                        print(f"ğŸ¤ Using OPTIMAL hybrid architecture:")
                        print(f"   âœ… Shared bottom layer (coordination)")
                        print(f"   âœ… Cross-attention (leftâ†”right awareness)")
                        print(f"   âœ… Separate output layers (independence)")
                        print(f"   âœ… Coordination loss weight={config.arm_coordination_loss_weight}")
                    else:
                        print(f"ğŸ¤ Using shared-bottom arm decoder (cross-attention disabled)")
                        print(f"   âš ï¸  This is similar to 'single MLP then split'")
                        print(f"   ğŸ’¡ Enable cross-attention for better coordination!")
                else:
                    # å®Œå…¨ç‹¬ç«‹çš„decoderï¼ˆåŸå§‹å®ç°ï¼‰
                    self.action_left_arm_decoder = CategorySpecificMLP(
                        num_categories=config.max_num_embodiments,
                        input_dim=self.hidden_size,
                        hidden_dim=self.hidden_size,
                        output_dim=config.action_left_arm_dim,
                    )
                    self.action_right_arm_decoder = CategorySpecificMLP(
                        num_categories=config.max_num_embodiments,
                        input_dim=self.hidden_size,
                        hidden_dim=self.hidden_size,
                        output_dim=config.action_right_arm_dim,
                    )
                    self.shared_arm_decoder = None
                    print(f"ğŸ”€ Using independent arm decoders")
                self.action_arm_decoder = None  # Not used when split
            else:
                # Single arm head
                self.action_arm_decoder = CategorySpecificMLP(
                    num_categories=config.max_num_embodiments,
                    input_dim=self.hidden_size,
                    hidden_dim=self.hidden_size,
                    output_dim=config.action_arm_dim,
                )
                self.action_left_arm_decoder = None
                self.action_right_arm_decoder = None
            
            self.action_claw_decoder = CategorySpecificMLP(
                num_categories=config.max_num_embodiments,
                input_dim=self.hidden_size,
                hidden_dim=self.hidden_size,
                output_dim=config.action_claw_dim,
            )
            self.action_decoder = None  # Not used in multi-head mode
            
            if config.split_arm_heads:
                total_dim = config.action_left_arm_dim + config.action_right_arm_dim + config.action_claw_dim
                print(f"ğŸ“Š Multi-head action: left_arm({config.action_left_arm_dim}D, indices 0-{config.action_left_arm_dim-1}) + "
                      f"right_arm({config.action_right_arm_dim}D, indices {config.action_left_arm_dim}-{config.action_left_arm_dim + config.action_right_arm_dim-1}) + "
                      f"claw({config.action_claw_dim}D, indices {config.action_arm_dim}-{config.action_arm_dim + config.action_claw_dim-1}) = {total_dim}D")
                print(f"   action_arm_dim={config.action_arm_dim} (left+right), actual_action_dim={config.action_dim}")
            else:
                print(f"ğŸ“Š Multi-head action: arm({config.action_arm_dim}D) + claw({config.action_claw_dim}D) = {config.action_arm_dim + config.action_claw_dim}D")
        else:
            self.action_decoder = CategorySpecificMLP(
                num_categories=config.max_num_embodiments,
                input_dim=self.hidden_size,
                hidden_dim=self.hidden_size,
                output_dim=self.action_dim,
            )
            self.action_arm_decoder = None
            self.action_left_arm_decoder = None
            self.action_right_arm_decoder = None
            self.action_claw_decoder = None
        
        # Learnable loss weights (å‚è€ƒ https://arxiv.org/pdf/1705.07115)
        if config.use_learnable_loss_weights and config.use_multi_action_heads:
            if config.split_arm_heads:
                self.task_log_sigma = nn.ParameterDict({
                    "left_arm": nn.Parameter(torch.zeros(())),    # log(Ïƒ_left_arm)
                    "right_arm": nn.Parameter(torch.zeros(())),   # log(Ïƒ_right_arm)
                    "claw": nn.Parameter(torch.zeros(())),        # log(Ïƒ_claw)
                })
                print(f"ğŸ¯ Learnable loss weights enabled: left_arm, right_arm, claw")
            else:
                self.task_log_sigma = nn.ParameterDict({
                    "arm": nn.Parameter(torch.zeros(())),    # log(Ïƒ_arm)
                    "claw": nn.Parameter(torch.zeros(())),  # log(Ïƒ_claw)
                })
                print(f"ğŸ¯ Learnable loss weights enabled: arm, claw")
            print(f"   Using uncertainty-based weighting from https://arxiv.org/pdf/1705.07115")
        else:
            self.task_log_sigma = None
        self.future_tokens = nn.Embedding(config.num_target_vision_tokens, self.input_embedding_dim)
        nn.init.normal_(self.future_tokens.weight, mean=0.0, std=0.02)

        self.vlln = nn.LayerNorm(config.backbone_embedding_dim) if config.use_vlln else nn.Identity()
        self.vl_self_attention = (
            SelfAttentionTransformer(**config.vl_self_attention_cfg) if config.use_vlln else nn.Identity()
        )

        if config.add_pos_embed:
            self.position_embedding = nn.Embedding(config.max_seq_len, self.input_embedding_dim)
            nn.init.normal_(self.position_embedding.weight, mean=0.0, std=0.02)

        self.beta_dist = Beta(config.noise_beta_alpha, config.noise_beta_beta)
        self.num_timestep_buckets = config.num_timestep_buckets
        self.config = config
        
        # Chain of Causation (CoC) Reasoning Head
        if config.use_coc_reasoning:
            self.reasoning_head = ReasoningHead(
                backbone_embedding_dim=config.backbone_embedding_dim,
                reasoning_hidden_dim=config.reasoning_hidden_dim,
                reasoning_vocab_size=config.reasoning_vocab_size,
                reasoning_max_length=config.reasoning_max_length,
                num_layers=config.reasoning_num_layers,
            )
            print(f"ğŸ§  Chain of Causation (CoC) Reasoning enabled:")
            print(f"   âœ… Reasoning vocab size: {config.reasoning_vocab_size}")
            print(f"   âœ… Reasoning max length: {config.reasoning_max_length}")
            print(f"   âœ… Reasoning hidden dim: {config.reasoning_hidden_dim}")
            print(f"   âœ… Reasoning conditioning: {config.reasoning_conditioning_type}")
            print(f"   âœ… Action decision types: {config.action_decision_types}")
        else:
            self.reasoning_head = None
        
        self.set_trainable_parameters(config.tune_projector, config.tune_diffusion_model)
        self.rtc_processor = rtc_processor


    def set_trainable_parameters(self, tune_projector: bool, tune_diffusion_model: bool):
        self.tune_projector = tune_projector
        self.tune_diffusion_model = tune_diffusion_model
        for p in self.parameters():
            p.requires_grad = True
        if not tune_projector:
            self.state_encoder.requires_grad_(False)
            self.action_encoder.requires_grad_(False)
            if self.config.use_multi_action_heads:
                if self.config.split_arm_heads:
                    if hasattr(self, 'shared_arm_decoder') and self.shared_arm_decoder is not None:
                        self.shared_arm_decoder.requires_grad_(False)
                    if self.action_left_arm_decoder is not None:
                        self.action_left_arm_decoder.requires_grad_(False)
                    if self.action_right_arm_decoder is not None:
                        self.action_right_arm_decoder.requires_grad_(False)
                else:
                    if self.action_arm_decoder is not None:
                        self.action_arm_decoder.requires_grad_(False)
                if self.action_claw_decoder is not None:
                    self.action_claw_decoder.requires_grad_(False)
            else:
                if self.action_decoder is not None:
                    self.action_decoder.requires_grad_(False)
            if self.config.add_pos_embed:
                self.position_embedding.requires_grad_(False)
        if not tune_diffusion_model:
            self.model.requires_grad_(False)
        
        # Handle reasoning head trainability
        if self.reasoning_head is not None:
            if not self.config.tune_reasoning_head:
                self.reasoning_head.requires_grad_(False)
                print(f"Tune reasoning head: False (frozen)")
            else:
                print(f"Tune reasoning head: True (trainable)")
        
        print(f"Tune action head projector: {self.tune_projector}")
        print(f"Tune action head diffusion model: {self.tune_diffusion_model}")
        # Check if any parameters are still trainable. If not, print a warning.
        if not tune_projector and not tune_diffusion_model:
            for name, p in self.named_parameters():
                if p.requires_grad:
                    print(f"Action head trainable parameter: {name}")
        if not any(p.requires_grad for p in self.parameters()):
            print("Warning: No action head trainable parameters found.")

    def set_frozen_modules_to_eval_mode(self):
        """
        Huggingface will call model.train() at each training_step. To ensure
        the expected behaviors for modules like dropout, batchnorm, etc., we
        need to call model.eval() for the frozen modules.
        """
        if self.training:
            if not self.tune_projector:
                self.state_encoder.eval()
                self.action_encoder.eval()
                if self.config.use_multi_action_heads:
                    if self.config.split_arm_heads:
                        if hasattr(self, 'shared_arm_decoder') and self.shared_arm_decoder is not None:
                            self.shared_arm_decoder.eval()
                        if self.action_left_arm_decoder is not None:
                            self.action_left_arm_decoder.eval()
                        if self.action_right_arm_decoder is not None:
                            self.action_right_arm_decoder.eval()
                    else:
                        if self.action_arm_decoder is not None:
                            self.action_arm_decoder.eval()
                    if self.action_claw_decoder is not None:
                        self.action_claw_decoder.eval()
                else:
                    if self.action_decoder is not None:
                        self.action_decoder.eval()
                if self.config.add_pos_embed:
                    self.position_embedding.eval()
            if not self.tune_diffusion_model:
                self.model.eval()

    def sample_time(self, batch_size, device, dtype):
        sample = self.beta_dist.sample([batch_size]).to(device, dtype=dtype)
        return (self.config.noise_s - sample) / self.config.noise_s

    def prepare_input(self, batch: dict) -> BatchFeature:
        return BatchFeature(data=batch)

    def process_backbone_output(self, backbone_output: BatchFeature) -> BatchFeature:
        """
            ç¬¬äºŒé˜¶æ®µ: Vision-Languageç‰¹å¾å¢å¼º(vl_self_attention)
            # å…³é”®æ­¥éª¤
            * 1) å¯¹å·²ç»èåˆçš„è§†è§‰-è¯­è¨€ç‰¹å¾è¿›è¡Œ4å±‚è‡ªæ³¨æ„åŠ›å¤„ç†
            * 2) è¿›ä¸€æ­¥å¼ºåŒ–è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„å…³è”
            * 3) ä¸ºåç»­çš„è·¨æ¨¡æ€æ³¨æ„åŠ›åšå‡†å¤‡
            # values:
            * backbone_features: è§†è§‰-è¯­è¨€ç‰¹å¾
            * vlln: è§†è§‰-è¯­è¨€ç‰¹å¾å½’ä¸€åŒ–
            * vl_self_attention: è§†è§‰-è¯­è¨€ç‰¹å¾è‡ªæ³¨æ„åŠ›å¤„ç†
            * backbone_output: è§†è§‰-è¯­è¨€ç‰¹å¾
            * return_dict: æ˜¯å¦è¿”å›å­—å…¸
            * return_dict: æ˜¯å¦è¿”å›å­—å…¸
        """
        backbone_features = backbone_output["backbone_features"]
        backbone_features = self.vlln(backbone_features)
        backbone_features = self.vl_self_attention(backbone_features)
        backbone_output["backbone_features"] = backbone_features
        return backbone_output

    def forward(self, backbone_output: BatchFeature, action_input: BatchFeature) -> BatchFeature:
        # Set frozen modules to eval
        self.set_frozen_modules_to_eval_mode()

        backbone_output = self.process_backbone_output(backbone_output)
        
        # Generate reasoning trace if CoC reasoning is enabled
        # æ ¹æ®è®ºæ–‡ Alpamayo-R1 (https://arxiv.org/pdf/2511.00088)ï¼ŒSFTé˜¶æ®µçš„æŸå¤±å‡½æ•°ä¸ºï¼š
        # L_SFT(Î¸) = -E_{(o, REASON, a) ~ D_CoC} [log Ï€_Î¸(REASON, a | o)]
        # è¿™åŒ…å«ä¸¤éƒ¨åˆ†ï¼š
        # 1. Reasoning traceçš„äº¤å‰ç†µæŸå¤±ï¼šlog Ï€_Î¸(REASON | o)
        # 2. Action decisionçš„äº¤å‰ç†µæŸå¤±ï¼ˆCoC-Action Consistencyï¼‰ï¼šç¡®ä¿reasoning traceå’Œactionä¹‹é—´çš„ä¸€è‡´æ€§
        reasoning_logits = None
        reasoning_conditioning = None
        action_decision_logits = None
        reasoning_trace_loss = None  # Reasoning traceçš„äº¤å‰ç†µæŸå¤±
        action_decision_loss = None  # Action decisionçš„äº¤å‰ç†µæŸå¤±ï¼ˆCoC-Action Consistencyï¼‰
        total_reasoning_loss = None  # æ€»reasoningæŸå¤± = reasoning_trace_loss + action_decision_loss
        
        if self.config.use_coc_reasoning and self.reasoning_head is not None:
            backbone_features = backbone_output.backbone_features  # (B, T, backbone_embedding_dim)
            
            # Get reasoning labels from action_input if available (for training)
            reasoning_labels = action_input.get("reasoning_labels", None) if hasattr(action_input, "get") else None
            if reasoning_labels is None and hasattr(action_input, "data"):
                reasoning_labels = action_input.data.get("reasoning_labels", None)
            
            # Get action decision labels from action_input if available (for training)
            action_decision_labels = None
            if hasattr(action_input, "get"):
                action_decision_labels = action_input.get("action_decision_labels", None)
            elif hasattr(action_input, "data"):
                action_decision_labels = action_input.data.get("action_decision_labels", None)
            
            # Generate reasoning
            # æ³¨æ„ï¼šåœ¨è®­ç»ƒæ—¶ï¼Œaction_decision_labelsä¼šè¢«ç”¨äºteacher forcingï¼Œç¡®ä¿conditioningä½¿ç”¨æ­£ç¡®çš„decision
            reasoning_logits, reasoning_conditioning, action_decision_logits = self.reasoning_head(
                backbone_features, reasoning_labels, action_decision_labels
            )
            
            # 1. è®¡ç®—Reasoning traceçš„äº¤å‰ç†µæŸå¤±
            # L_reasoning = -log Ï€_Î¸(REASON | o)
            # è¿™æ˜¯æ€ç»´é“¾reasoning traceçš„äº¤å‰ç†µæŸå¤±
            if reasoning_labels is not None and reasoning_logits is not None:
                reasoning_trace_loss = F.cross_entropy(
                    reasoning_logits.reshape(-1, reasoning_logits.shape[-1]),
                    reasoning_labels.reshape(-1),
                    ignore_index=-100,  # Ignore padding tokens
                    reduction="mean"
                )
            
            # 2. è®¡ç®—Action decisionçš„äº¤å‰ç†µæŸå¤±ï¼ˆCoC-Action Consistencyï¼‰
            # L_action_decision = -log Ï€_Î¸(action_decision | o)
            # è¿™æ˜¯åŠ¨ä½œä¸€è‡´æ€§å¥–åŠ±ï¼Œç¡®ä¿reasoning traceé¢„æµ‹çš„action decisionä¸ground truthä¸€è‡´
            # è¿™æ˜¯CoC-Action Consistencyçš„å…³é”®ç»„æˆéƒ¨åˆ†
            # æ³¨æ„ï¼šaction_decision_labelså·²ç»åœ¨ä¸Šé¢è·å–è¿‡äº†ï¼ˆç¬¬933-937è¡Œï¼‰ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
            if action_decision_labels is not None and action_decision_logits is not None:
                action_decision_loss = F.cross_entropy(
                    action_decision_logits,
                    action_decision_labels,
                    reduction="mean"
                )
            
            # 3. æ€»reasoningæŸå¤± = reasoning traceæŸå¤± + action decisionæŸå¤±
            # è¿™å®ç°äº†è®ºæ–‡ä¸­çš„ L_SFT(Î¸) = -E[log Ï€_Î¸(REASON, a | o)]
            if reasoning_trace_loss is not None and action_decision_loss is not None:
                total_reasoning_loss = reasoning_trace_loss + action_decision_loss
            elif reasoning_trace_loss is not None:
                total_reasoning_loss = reasoning_trace_loss
            elif action_decision_loss is not None:
                total_reasoning_loss = action_decision_loss

        if self.config.expand_batch is not None:
            for k, v in backbone_output.items():
                ndim = len(v.shape)
                factors = [self.config.expand_batch]
                while len(factors) < ndim:
                    factors.append(1)
                factors = tuple(factors)
                expanded = v.repeat(*factors)
                backbone_output[k] = expanded

            for k, v in action_input.items():
                ndim = len(v.shape)
                factors = [self.config.expand_batch]
                while len(factors) < ndim:
                    factors.append(1)
                factors = tuple(factors)
                expanded = v.repeat(*factors)
                action_input[k] = expanded

        # Get vision and language embeddings.
        vl_embs = backbone_output.backbone_features
        device = vl_embs.device

        # Get embodiment ID.
        embodiment_id = action_input.embodiment_id

        # Embed state.
        state_features = self.state_encoder(action_input.state, embodiment_id)

        # Embed noised action trajectory.
        # NOTE: Processor (GrootPackInputsStep) already pads action to max_action_dim (32)
        # So action_input.action is already (B, T, encoder_action_dim=32)
        
        # 1) è·å–çœŸå®çš„ action (ground truth)
        actions = action_input.action  # (B, T, encoder_action_dim)
        action_mask = action_input.action_mask  # (B, T, encoder_action_dim) - marks valid dimensions
        
        # Ensure actions match encoder_action_dim (should already be padded by processor)
        if actions.shape[-1] != self.encoder_action_dim:
            if actions.shape[-1] < self.encoder_action_dim:
                # Pad if needed (shouldn't happen if processor works correctly)
                pad_size = self.encoder_action_dim - actions.shape[-1]
                padding = torch.zeros(
                    (actions.shape[0], actions.shape[1], pad_size),
                    device=actions.device,
                    dtype=actions.dtype
                )
                actions = torch.cat([actions, padding], dim=-1)
            else:
                # Truncate if larger (shouldn't happen)
                actions = actions[:, :, :self.encoder_action_dim]
        # 2) ç”Ÿæˆéšæœºå™ªå£°
        noise = torch.randn(actions.shape, device=actions.device, dtype=actions.dtype)
        # 3) éšæœºé‡‡æ ·æ—¶é—´æ­¥ t âˆˆ [0, 1]
        t = self.sample_time(actions.shape[0], device=actions.device, dtype=actions.dtype)
        t = t[:, None, None]  # shape (B,1,1) for broadcast
        # 4) åˆ›å»ºåŠ å™ªè½¨è¿¹ï¼ˆFlow Matching æ ¸å¿ƒï¼‰
        # å½“ t=0ï¼šçº¯å™ªå£°
        # å½“ t=1ï¼šçœŸå® action
        noisy_trajectory = (1 - t) * noise + t * actions
        
        # For velocity, extract only the actual action dimensions (first actual_action_dim)
        # This matches the original data dimension before padding
        velocity = actions[:, :, :self.actual_action_dim] - noise[:, :, :self.actual_action_dim]

        # Convert (continuous) t -> discrete if needed
        t_discretized = (t[:, 0, 0] * self.num_timestep_buckets).long()
        # 5) ç¼–ç åŠ å™ªè½¨è¿¹ä¸º action_features
        action_features = self.action_encoder(noisy_trajectory, t_discretized, embodiment_id)

        # Maybe add position embedding.
        if self.config.add_pos_embed:
            pos_ids = torch.arange(action_features.shape[1], dtype=torch.long, device=device)
            pos_embs = self.position_embedding(pos_ids).unsqueeze(0)
            action_features = action_features + pos_embs

        # Join vision, language, state and action embedding along sequence dimension.
        """
            ç¬¬ä¸‰é˜¶æ®µ: Vision-Languageä¸State-Actionèåˆ(DiT Cross-Attention)
            # å…³é”®æ­¥éª¤
            * 1) å°†è§†è§‰-è¯­è¨€ç‰¹å¾å’ŒçŠ¶æ€-åŠ¨ä½œç‰¹å¾æ‹¼æ¥åœ¨ä¸€èµ·
            * 2) é€šè¿‡DiTçš„Cross-Attentionæœºåˆ¶, è®©è§†è§‰-è¯­è¨€ç‰¹å¾å’ŒçŠ¶æ€-åŠ¨ä½œç‰¹å¾ç›¸äº’å…³æ³¨
            * 3) è¾“å‡º: çŠ¶æ€-åŠ¨ä½œç‰¹å¾
            
            # values:
            * future_tokens: æœªæ¥tokens
            * vl_embs: è§†è§‰-è¯­è¨€ç‰¹å¾ # Key/Value
            * sa_embs: çŠ¶æ€-åŠ¨ä½œç‰¹å¾ # Query
            * vl_attn_mask: è§†è§‰-è¯­è¨€ç‰¹å¾çš„æ³¨æ„åŠ›æ©ç 
            * model_output: æ¨¡å‹è¾“å‡º
            * return_dict: æ˜¯å¦è¿”å›å­—å…¸
            * return_dict: æ˜¯å¦è¿”å›å­—å…¸
        """
        future_tokens = self.future_tokens.weight.unsqueeze(0).expand(vl_embs.shape[0], -1, -1)
        # 6) æ‹¼æ¥ä¸º hidden_states
        sa_embs = torch.cat((state_features, future_tokens, action_features), dim=1)

        vl_attn_mask = backbone_output.backbone_attention_mask

        # 7) DiT Cross-Attention
        # æ³¨æ„ï¼šå¦‚æœreasoning_conditioning_type="dit"æˆ–"both"ï¼Œå¯ä»¥åœ¨DiTè¾“å…¥å‰åº”ç”¨conditioning
        # ä½†ç›®å‰DiTæ¥å£ä¸æ”¯æŒreasoning_conditioningå‚æ•°ï¼Œæ‰€ä»¥åªåœ¨decoderè¾“å…¥å‰åº”ç”¨
        # è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºconditioningåœ¨decoderè¾“å…¥å‰åº”ç”¨ä¹Ÿèƒ½æœ‰æ•ˆå¼•å¯¼åŠ¨ä½œç”Ÿæˆ
        model_output = self.model(
            hidden_states=sa_embs,
            encoder_hidden_states=vl_embs,
            encoder_attention_mask=vl_attn_mask,
            timestep=t_discretized,
            return_all_hidden_states=False,  # NOTE (YL): not using flare now
        )
        # 8. é¢„æµ‹ velocity
        # pred_velocity = self.action_decoder(model_output)
        # 9. è®¡ç®—æŸå¤±
        # loss = MSE(pred_velocity, actions - noise)
        
        # Slice out only the action portion of model output
        model_output_actions = model_output[:, -actions.shape[1] :]
        
        # Apply reasoning conditioning to model_output_actions if reasoning is enabled
        # å…³é”®ï¼šreasoning_conditioningå·²ç»èåˆäº†action decisionçš„ä¿¡æ¯ï¼ˆåœ¨ReasoningHeadä¸­ï¼‰
        # è¿™ç¡®ä¿äº†action decisionèƒ½å¤ŸçœŸæ­£å¼•å¯¼DiTçš„åŠ¨ä½œç”Ÿæˆæ–¹å‘
        # 
        # å®Œæ•´é“¾è·¯ï¼š
        # 1. backbone_features â†’ reasoning trace (æ€ç»´é“¾)
        # 2. reasoning trace â†’ action decision (åŠ¨ä½œå†³ç­–)
        # 3. action decision â†’ action_decision_embedding (å†³ç­–åµŒå…¥)
        # 4. action_decision_embedding + base_conditioning â†’ reasoning_conditioning (èåˆçš„æ¡ä»¶å‘é‡)
        # 5. reasoning_conditioning â†’ æŠ•å½±åˆ°decoderç»´åº¦ â†’ æ®‹å·®è¿æ¥åˆ°model_output_actions
        # 6. æ¡ä»¶åŒ–çš„model_output_actions â†’ decoder â†’ åŠ¨ä½œé¢„æµ‹
        #
        # ä¾‹å¦‚ï¼šå¦‚æœaction decisionæ˜¯"left_search_grasp_pull"ï¼š
        # - action_decision_embeddingä¼šç¼–ç "å·¦æ‰‹æœç´¢æŠ“å–æ‹‰å¼€ï¼Œå³æ‰‹ä¸åŠ¨"çš„ä¿¡æ¯
        # - è¿™ä¸ªembeddingä¼šé€šè¿‡æ®‹å·®è¿æ¥åç½®model_output_actions
        # - æœ€ç»ˆdecoderä¼šç”Ÿæˆåç½®å·¦æ‰‹åŠ¨ä½œï¼ˆæœç´¢ã€æŠ“å–ã€æ‹‰å¼€ï¼‰ï¼Œå³æ‰‹ä¿æŒé™æ­¢çš„åŠ¨ä½œ
        if self.config.use_coc_reasoning and reasoning_conditioning is not None:
            # Project reasoning conditioning to match model_output_actions dimension
            # Use a cached projection layer if available, otherwise create one
            if not hasattr(self, '_reasoning_proj'):
                self._reasoning_proj = nn.Linear(
                    self.config.reasoning_hidden_dim, 
                    model_output_actions.shape[-1]
                ).to(model_output_actions.device)
            # æŠ•å½±å¹¶æ‰©å±•ç»´åº¦ï¼šreasoning_conditioning (B, reasoning_hidden_dim) 
            # â†’ (B, hidden_size) â†’ (B, 1, hidden_size)
            # ç„¶åé€šè¿‡å¹¿æ’­è‡ªåŠ¨æ‰©å±•åˆ° (B, T, hidden_size)
            reasoning_cond_expanded = self._reasoning_proj(reasoning_conditioning).unsqueeze(1)  # (B, 1, hidden_size)
            
            # Add reasoning conditioning to model output (residual connection)
            # This biases the action generation towards the reasoning decision
            # æ³¨æ„ï¼šç›®å‰åªåœ¨decoderè¾“å…¥å‰åº”ç”¨ï¼ˆreasoning_conditioning_type="decoder"æˆ–"both"ï¼‰
            # å¦‚æœè®¾ç½®ä¸º"dit"ï¼Œéœ€è¦åœ¨DiTå†…éƒ¨åº”ç”¨ï¼Œä½†è¿™éœ€è¦ä¿®æ”¹DiTæ¥å£
            if self.config.reasoning_conditioning_type in ["decoder", "both"]:
                model_output_actions = model_output_actions + reasoning_cond_expanded  # (B, T, hidden_size)
        
        # Multi-head action prediction
        if self.config.use_multi_action_heads:
            if self.config.split_arm_heads:
                # Split arm into left and right
                if self.config.use_shared_arm_features and hasattr(self, 'shared_arm_decoder') and self.shared_arm_decoder is not None:
                    # ä½¿ç”¨å…±äº«åº•å±‚ç‰¹å¾çš„decoder
                    pred_left_arm, pred_right_arm = self.shared_arm_decoder(model_output_actions, embodiment_id)
                else:
                    # ä½¿ç”¨ç‹¬ç«‹çš„decoder
                    pred_left_arm = self.action_left_arm_decoder(model_output_actions, embodiment_id)
                    pred_right_arm = self.action_right_arm_decoder(model_output_actions, embodiment_id)
                pred_claw = self.action_claw_decoder(model_output_actions, embodiment_id)
                pred_actions = torch.cat([pred_left_arm, pred_right_arm, pred_claw], dim=-1)  # (B, T, action_dim)
                
                # Split ground truth velocity into corresponding parts
                # velocity shape: (B, T, actual_action_dim=16)
                # Structure: [left_arm(0-6, 7D), right_arm(7-13, 7D), claw(14-15, 2D)]
                velocity_left_arm = velocity[:, :, :self.config.action_left_arm_dim]  # (B, T, 7) - indices 0-6
                velocity_right_arm = velocity[:, :, self.config.action_left_arm_dim:self.config.action_left_arm_dim + self.config.action_right_arm_dim]  # (B, T, 7) - indices 7-13
                velocity_claw = velocity[:, :, self.config.action_arm_dim:]  # (B, T, 2) - indices 14-15
                
                # Compute loss for each head
                # action_mask shape: (B, T, encoder_action_dim), extract only actual_action_dim
                action_mask = action_input.action_mask[:, :, :self.actual_action_dim]  # (B, T, 16)
                # Split mask for left_arm, right_arm and claw (same structure as velocity)
                action_mask_left_arm = action_mask[:, :, :self.config.action_left_arm_dim]  # (B, T, 7) - indices 0-6
                action_mask_right_arm = action_mask[:, :, self.config.action_left_arm_dim:self.config.action_left_arm_dim + self.config.action_right_arm_dim]  # (B, T, 7) - indices 7-13
                action_mask_claw = action_mask[:, :, self.config.action_arm_dim:]  # (B, T, 2) - indices 14-15
                
                loss_left_arm = F.mse_loss(pred_left_arm, velocity_left_arm, reduction="none") * action_mask_left_arm
                loss_right_arm = F.mse_loss(pred_right_arm, velocity_right_arm, reduction="none") * action_mask_right_arm
                loss_claw = F.mse_loss(pred_claw, velocity_claw, reduction="none") * action_mask_claw
                
                # åè°ƒæ€§æŸå¤±ï¼šé¼“åŠ±å·¦å³æ‰‹åŠ¨ä½œçš„åè°ƒæ€§ï¼ˆå¯é€‰ï¼‰
                coordination_loss = None
                if self.config.arm_coordination_loss_weight > 0:
                    # è®¡ç®—å·¦å³æ‰‹é€Ÿåº¦çš„å·®å¼‚ï¼Œé¼“åŠ±å®ƒä»¬åœ¨æŸäº›ç»´åº¦ä¸Šä¿æŒåŒæ­¥
                    # è¿™é‡Œä½¿ç”¨é€Ÿåº¦å·®çš„L2èŒƒæ•°ä½œä¸ºåè°ƒæ€§æŸå¤±
                    # æ³¨æ„ï¼šä¸æ˜¯å®Œå…¨åŒæ­¥ï¼Œè€Œæ˜¯é¼“åŠ±åè°ƒï¼ˆæ¯”å¦‚æ‹‰ç®±å­æ—¶å·¦å³æ‰‹åº”è¯¥åŒæ­¥ï¼‰
                    left_arm_magnitude = torch.norm(pred_left_arm, dim=-1, keepdim=True)  # (B, T, 1)
                    right_arm_magnitude = torch.norm(pred_right_arm, dim=-1, keepdim=True)  # (B, T, 1)
                    # é¼“åŠ±å·¦å³æ‰‹çš„é€Ÿåº¦å¹…åº¦ç›¸ä¼¼ï¼ˆä½†ä¸å®Œå…¨ç›¸åŒï¼‰
                    coordination_loss = F.mse_loss(left_arm_magnitude, right_arm_magnitude, reduction="none")
                    # åªå¯¹æœ‰æ•ˆçš„åŠ¨ä½œç»´åº¦è®¡ç®—
                    valid_mask = (action_mask_left_arm.sum(dim=-1, keepdim=True) > 0) & (action_mask_right_arm.sum(dim=-1, keepdim=True) > 0)
                    coordination_loss = (coordination_loss * valid_mask).sum() / (valid_mask.sum() + 1e-8)
                
                # Use learnable weights or fixed weights
                if self.config.use_learnable_loss_weights and self.task_log_sigma is not None:
                    loss_left_arm_mean = loss_left_arm.sum() / action_mask_left_arm.sum()
                    loss_right_arm_mean = loss_right_arm.sum() / action_mask_right_arm.sum()
                    loss_claw_mean = loss_claw.sum() / action_mask_claw.sum()
                    
                    s_left_arm = self.task_log_sigma["left_arm"]
                    s_right_arm = self.task_log_sigma["right_arm"]
                    s_claw = self.task_log_sigma["claw"]
                    precision_left_arm = torch.exp(-2.0 * s_left_arm)
                    precision_right_arm = torch.exp(-2.0 * s_right_arm)
                    precision_claw = torch.exp(-2.0 * s_claw)
                    
                    loss = precision_left_arm * loss_left_arm_mean + precision_right_arm * loss_right_arm_mean + precision_claw * loss_claw_mean + s_left_arm + s_right_arm + s_claw
                    
                    # æ·»åŠ åè°ƒæ€§æŸå¤±
                    if coordination_loss is not None:
                        loss = loss + self.config.arm_coordination_loss_weight * coordination_loss
                    
                    output_dict = {
                        "loss": loss,
                        "left_arm_loss": loss_left_arm_mean.item(),
                        "right_arm_loss": loss_right_arm_mean.item(),
                        "claw_loss": loss_claw_mean.item(),
                        "sigma_left_arm": torch.exp(s_left_arm).item(),
                        "sigma_right_arm": torch.exp(s_right_arm).item(),
                        "sigma_claw": torch.exp(s_claw).item(),
                        "weight_left_arm": precision_left_arm.item(),
                        "weight_right_arm": precision_right_arm.item(),
                        "weight_claw": precision_claw.item(),
                    }
                    if coordination_loss is not None:
                        output_dict["arm_coordination_loss"] = coordination_loss.item()
                else:
                    # Use fixed weights
                    loss_left_arm_mean = loss_left_arm.sum() / action_mask_left_arm.sum()
                    loss_right_arm_mean = loss_right_arm.sum() / action_mask_right_arm.sum()
                    loss_claw_mean = loss_claw.sum() / action_mask_claw.sum()
                    loss = self.config.left_arm_loss_weight * loss_left_arm_mean + self.config.right_arm_loss_weight * loss_right_arm_mean + self.config.claw_loss_weight * loss_claw_mean
                    
                    # æ·»åŠ åè°ƒæ€§æŸå¤±
                    if coordination_loss is not None:
                        loss = loss + self.config.arm_coordination_loss_weight * coordination_loss
                    
                    output_dict = {
                        "loss": loss,
                        "left_arm_loss": loss_left_arm_mean.item(),
                        "right_arm_loss": loss_right_arm_mean.item(),
                        "claw_loss": loss_claw_mean.item(),
                    }
                    if coordination_loss is not None:
                        output_dict["arm_coordination_loss"] = coordination_loss.item()
            else:
                # Single arm head (original behavior)
                pred_arm = self.action_arm_decoder(model_output_actions, embodiment_id)
                pred_claw = self.action_claw_decoder(model_output_actions, embodiment_id)
                pred_actions = torch.cat([pred_arm, pred_claw], dim=-1)  # (B, T, action_dim)
                
                # Split ground truth velocity into corresponding parts
                velocity_arm = velocity[:, :, :self.config.action_arm_dim]  # (B, T, action_arm_dim)
                velocity_claw = velocity[:, :, self.config.action_arm_dim:]  # (B, T, action_claw_dim)
                
                # Compute loss for each head
                action_mask = action_input.action_mask[:, :, :self.actual_action_dim]  # (B, T, actual_action_dim)
                # Split mask for arm and claw
                action_mask_arm = action_mask[:, :, :self.config.action_arm_dim]  # (B, T, action_arm_dim)
                action_mask_claw = action_mask[:, :, self.config.action_arm_dim:]  # (B, T, action_claw_dim)
                
                loss_arm = F.mse_loss(pred_arm, velocity_arm, reduction="none") * action_mask_arm
                loss_claw = F.mse_loss(pred_claw, velocity_claw, reduction="none") * action_mask_claw
                
                # Use learnable weights or fixed weights
                if self.config.use_learnable_loss_weights and self.task_log_sigma is not None:
                    loss_arm_mean = loss_arm.sum() / action_mask_arm.sum()
                    loss_claw_mean = loss_claw.sum() / action_mask_claw.sum()
                    
                    s_arm = self.task_log_sigma["arm"]
                    s_claw = self.task_log_sigma["claw"]
                    precision_arm = torch.exp(-2.0 * s_arm)  # 1 / ÏƒÂ²
                    precision_claw = torch.exp(-2.0 * s_claw)
                    
                    loss = precision_arm * loss_arm_mean + precision_claw * loss_claw_mean + s_arm + s_claw
                    
                    output_dict = {
                        "loss": loss,
                        "arm_loss": loss_arm_mean.item(),
                        "claw_loss": loss_claw_mean.item(),
                        "sigma_arm": torch.exp(s_arm).item(),
                        "sigma_claw": torch.exp(s_claw).item(),
                        "weight_arm": precision_arm.item(),
                        "weight_claw": precision_claw.item(),
                    }
                else:
                    # Use fixed weights
                    loss_arm_mean = loss_arm.sum() / action_mask_arm.sum()
                    loss_claw_mean = loss_claw.sum() / action_mask_claw.sum()
                    loss = self.config.arm_loss_weight * loss_arm_mean + self.config.claw_loss_weight * loss_claw_mean
                    
                    output_dict = {
                        "loss": loss,
                        "arm_loss": loss_arm_mean.item(),
                        "claw_loss": loss_claw_mean.item(),
                    }
        else:
            # Single head (original behavior)
            pred = self.action_decoder(model_output_actions, embodiment_id)
            pred_actions = pred
            
            # Slice out only the action portion of pred and target.
            action_mask = action_input.action_mask
            loss = F.mse_loss(pred_actions, velocity, reduction="none") * action_mask
            loss = loss.sum() / action_mask.sum()
            output_dict = {
                "loss": loss,
            }
        
        # Add reasoning loss to total loss
        # æ ¹æ®è®ºæ–‡ï¼Œæ€»æŸå¤± = åŠ¨ä½œé¢„æµ‹æŸå¤± + reasoning_loss_weight * (reasoning_trace_loss + action_decision_loss)
        # è¿™å®ç°äº† L_SFT(Î¸) = -E[log Ï€_Î¸(REASON, a | o)]
        if total_reasoning_loss is not None:
            total_loss = output_dict["loss"] + self.config.reasoning_loss_weight * total_reasoning_loss
            output_dict["loss"] = total_loss
            
            # åˆ†åˆ«è®°å½•å„ä¸ªæŸå¤±é¡¹ï¼Œä¾¿äºç›‘æ§å’Œè°ƒè¯•
            if reasoning_trace_loss is not None:
                output_dict["reasoning_trace_loss"] = reasoning_trace_loss.item() if isinstance(reasoning_trace_loss, torch.Tensor) else reasoning_trace_loss
            if action_decision_loss is not None:
                output_dict["action_decision_loss"] = action_decision_loss.item() if isinstance(action_decision_loss, torch.Tensor) else action_decision_loss
                # è®°å½•CoC-Action ConsistencyæŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
                output_dict["coc_action_consistency_loss"] = output_dict["action_decision_loss"]
            
            # æ€»reasoningæŸå¤±ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
            output_dict["reasoning_loss"] = total_reasoning_loss.item() if isinstance(total_reasoning_loss, torch.Tensor) else total_reasoning_loss
            
            # Add action decision prediction for monitoring
            if action_decision_logits is not None:
                # Get predicted action decision
                predicted_decision = torch.argmax(action_decision_logits, dim=-1)  # (B,)
                output_dict["predicted_action_decision"] = predicted_decision.cpu().numpy().tolist()
        
        return BatchFeature(data=output_dict)

    @torch.no_grad()
    def get_action(self, backbone_output: BatchFeature, action_input: BatchFeature, rtc_enabled: bool, **kwargs) -> BatchFeature:
        backbone_output = self.process_backbone_output(backbone_output)
        
        # Generate reasoning trace if CoC reasoning is enabled
        reasoning_conditioning = None
        action_decision_logits = None
        
        if self.config.use_coc_reasoning and self.reasoning_head is not None:
            backbone_features = backbone_output.backbone_features  # (B, T, backbone_embedding_dim)
            
            # Generate reasoning (inference mode, no labels)
            _, reasoning_conditioning, action_decision_logits = self.reasoning_head(
                backbone_features, reasoning_labels=None
            )
            
            # Get predicted action decision
            if action_decision_logits is not None:
                predicted_decision_idx = torch.argmax(action_decision_logits, dim=-1)  # (B,)
                decision_map = {
                    0: "left_search_grasp_pull",
                    1: "left_hold_right_search_grasp",
                    2: "right_search_grasp_pull",
                    3: "right_hold_left_search_grasp",
                    4: "both_search_grasp",
                    5: "both_hold_lift",
                }
                predicted_decision = [decision_map[idx.item()] for idx in predicted_decision_idx]
                print(f"ğŸ§  Predicted action decision: {predicted_decision}")

        # Get vision and language embeddings.
        vl_embs = backbone_output.backbone_features
        embodiment_id = action_input.embodiment_id

        # Embed state.
        state_features = self.state_encoder(action_input.state, embodiment_id)

        # Set initial actions as the sampled noise.
        # Use encoder_action_dim for internal processing (compatible with pretrained model)
        batch_size = vl_embs.shape[0]
        device = vl_embs.device
        # 1. åˆå§‹åŒ–ï¼šä»éšæœºå™ªå£°å¼€å§‹
        actions = torch.randn(
            size=(batch_size, self.config.action_horizon, self.encoder_action_dim),
            dtype=vl_embs.dtype,
            device=device,
        )
        # Zero out padded dimensions to match training behavior
        # In training, padded dimensions (after actual_action_dim) are always 0
        if self.encoder_action_dim != self.actual_action_dim:
            actions[:, :, self.actual_action_dim:] = 0.0

        x_t = actions

        num_steps = self.num_inference_timesteps
        dt = 1.0 / num_steps
        # 2. è¿­ä»£å»å™ªï¼ˆä¾‹å¦‚ 4 æ­¥ï¼‰
        for t in range(num_steps):
            t_cont = t / float(num_steps)  # e.g. goes 0, 1/N, 2/N, ...
            t_discretized = int(t_cont * self.num_timestep_buckets)

            def denoise_step_partial_call(input_x_t, current_timestep=t_discretized, state_features=state_features, vl_embs=vl_embs, embodiment_id=embodiment_id, reasoning_conditioning=reasoning_conditioning):
                return self.denoise_step(x_t=input_x_t, timestep=current_timestep, vl_embs=vl_embs, state_features=state_features, embodiment_id=embodiment_id, reasoning_conditioning=reasoning_conditioning)

            if rtc_enabled:
                inference_delay = kwargs.get("inference_delay")
                prev_chunk_left_over = kwargs.get("prev_chunk_left_over")
                execution_horizon = kwargs.get("execution_horizon")

                v_t = self.rtc_processor.denoise_step(
                    x_t=x_t,
                    prev_chunk_left_over=prev_chunk_left_over,
                    inference_delay=inference_delay,
                    time=t_discretized,
                    original_denoise_step_partial=denoise_step_partial_call,
                    execution_horizon=execution_horizon,
                )
            else:
                v_t = denoise_step_partial_call(x_t)
            # v_t = denoise_step_partial_call(x_t)

            x_t = x_t + dt * v_t

            if self.encoder_action_dim != self.actual_action_dim:
                x_t[:, :, self.actual_action_dim:] = 0.0

            # # Record x_t and v_t after Euler step
            # if self.rtc_processor is not None and self.rtc_processor.is_debug_enabled():
            #     self.rtc_processor.track(time=time, x_t=x_t, v_t=v_t)
        # 3. è¿”å›æœ€ç»ˆç”Ÿæˆçš„ action
        actions_output = x_t[:, :, :self.actual_action_dim]
        return BatchFeature(data={"action_pred": actions_output})

    def denoise_step(self, x_t: torch.Tensor, timestep, vl_embs, state_features, embodiment_id, reasoning_conditioning=None) -> torch.Tensor:
        """
        å•æ­¥é¢„æµ‹ velocity
        """
        # å•æ­¥è°ƒç”¨ _predict_velocity
        batch_size = x_t.shape[0]
        # timesteps_tensor = torch.full(size=(batch_size,), fill_value=timestep.item(), device=x_t.device)
        timesteps_tensor = torch.full(size=(batch_size,), fill_value=timestep, device=x_t.device)
        v_t = self._predict_velocity(vl_embs, state_features, x_t, timesteps_tensor, embodiment_id, reasoning_conditioning=reasoning_conditioning)
        return v_t

    def _predict_velocity(
            self,
            vl_embs: torch.Tensor,
            state_features: torch.Tensor,
            actions: torch.Tensor,
            timesteps_tensor: torch.Tensor,
            embodiment_id: torch.Tensor,
            reasoning_conditioning: torch.Tensor | None = None,
        ) -> torch.Tensor:
            """v_pi(A, o, tau) in the RTC paper: predicts velocity field for the current action chunk."""
            action_features = self.action_encoder(actions, timesteps_tensor, embodiment_id)
            if self.config.add_pos_embed:
                pos_ids = torch.arange(action_features.shape[1], dtype=torch.long, device=actions.device)
                pos_embs = self.position_embedding(pos_ids).unsqueeze(0)
                action_features = action_features + pos_embs

            future_tokens = self.future_tokens.weight.unsqueeze(0).expand(vl_embs.shape[0], -1, -1)
            sa_embs = torch.cat((state_features, future_tokens, action_features), dim=1)

            model_output = self.model(
                hidden_states=sa_embs,
                encoder_hidden_states=vl_embs,
                timestep=timesteps_tensor,
            )
            model_output_actions = model_output[:, -self.action_horizon :]
            
            # Apply reasoning conditioning to model_output_actions if reasoning is enabled
            if self.config.use_coc_reasoning and reasoning_conditioning is not None:
                # Project reasoning conditioning to match model_output_actions dimension
                # Use a cached projection layer if available, otherwise create one
                if not hasattr(self, '_reasoning_proj'):
                    self._reasoning_proj = nn.Linear(
                        self.config.reasoning_hidden_dim, 
                        model_output_actions.shape[-1]
                    ).to(model_output_actions.device)
                reasoning_cond_expanded = self._reasoning_proj(reasoning_conditioning).unsqueeze(1)  # (B, 1, hidden_size)
                
                # Add reasoning conditioning to model output (residual connection)
                # This biases the action generation towards the reasoning decision
                if self.config.reasoning_conditioning_type in ["decoder", "both"]:
                    model_output_actions = model_output_actions + reasoning_cond_expanded

            if self.config.use_multi_action_heads:
                if self.config.split_arm_heads:
                    # Split arm into left and right
                    if self.config.use_shared_arm_features and hasattr(self, 'shared_arm_decoder') and self.shared_arm_decoder is not None:
                        # ä½¿ç”¨å…±äº«åº•å±‚ç‰¹å¾çš„decoder
                        pred_left_arm, pred_right_arm = self.shared_arm_decoder(model_output_actions, embodiment_id)
                    else:
                        # ä½¿ç”¨ç‹¬ç«‹çš„decoder
                        pred_left_arm = self.action_left_arm_decoder(model_output_actions, embodiment_id)
                        pred_right_arm = self.action_right_arm_decoder(model_output_actions, embodiment_id)
                    pred_claw = self.action_claw_decoder(model_output_actions, embodiment_id)
                    pred_velocity = torch.cat([pred_left_arm, pred_right_arm, pred_claw], dim=-1)  # (B, T, action_dim)
                else:
                    # Single arm head
                    pred_arm = self.action_arm_decoder(model_output_actions, embodiment_id)
                    pred_claw = self.action_claw_decoder(model_output_actions, embodiment_id)
                    pred_velocity = torch.cat([pred_arm, pred_claw], dim=-1)  # (B, T, action_dim)
            else:
                pred_velocity = self.action_decoder(model_output_actions, embodiment_id)  # (B, T, action_dim)

            # Pad/truncate to encoder_action_dim so the action_encoder input format stays consistent.
            if self.encoder_action_dim != self.actual_action_dim:
                if self.encoder_action_dim > self.actual_action_dim:
                    pad_size = self.encoder_action_dim - self.actual_action_dim
                    padding = torch.zeros(
                        (pred_velocity.shape[0], pred_velocity.shape[1], pad_size),
                        device=pred_velocity.device,
                        dtype=pred_velocity.dtype,
                    )
                    pred_velocity = torch.cat([pred_velocity, padding], dim=-1)
                else:
                    pred_velocity = pred_velocity[:, :, : self.encoder_action_dim]

            return pred_velocity

    @property
    def device(self):
        return next(iter(self.parameters())).device

    @property
    def dtype(self):
        return next(iter(self.parameters())).dtype
