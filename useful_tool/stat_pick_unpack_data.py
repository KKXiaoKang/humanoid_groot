#!/usr/bin/env python3
"""
ç»Ÿè®¡æ•°æ®é›†ä¸­4311ã€4611ã€4322ä¸‰ç±»ç®±ä½“çš„æ•°æ®æ¡ç›®æ•°
ç›´æ¥è¯»å–parquetæ–‡ä»¶ï¼Œåªè¯»å–task_indexåˆ—ï¼Œä¸åŠ è½½å›¾ç‰‡æ•°æ®
æ”¯æŒå¯¹"Pick up"ä»»åŠ¡è¿›è¡Œé™é‡‡æ ·
"""

import sys
import argparse
import random
from pathlib import Path
import re
from collections import defaultdict

try:
    import pyarrow.parquet as pq
    import pyarrow as pa
except ImportError:
    print("éœ€è¦å®‰è£… pyarrow: pip install pyarrow")
    sys.exit(1)

# æ·»åŠ srcè·¯å¾„åˆ°sys.path
sys.path.insert(0, str(Path(__file__).parent.parent))

from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.dataset_tools import delete_episodes



# ============================================================================
# ç»Ÿè®¡ä¸‰ä¸ªæ•°æ®é›†ä¸­çš„ä»»åŠ¡ç±»å‹
# ============================================================================

# ä¸‰ä¸ªæ•°æ®é›†è·¯å¾„
DATASET_PATHS = [
    Path("/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1215_5w_groot_4311_4322_4611_4633_downsample"),
    Path("/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1221_5w_random_height_4322_4611_downsample"),
    Path("/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1223_5w_dense_stacking_downsample"),
]

def find_pickup_episodes(dataset_paths):
    """
    æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"Pick up"ä»»åŠ¡çš„episode
    
    Returns:
        dict: {dataset_path: [episode_indices]}
    """
    pattern_pickup = re.compile(r'Pick up', re.IGNORECASE)
    pickup_episodes = {}
    
    for dataset_path in dataset_paths:
        dataset_name = dataset_path.name
        print(f"\næŸ¥æ‰¾æ•°æ®é›† {dataset_name} ä¸­çš„'Pick up'ä»»åŠ¡...")
        
        pickup_indices = []
        episodes_dir = dataset_path / "meta" / "episodes"
        if not episodes_dir.exists():
            print(f"  è­¦å‘Š: {episodes_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            pickup_episodes[dataset_path] = []
            continue
        
        episodes_files = sorted(episodes_dir.glob("**/*.parquet"))
        print(f"  æ‰¾åˆ° {len(episodes_files)} ä¸ªepisodesæ–‡ä»¶")
        
        for episodes_file in episodes_files:
            try:
                table = pq.read_table(episodes_file)
                df = table.to_pandas()
                
                for _, row in df.iterrows():
                    episode_index = row.get('episode_index', None)
                    if episode_index is None:
                        continue
                    
                    tasks = row.get('tasks', [])
                    
                    # å¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
                    task_list = []
                    if isinstance(tasks, (list, tuple)):
                        task_list = list(tasks)
                    elif hasattr(tasks, '__iter__') and not isinstance(tasks, str):
                        try:
                            task_list = list(tasks)
                        except:
                            task_list = [str(tasks)]
                    elif isinstance(tasks, str):
                        try:
                            import ast
                            parsed = ast.literal_eval(tasks)
                            if isinstance(parsed, (list, tuple)):
                                task_list = list(parsed)
                            else:
                                task_list = [parsed]
                        except:
                            task_list = [tasks]
                    else:
                        task_list = [str(tasks)]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«"Pick up"ä»»åŠ¡
                    has_pickup = False
                    for task in task_list:
                        if isinstance(task, str) and pattern_pickup.search(task):
                            has_pickup = True
                            break
                    
                    if has_pickup:
                        pickup_indices.append(int(episode_index))
            
            except Exception as e:
                print(f"  è¯»å–æ–‡ä»¶ {episodes_file} æ—¶å‡ºé”™: {e}")
                continue
        
        pickup_episodes[dataset_path] = sorted(set(pickup_indices))
        print(f"  æ‰¾åˆ° {len(pickup_episodes[dataset_path])} ä¸ªåŒ…å«'Pick up'ä»»åŠ¡çš„episode")
    
    return pickup_episodes

def downsample_pickup_episodes(dataset_paths, target_count=850, random_seed=42):
    """
    å¯¹"Pick up"ä»»åŠ¡è¿›è¡Œé™é‡‡æ ·
    
    Args:
        dataset_paths: æ•°æ®é›†è·¯å¾„åˆ—è¡¨
        target_count: ç›®æ ‡ä¿ç•™çš„episodeæ•°é‡
        random_seed: éšæœºç§å­
    
    Returns:
        dict: {dataset_path: [episodes_to_delete]}
    """
    print("\n" + "=" * 80)
    print("å¼€å§‹é™é‡‡æ ·'Pick up'ä»»åŠ¡")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"Pick up"çš„episode
    pickup_episodes = find_pickup_episodes(dataset_paths)
    
    # æ”¶é›†æ‰€æœ‰episodeï¼ˆå¸¦æ•°æ®é›†ä¿¡æ¯ï¼‰
    all_pickup_episodes = []
    for dataset_path, episode_indices in pickup_episodes.items():
        for ep_idx in episode_indices:
            all_pickup_episodes.append((dataset_path, ep_idx))
    
    total_pickup = len(all_pickup_episodes)
    print(f"\næ€»å…±æ‰¾åˆ° {total_pickup} ä¸ªåŒ…å«'Pick up'ä»»åŠ¡çš„episode")
    
    if total_pickup <= target_count:
        print(f"âš ï¸  è­¦å‘Š: æ€»æ•°é‡ ({total_pickup}) å°äºç­‰äºç›®æ ‡æ•°é‡ ({target_count})ï¼Œæ— éœ€é™é‡‡æ ·")
        return {path: [] for path in dataset_paths}
    
    # éšæœºé€‰æ‹©è¦ä¿ç•™çš„episode
    random.seed(random_seed)
    episodes_to_keep = random.sample(all_pickup_episodes, target_count)
    episodes_to_keep_set = set(episodes_to_keep)
    
    # è®¡ç®—æ¯ä¸ªæ•°æ®é›†éœ€è¦åˆ é™¤çš„episode
    episodes_to_delete = {}
    for dataset_path in dataset_paths:
        episodes_to_delete[dataset_path] = []
        for ep_idx in pickup_episodes[dataset_path]:
            if (dataset_path, ep_idx) not in episodes_to_keep_set:
                episodes_to_delete[dataset_path].append(ep_idx)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\né™é‡‡æ ·è®¡åˆ’:")
    print(f"  æ€»episodeæ•°: {total_pickup}")
    print(f"  ä¿ç•™episodeæ•°: {target_count}")
    print(f"  åˆ é™¤episodeæ•°: {total_pickup - target_count}")
    print(f"\nå„æ•°æ®é›†åˆ é™¤è®¡åˆ’:")
    for dataset_path in dataset_paths:
        delete_count = len(episodes_to_delete[dataset_path])
        keep_count = len(pickup_episodes[dataset_path]) - delete_count
        print(f"  {dataset_path.name}: ä¿ç•™ {keep_count}, åˆ é™¤ {delete_count}")
    
    return episodes_to_delete

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿè®¡æ•°æ®é›†ä»»åŠ¡ç±»å‹å¹¶æ”¯æŒé™é‡‡æ ·')
    parser.add_argument('--downsample-pickup', type=int, default=None,
                        help='å¯¹"Pick up"ä»»åŠ¡è¿›è¡Œé™é‡‡æ ·ï¼Œä¿ç•™æŒ‡å®šæ•°é‡çš„episodeï¼ˆä¾‹å¦‚ï¼š850ï¼‰')
    parser.add_argument('--random-seed', type=int, default=42,
                        help='éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰')
    parser.add_argument('--dry-run', action='store_true',
                        help='åªæ˜¾ç¤ºè®¡åˆ’ï¼Œä¸å®é™…æ‰§è¡Œåˆ é™¤æ“ä½œ')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™åœ¨åŸæ•°æ®é›†ç›®å½•æ—è¾¹åˆ›å»º_downsampç‰ˆæœ¬ï¼Œä¿ç•™åŸæ•°æ®é›†ä¸å˜ï¼‰')
    
    args = parser.parse_args()
    
    # ============================================================================
    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
    # ============================================================================
    
    print("\n\n" + "=" * 80)
    print("ä»»åŠ¡ç±»å‹ç»Ÿè®¡ï¼ˆ1215_four_box, 1221_random, 1223_denseï¼‰")
    print("=" * 80)
    
    # ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    task_type_counts = {
        "Depalletize on the left": 0,
        "Depalletize on the right": 0,
        "Pick up": 0,
    }
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    pattern_left = re.compile(r'Depalletize.*on the left', re.IGNORECASE)
    pattern_right = re.compile(r'Depalletize.*on the right', re.IGNORECASE)
    pattern_pickup = re.compile(r'Pick up', re.IGNORECASE)
    
    total_episodes = 0
    dataset_stats = {}  # æ¯ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
    
    for dataset_path in DATASET_PATHS:
        dataset_name = dataset_path.name
        print(f"\nå¤„ç†æ•°æ®é›†: {dataset_name}")
        
        # åˆå§‹åŒ–è¯¥æ•°æ®é›†çš„ç»Ÿè®¡
        dataset_task_counts = {
            "Depalletize on the left": 0,
            "Depalletize on the right": 0,
            "Pick up": 0,
        }
        dataset_episodes = 0
        
        # æŸ¥æ‰¾æ‰€æœ‰episodesæ–‡ä»¶
        episodes_dir = dataset_path / "meta" / "episodes"
        if not episodes_dir.exists():
            print(f"  è­¦å‘Š: {episodes_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        episodes_files = sorted(episodes_dir.glob("**/file-000.parquet"))
        print(f"  æ‰¾åˆ° {len(episodes_files)} ä¸ªepisodesæ–‡ä»¶")
        
        for episodes_file in episodes_files:
            try:
                # è¯»å–episodesæ–‡ä»¶
                table = pq.read_table(episodes_file)
                df = table.to_pandas()
                
                # éå†æ¯ä¸ªepisode
                for _, row in df.iterrows():
                    total_episodes += 1
                    dataset_episodes += 1
                    tasks = row.get('tasks', [])
                    
                    # å¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
                    task_list = []
                    if isinstance(tasks, (list, tuple)):
                        task_list = list(tasks)
                    elif hasattr(tasks, '__iter__') and not isinstance(tasks, str):
                        try:
                            task_list = list(tasks)
                        except:
                            task_list = [str(tasks)]
                    elif isinstance(tasks, str):
                        # å¯èƒ½æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼Œå°è¯•è§£æ
                        try:
                            import ast
                            parsed = ast.literal_eval(tasks)
                            if isinstance(parsed, (list, tuple)):
                                task_list = list(parsed)
                            else:
                                task_list = [parsed]
                        except:
                            task_list = [tasks]
                    else:
                        task_list = [str(tasks)]
                    
                    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
                    for task in task_list:
                        if isinstance(task, str):
                            if pattern_left.search(task):
                                task_type_counts["Depalletize on the left"] += 1
                                dataset_task_counts["Depalletize on the left"] += 1
                            elif pattern_right.search(task):
                                task_type_counts["Depalletize on the right"] += 1
                                dataset_task_counts["Depalletize on the right"] += 1
                            elif pattern_pickup.search(task):
                                task_type_counts["Pick up"] += 1
                                dataset_task_counts["Pick up"] += 1
            
            except Exception as e:
                print(f"  è¯»å–æ–‡ä»¶ {episodes_file} æ—¶å‡ºé”™: {e}")
                continue
    
        # ä¿å­˜è¯¥æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
        dataset_stats[dataset_name] = {
            "episodes": dataset_episodes,
            "tasks": dataset_task_counts.copy()
        }
        print(f"  å®Œæˆ: {dataset_episodes} ä¸ªepisodes")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n{'=' * 80}")
    print("ä»»åŠ¡ç±»å‹ç»Ÿè®¡ç»“æœ:")
    print(f"{'=' * 80}")
    
    # æŒ‰æ•°æ®é›†è¾“å‡º
    print(f"\nå„æ•°æ®é›†çš„ç»Ÿè®¡:")
    for dataset_name, stats in dataset_stats.items():
        print(f"\n  {dataset_name}:")
        print(f"    Episodesæ•°: {stats['episodes']:,}")
        dataset_total = sum(stats['tasks'].values())
        for task_type, count in stats['tasks'].items():
            percentage = (count / dataset_total * 100) if dataset_total > 0 else 0
            print(f"      {task_type}: {count:,} æ¡ ({percentage:.2f}%)")
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\n{'=' * 80}")
    print("æ€»ä½“ç»Ÿè®¡:")
    print(f"{'=' * 80}")
    print(f"\næ€»episodesæ•°: {total_episodes:,}")
    print(f"\nå„ä»»åŠ¡ç±»å‹çš„æ€»æ•°é‡:")
    total_tasks = sum(task_type_counts.values())
    for task_type, count in task_type_counts.items():
        percentage = (count / total_tasks * 100) if total_tasks > 0 else 0
        print(f"  {task_type}: {count:,} æ¡ ({percentage:.2f}%)")
    
    # ============================================================================
    # æ‰§è¡Œé™é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    # ============================================================================
    
    if args.downsample_pickup is not None:
        print(f"\n{'=' * 80}")
        print("å¼€å§‹æ‰§è¡Œé™é‡‡æ ·")
        print(f"{'=' * 80}")
        
        # è®¡ç®—éœ€è¦åˆ é™¤çš„episode
        episodes_to_delete = downsample_pickup_episodes(
            DATASET_PATHS,
            target_count=args.downsample_pickup,
            random_seed=args.random_seed
        )
        
        if args.dry_run:
            print("\nâš ï¸  DRY RUNæ¨¡å¼ï¼šåªæ˜¾ç¤ºè®¡åˆ’ï¼Œä¸å®é™…æ‰§è¡Œåˆ é™¤")
            print("è¦å®é™…æ‰§è¡Œåˆ é™¤ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
        else:
            # ç¡®è®¤æ“ä½œ
            total_to_delete = sum(len(episodes) for episodes in episodes_to_delete.values())
            if total_to_delete > 0:
                print(f"\nâš ï¸  è­¦å‘Š: å³å°†åˆ é™¤ {total_to_delete} ä¸ªepisode")
                response = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): ").strip().lower()
                if response != 'yes':
                    print("æ“ä½œå·²å–æ¶ˆ")
                    return
            
            # å¯¹æ¯ä¸ªæ•°æ®é›†æ‰§è¡Œåˆ é™¤
            for dataset_path, delete_indices in episodes_to_delete.items():
                if not delete_indices:
                    print(f"\nè·³è¿‡æ•°æ®é›† {dataset_path.name}ï¼ˆæ— éœ€åˆ é™¤ï¼‰")
                    continue
                
                print(f"\nå¤„ç†æ•°æ®é›†: {dataset_path.name}")
                print(f"  å‡†å¤‡åˆ é™¤ {len(delete_indices)} ä¸ªepisode...")
                
                try:
                    # åŠ è½½æ•°æ®é›†
                    # rootåº”è¯¥æŒ‡å‘æ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å«meta/å’Œdata/çš„ç›®å½•ï¼‰
                    # è®¾ç½®force_cache_sync=Falseä»¥é¿å…ä»Hubä¸‹è½½
                    dataset = LeRobotDataset(
                        repo_id=dataset_path.name,
                        root=dataset_path,  # ç›´æ¥ä½¿ç”¨æ•°æ®é›†è·¯å¾„ä½œä¸ºroot
                        force_cache_sync=False
                    )
                    
                    # éªŒè¯è¦åˆ é™¤çš„episodeç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    total_episodes = dataset.meta.total_episodes
                    episodes_to_keep = total_episodes - len(delete_indices)
                    print(f"  ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
                    print(f"     æ€»episodeæ•°: {total_episodes}")
                    print(f"     åˆ é™¤episodeæ•°: {len(delete_indices)}")
                    print(f"     ä¿ç•™episodeæ•°: {episodes_to_keep}")
                    print(f"  ğŸ’¡ æ³¨æ„: 'Processing data files' æ˜¾ç¤ºçš„æ˜¯æ•°æ®æ–‡ä»¶æ•°é‡ï¼Œä¸æ˜¯episodeæ•°é‡")
                    print(f"     ä¸€ä¸ªæ•°æ®æ–‡ä»¶å¯èƒ½åŒ…å«å¤šä¸ªepisodeçš„æ•°æ®")
                    invalid_indices = [idx for idx in delete_indices if idx < 0 or idx >= total_episodes]
                    if invalid_indices:
                        print(f"  âš ï¸  è­¦å‘Š: å‘ç°æ— æ•ˆçš„episodeç´¢å¼•: {invalid_indices[:10]}{'...' if len(invalid_indices) > 10 else ''}")
                        print(f"  âš ï¸  æ•°æ®é›†æ€»episodeæ•°: {total_episodes}")
                        # è¿‡æ»¤æ‰æ— æ•ˆçš„ç´¢å¼•
                        delete_indices = [idx for idx in delete_indices if idx not in invalid_indices]
                        if not delete_indices:
                            print(f"  âš ï¸  æ‰€æœ‰è¦åˆ é™¤çš„episodeç´¢å¼•éƒ½æ— æ•ˆï¼Œè·³è¿‡æ­¤æ•°æ®é›†")
                            continue
                        print(f"  âš ï¸  è¿‡æ»¤åï¼Œå®é™…åˆ é™¤ {len(delete_indices)} ä¸ªepisode")
                    
                    # ç¡®å®šè¾“å‡ºç›®å½•å’Œrepo_id
                    # é»˜è®¤åœ¨åŸæ•°æ®é›†ç›®å½•æ—è¾¹åˆ›å»ºæ–°æ•°æ®é›†ï¼ˆä¿ç•™åŸæ•°æ®é›†ä¸å˜ï¼‰
                    if args.output_dir:
                        output_dir = Path(args.output_dir) / f"{dataset_path.name}_downsample"
                    else:
                        # åœ¨åŸæ•°æ®é›†ç›®å½•çš„çˆ¶ç›®å½•ä¸‹åˆ›å»ºæ–°æ•°æ®é›†
                        output_dir = dataset_path.parent / f"{dataset_path.name}_downsample"
                    
                    # å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
                    if output_dir.exists():
                        print(f"  âš ï¸  è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}")
                        print(f"  âš ï¸  æ­£åœ¨åˆ é™¤æ—§ç›®å½•...")
                        import shutil
                        shutil.rmtree(output_dir)
                        print(f"  âœ… æ—§ç›®å½•å·²åˆ é™¤")
                    
                    # æ–°æ•°æ®é›†çš„repo_id
                    new_repo_id = f"{dataset_path.name}_downsample"
                    
                    # æ‰§è¡Œåˆ é™¤
                    new_dataset = delete_episodes(
                        dataset=dataset,
                        episode_indices=delete_indices,
                        output_dir=output_dir,
                        repo_id=new_repo_id
                    )
                    
                    print(f"  âœ… å®Œæˆï¼æ–°æ•°æ®é›†ä¿å­˜åœ¨: {new_dataset.root}")
                    print(f"  âœ… æ–°æ•°æ®é›†åŒ…å« {new_dataset.meta.total_episodes} ä¸ªepisode")
                
                except Exception as e:
                    print(f"  âŒ å¤„ç†æ•°æ®é›† {dataset_path.name} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(f"\n{'=' * 80}")
            print("é™é‡‡æ ·å®Œæˆï¼")
            print(f"{'=' * 80}")

if __name__ == '__main__':
    main()

