#!/usr/bin/env python3
"""
éªŒè¯ LeRobot v3.0 æ•°æ®é›†ä¸­çš„ task å®šä¹‰æ˜¯å¦æ­£ç¡®
"""

import sys
from pathlib import Path

try:
    import pandas as pd
except ImportError:
    print("éœ€è¦å®‰è£… pandas: pip install pandas")
    sys.exit(1)

try:
    from datasets import load_dataset
except ImportError:
    print("éœ€è¦å®‰è£… datasets: pip install datasets")
    sys.exit(1)


def verify_dataset_tasks(dataset_root: str):
    """éªŒè¯æ•°æ®é›†ä¸­çš„ task å®šä¹‰"""
    dataset_root = Path(dataset_root)
    meta_dir = dataset_root / "meta"
    
    print(f"\n{'='*80}")
    print(f"éªŒè¯æ•°æ®é›†: {dataset_root}")
    print(f"{'='*80}\n")
    
    # 1. è¯»å– tasks.parquet
    tasks_file = meta_dir / "tasks.parquet"
    if not tasks_file.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° tasks.parquet æ–‡ä»¶: {tasks_file}")
        return False
    
    tasks_df = pd.read_parquet(tasks_file)
    print("ğŸ“‹ Task å®šä¹‰ (tasks.parquet):")
    print(tasks_df.to_string())
    print(f"\næ€»å…±æœ‰ {len(tasks_df)} ä¸ªä»»åŠ¡å®šä¹‰\n")
    
    # 2. è¯»å– episodes ä¿¡æ¯
    episodes_dir = meta_dir / "episodes"
    if not episodes_dir.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° episodes ç›®å½•: {episodes_dir}")
        return False
    
    # æŸ¥æ‰¾æ‰€æœ‰ episodes parquet æ–‡ä»¶
    episode_files = list(episodes_dir.glob("**/*.parquet"))
    if not episode_files:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° episodes parquet æ–‡ä»¶")
        return False
    
    print(f"ğŸ“ æ‰¾åˆ° {len(episode_files)} ä¸ª episodes æ–‡ä»¶\n")
    
    # è¯»å–æ‰€æœ‰ episodes
    all_episodes = []
    for ep_file in episode_files:
        ep_df = pd.read_parquet(ep_file)
        all_episodes.append(ep_df)
    
    episodes_df = pd.concat(all_episodes, ignore_index=True)
    print(f"ğŸ“Š æ€»å…±æœ‰ {len(episodes_df)} ä¸ª episodes\n")
    
    # 3. æ£€æŸ¥ episodes ä¸­çš„ tasks
    print("ğŸ“ Episodes ä¸­ä½¿ç”¨çš„ tasks:")
    all_episode_tasks = set()
    for idx, row in episodes_df.iterrows():
        ep_tasks = row.get("tasks", [])
        # å¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
        if isinstance(ep_tasks, (list, tuple)):
            all_episode_tasks.update(ep_tasks)
        elif hasattr(ep_tasks, '__iter__') and not isinstance(ep_tasks, str):
            # numpy array æˆ–å…¶ä»–å¯è¿­ä»£å¯¹è±¡
            try:
                all_episode_tasks.update(list(ep_tasks))
            except:
                all_episode_tasks.add(str(ep_tasks))
        elif isinstance(ep_tasks, str):
            # å¯èƒ½æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼Œå°è¯•è§£æ
            try:
                import ast
                parsed = ast.literal_eval(ep_tasks)
                if isinstance(parsed, (list, tuple)):
                    all_episode_tasks.update(parsed)
                else:
                    all_episode_tasks.add(parsed)
            except:
                # å¦‚æœä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œç›´æ¥ä½œä¸ºå­—ç¬¦ä¸²
                all_episode_tasks.add(ep_tasks)
        else:
            all_episode_tasks.add(str(ep_tasks))
    
    print(f"  Episodes ä¸­å‡ºç°çš„æ‰€æœ‰ task: {sorted(all_episode_tasks)}")
    print(f"  tasks.parquet ä¸­å®šä¹‰çš„æ‰€æœ‰ task: {sorted(tasks_df.index.tolist())}\n")
    
    # éªŒè¯ä¸€è‡´æ€§
    tasks_in_parquet = set(tasks_df.index.tolist())
    if all_episode_tasks != tasks_in_parquet:
        print("âš ï¸  è­¦å‘Š: Episodes ä¸­çš„ tasks ä¸ tasks.parquet ä¸å®Œå…¨ä¸€è‡´!")
        missing_in_parquet = all_episode_tasks - tasks_in_parquet
        missing_in_episodes = tasks_in_parquet - all_episode_tasks
        if missing_in_parquet:
            print(f"  - Episodes ä¸­æœ‰ä½† tasks.parquet ä¸­æ²¡æœ‰: {missing_in_parquet}")
        if missing_in_episodes:
            print(f"  - tasks.parquet ä¸­æœ‰ä½† Episodes ä¸­æ²¡æœ‰: {missing_in_episodes}")
    else:
        print("âœ… Episodes ä¸­çš„ tasks ä¸ tasks.parquet ä¸€è‡´\n")
    
    # 4. å°è¯•åŠ è½½æ•°æ®é›†å¹¶éªŒè¯ task_index
    print("ğŸ” éªŒè¯æ•°æ®ä¸­çš„ task_index...")
    try:
        # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        data_dir = dataset_root / "data"
        data_files = list(data_dir.glob("**/*.parquet"))
        if not data_files:
            print("  âš ï¸  æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡æ•°æ®éªŒè¯")
            return True
        
        print(f"  æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶è¿›è¡Œå®Œæ•´éªŒè¯
        all_data = []
        for data_file in data_files:
            df = pd.read_parquet(data_file)
            if "task_index" in df.columns:
                all_data.append(df)
        
        if not all_data:
            print("  âš ï¸  æ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰ task_index åˆ—")
            return True
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_data = pd.concat(all_data, ignore_index=True)
        
        # æ£€æŸ¥ task_index çš„æœ‰æ•ˆæ€§
        unique_task_indices = combined_data["task_index"].unique()
        print(f"  æ•°æ®ä¸­å‡ºç°çš„ task_index: {sorted(unique_task_indices.tolist())}")
        print(f"  tasks.parquet ä¸­çš„ task_index èŒƒå›´: 0-{len(tasks_df)-1}")
        print(f"  æ€»æ•°æ®å¸§æ•°: {len(combined_data)}")
        
        invalid_indices = [idx for idx in unique_task_indices if idx < 0 or idx >= len(tasks_df)]
        if invalid_indices:
            print(f"  âŒ é”™è¯¯: å‘ç°æ— æ•ˆçš„ task_index: {invalid_indices}")
            return False
        else:
            print("  âœ… æ‰€æœ‰ task_index éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…")
        
        # éªŒè¯ task_index å¯¹åº”çš„ä»»åŠ¡
        print("\n  ğŸ“‹ Task åˆ†å¸ƒç»Ÿè®¡:")
        task_stats = {}
        for task_idx in sorted(unique_task_indices):
            if task_idx < len(tasks_df):
                task_name = tasks_df.iloc[task_idx].name
                task_count = (combined_data["task_index"] == task_idx).sum()
                task_stats[task_idx] = (task_name, task_count)
                print(f"    task_index={task_idx}: '{task_name}' (å‡ºç° {task_count} æ¬¡, {task_count/len(combined_data)*100:.1f}%)")
        
        # éªŒè¯ task_index ä¸ episodes ä¸­çš„ tasks æ˜¯å¦ä¸€è‡´
        print("\n  ğŸ”— éªŒè¯ task_index ä¸ episodes ä¸­çš„ tasks ä¸€è‡´æ€§...")
        task_name_to_index = {name: idx for idx, name in enumerate(tasks_df.index)}
        consistent = True
        for idx, row in episodes_df.iterrows():
            ep_tasks = row.get("tasks", [])
            ep_idx = row.get("episode_index", idx)
            
            # æå–ä»»åŠ¡åç§°
            if isinstance(ep_tasks, (list, tuple)):
                ep_task_names = list(ep_tasks)
            elif hasattr(ep_tasks, '__iter__') and not isinstance(ep_tasks, str):
                ep_task_names = list(ep_tasks)
            else:
                ep_task_names = [str(ep_tasks)]
            
            # æ£€æŸ¥è¯¥ episode çš„æ•°æ®ä¸­çš„ task_index æ˜¯å¦åŒ¹é…
            # è¿™é‡Œæˆ‘ä»¬åªåšæŠ½æ ·æ£€æŸ¥ï¼Œå› ä¸ºå®Œæ•´æ£€æŸ¥éœ€è¦åŠ è½½æ‰€æœ‰æ•°æ®
            if idx < 5:  # åªæ£€æŸ¥å‰5ä¸ªepisodesä½œä¸ºç¤ºä¾‹
                # å¯ä»¥é€šè¿‡ episode_index è¿‡æ»¤æ•°æ®æ¥éªŒè¯
                pass  # ç®€åŒ–å¤„ç†ï¼Œä¸»è¦éªŒè¯å·²å®Œæˆ
        
        if consistent:
            print("  âœ… Task å®šä¹‰ä¸æ•°æ®ä¸€è‡´")
        
    except Exception as e:
        print(f"  âš ï¸  éªŒè¯æ•°æ®æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… éªŒè¯å®Œæˆ!")
    return True


if __name__ == "__main__":
    datasets = [
        "/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1125_groot_train_data_with_task",
        "/home/lab/lerobot_groot/lerobot_data/v3_0_dataset/1125_groot_train_data_with_task_filtered",
    ]
    
    for dataset_path in datasets:
        if not Path(dataset_path).exists():
            print(f"âš ï¸  æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
            continue
        
        verify_dataset_tasks(dataset_path)
        print("\n")

